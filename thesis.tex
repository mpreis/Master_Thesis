% Copyright (c) 2016, Mario Preishuber. All rights reserved.
%
% Notation: <option description> (<options>)
%
% Specify
%   - layout (onecolumn, twocolumn),
%   - thesis type (bachelor, master),
%   - language (english, naustrian)
%   - indicate that you want to use the university seal as background of the
%     titlepage (seal)
%   - indicate that you want signature fields (signatures)
%   - chapter headings to be on the next page or the next recto, verso page
%     (openany, openright, openleft) (standard memoir option, passed through)
\documentclass[onecolumn, openright, master, english, signatures]{dbrgrptt}
\usepackage[round-precision=2,round-mode=places,scientific-notation=engineering]{siunitx}
\usepackage[nameinlink]{cleveref}
\usepackage{listings}
\usepackage{multirow}
\usepackage{csvsimple}
\usepackage{rotating}
\usepackage[numbers]{natbib}
\usepackage{tikz}
\usetikzlibrary{arrows, fit, positioning, shapes}
\input{figs/tikz-settings}

\hypersetup{
    colorlinks=false,
    pdfborder={0 0 0},
}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Titling page / Initialization
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thesistitle{%
Towards cache-optimal address allocation:%
% ~How fast could your code have run if you had known where to allocate memory?%
~How slow is your code?
}%
\thesisdate{\today}%
\thesisauthor{Mario Preishuber}{01120643}%
\setsupervisor{Univ.-Prof. Dr. Christoph Kirsch}%

% pages after this command (and before \mainmatter) are numbered roman
\frontmatter%

\input{preface}%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Main content
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\mainmatter%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}\label{cha:introduction}
% motivation
Caches are based on two major observations: When data is accessed once, it is likely (1) that nearby data is accessed in the near future (\emph{spatial locality}~\cite{jacob2010memory}) and (2) that the same data is accessed again in the near future (\emph{temporal locality}~\cite{jacob2010memory}).
To address spatial locality so-called \emph{cache lines} have been introduced.
A cache line is the smallest unit of data that can be loaded from the main memory.
All cache lines of a cache are of the same size and dependent on the cache line size, the values of multiple addresses fit in one cache line.
A program accesses the data stored in the main memory via \emph{addresses}.
An address is a unique identifier for a certain amount of data, e.g. 8 byte are a common amount of data accessible by one address~\cite{patterson2011computer}.
%
There are two different types of memory accesses, reading and modifying.
Reading data is realized by executing a \emph{load} instruction.
A load instruction reads the data of an address and stores the read value into one of the \ac{CPU}s registers for further computations.
At first it is tried to read the data from the cache.
If the data of the requested address is in the cache, it is read from there.
Otherwise the data has to be read from the main memory.
Data is loaded from the main memory, stored in the cache, and finally loaded into one of the \ac{CPU}s registers.
To modify data, a \emph{store} instruction has to be executed.
If the data to modify is already in the cache, the main memory is not accessed.
Otherwise, the data has to be loaded from the main memory into the cache first.
Independent of the access type, if the accessed data is available in the cache, this is called a \emph{cache hit}, otherwise when the data is not in the cache this is called a \emph{cache miss}.
%
Since a cache is limited in space and typically the data required for the execution, which is called the \emph{working set}~\cite{denning1968working}, is larger then the cache size, eventually the cache will run out of space.
Whenever the cache is full and an address is accessed that is not in the cache, it is necessary to free up space.
For this reason every cache implements an \emph{eviction policy} to decide on an eviction candidate.
The eviction candidate is written back to the main memory to make space for the requested data, then it is \emph{evicted}.
The choice of the eviction policy aims to satisfy temporal locality, e.g. a common eviction policy is to evict the least recently used address.
%
The fact that cached data is grouped, such a group of data is called \emph{cache lines}, influences the cache performance significantly.
Assume iterating over an array.
In such a case the performance might be excellent, because these are perfect conditions for spatial locality and temporal locality.
However, assume iterating over a dynamically allocated linked list.
Compared to an array it is not ensured that the list elements are stored contiguous in memory.
It is more likely that elements are distributed across the whole address space.
In a worst case scenario each iteration forces a cache miss.
This issue is a consequence of the \emph{memory layout} generated by the allocator through dynamic allocations.

% problem statement
We are interested in the following problem: given a trace of load and store instructions, can we find metrics that characterize the trace performance for a given cache and implement an execution engine for computing their quantities?

% methods & approach
For the purpose of this work we focus on the sequence of load and store instructions of a program, the so-called \emph{memory access trace} (short \emph{trace}).
We analyze traces of the SPEC 2006 benchmarks and the V8 benchmarks.
For each trace of a benchmark the following four metrics are computed:
(1) the \emph{accesses} represent the number of accesses on an address,
(2) the \emph{access distance} is defined as the number of accesses on other addresses between two accesses on the same address,
(3) the \emph{liveness interval length} represents the timespan an address is in use, and
(4) the \emph{overlapping liveness} is defined as the number of overlapping liveness intervals at a certain point of the execution.
In this work each store instruction indicates the beginning of a liveness interval.
A liveness interval ends with the last load instruction before the next store instruction, i.e. an address might consist of multiple liveness intervals.
%
First, our analysis observes the sequence of load and store instructions of a benchmark.
Next, the observed trace is analyzed according to the four metrics, \emph{accesses}, \emph{access distance}, \emph{liveness interval length}, and \emph{overlapping liveness}.
After the metrics have been saved, the procedure for the performance analysis starts.
The performance analysis uses different \emph{allocators} to modify the addresses used by the trace.
The modification of the used addresses is called \emph{trace transformation}.
Each transformed trace is executed on four simulated caches.
The simulated caches differ in the applied eviction policy and their information about the executed trace.
During the execution, the number of main memory accesses, the number of cache misses, and cache hits are counted to determine the performance of a trace for a certain simulated cache.

% conjecture
Our conjecture is that the four metrics \emph{accesses}, \emph{access distance}, \emph{liveness interval length}, and \emph{Overlapping Liveness} characterize the performance of a program.
Furthermore, we are convinced that caches may be more effective if memory is reused quickly and independently of where the data is located.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Theoretical Foundations}\label{cha:theoretical-foundations}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Hardware Model
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Hardware Model}\label{sec:hardware-model}

This section deals with the applied hardware model. The model used is reduced to the minimal required core components of a modern computer system. It consists of three components as illustrated in \Cref{fig:hardware-model}. The three components are the central processing unit, a cache, and the main memory.

\begin{figure}[!ht]
  \centering
  \input{figs/tikz/hardware-model}
  \caption{Hardware Model}
  \label{fig:hardware-model}
\end{figure}

\subsection{Central Processing Unit}
The \emph{\ac{CPU}} is the heart of a computer system.
The purpose of a \ac{CPU} is to process and execute a given program.
A \emph{program} consists of a sequence of instructions which operate on data.
It is processed sequentially, that is, instruction by instruction.
An \emph{instruction} is a command with the purpose to perform a specific action, e.g. to add two numbers or to modify data.
\emph{Data} is the information required by a program that is necessary for its execution, e.g. values for computations.
The \ac{CPU} consists of a limited number of so-called \emph{registers}.
A \emph{register} is an extremely small and extremely fast memory unit which allows the \ac{CPU} to execute computations.
Registers are the only memory unit where the \ac{CPU} is able to apply arithmetic operations.
The actual size of a single register and the number of registers available for computations depend on the architecture of the hardware.

For the purpose of this work, only instructions reading or writing data are taken into account.
In order to read data, a \emph{load} instruction is required; in order to write data, a \emph{store} instructions is applied.
These two instructions access the data of a program stored at the main memory, e.g. for the purpose of a calculation it might be necessary to get some data from the main memory and save its result at the main memory for later usage.
Such data can be accessed by executing a load or store instruction on main memory.
However, there are several more instructions available on modern computer systems, e.g. arithmetical operations.

\subsection{Main Memory}
The \emph{main memory} is a storage containing all data required to execute a program.
In general, the \ac{CPU} has to load data from the main memory and store data in the main memory to process a program.
Furthermore, even the program itself is stored at the main memory during its execution.
Each instruction of a program has to be loaded before the \ac{CPU} is able to execute it.
In case of a load instruction, the \ac{CPU} first loads the instruction itself.
Then the CPU interprets the instruction to find out what to do, e.g. execute a load of data.
Last the \ac{CPU} executes the instruction, e.g. it loads the actually required data into some of the available registers.

The main memory is structured in equally sized chunks of memory, that are for example the size of 8 byte.
These are then called a \emph{word}.
Each of these memory chunks can be accessed by the \ac{CPU} via a unique identifier, its \emph{physical address}.
To simplify a programmers life and make programs easier portable, \emph{virtual addresses} have been introduced.
A virtual address (short \emph{address}) maps a physical address.
Each program on a computer is given its own virtual address space which starts at address 0; this is an important assumption.
In reality the first physical address of a program is very unlikely to be 0.
This mapping of physical addresses to virtual addresses which always start at address 0 makes programs easier portable.
For the purpose of this work, it is important to keep in mind that load and store instruction operate on addresses, e.g. \texttt{load \&address} where \texttt{\&address} represents the address of the data that is to be loaded.

\subsection{Cache}
A \emph{cache} is a small high-speed memory which temporarily holds data of the addresses used by the currently processed program.
Smith describes the concept of caches in \cite{smith1982cache}.

Caches are based on two major observations, namely \emph{Temporal locality} and \emph{Spatial locality}.
\emph{Temporal locality} means that if data is accessed it is likely that the same data is accessed again in the near future.
\emph{Spatial locality} means that if data is accessed, it is likely that other data nearby is also accessed in the near future.
Speaking about \emph{accessing an address} is equivalent with \emph{accessing data stored at an address in the main memory}.
The same accounts for cached addresses.
For the \ac{CPU} a cache is invisible.
Regardless of whether or there is a cache or not the \ac{CPU} always just wants to access a certain address.
If there is cache present, it simply takes less time to load the value of an address into the \ac{CPU}s register.
This is because caches are high-speed memory units.

A \emph{cache hit} occurs whenever the \ac{CPU} wants to access an address which is already in the cache.
Then no main memory access is required.
The data is directly accessed via cache.

A \emph{cache miss} occurs whenever the \ac{CPU} accesses an address that is not currently in the cache.
In such a situation it is required to load the data of the requested address from the main memory into the cache.
Before the value stored at this address, it is loaded into one of the registers.
Such an operation is expensive as explained in \Cref{sec:performance}.

The \emph{cache policy} decides which address has to be \emph{evicted}, i.e., which address has to be written back into the main memory in order to make free space available.
Since caches are limited in the number of addresses that can be temporarily stored, a cache will eventually run out of space.
In the case that the cache is full and a cache miss occurs, it is required to make space available to load the requested address.
There are different algorithms trying to make an appropriate chose of the address, that is to be evicted.
For more details see \Cref{sec:caches}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Memory Access Trace
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Memory Access Trace}\label{sec:memory-access-trace}

The \emph{\ac{trace}} represents all memory accesses for a given program.
More precisely, the \ac{trace} is a sequence of load and store instructions observed by analyzing a given program.
Furthermore, for the purpose of this work it does not matter which value is stored at a certain address.
Therefore, the stored values are all dropped.
This results in a sequence of tuples consisting of the instruction type, which is either \emph{load} or \emph{store}, and an address.
\Cref{fig:mat-example-trace} shows an example of a \ac{trace} where addresses are annotated with \texttt{\&} known from programming languages like C.

\Cref{fig:mat-example-c-code} shows a simple C program which sums up three numbers.
First, all used variables are declared.
Afterwards, the variables \texttt{sum}, \texttt{x}, and \texttt{y} are initialized with the values \texttt{0}, \texttt{1}, and \texttt{2}.
Followed by the first computation, the value of \texttt{x} is added to \texttt{sum}s value and stored in \texttt{sum}.
Next, the variable \texttt{z} is initialized with value \texttt{3}.
Then, \texttt{sum} is increased by the value of \texttt{y}.
Finally, the computation is completed by adding the value of \texttt{z} to \texttt{sum}.

\begin{figure}[!ht]
  \centering
  \begin{tabular}{c}
  \lstinputlisting[language=C]{figs/code/memory-access-trace/mat.c}
  \end{tabular}
  \caption{C code example of summing three numbers.}
  \label{fig:mat-example-c-code}
\end{figure}

\Cref{fig:mat-example-assembly-code} shows a snippet of assembly code generated by compiling the code of \Cref{fig:mat-example-c-code}.
For compilation GCC 4.8.5 on Ubuntu 16.04 for AMD Opteron\texttrademark Processor 6376 with x86\_64 Architecture was used.
Since the declaration of the variables is only important for the compiler, no assembly code is generated for: \texttt{int sum, x, y, z;}.
Therefore, the first line of assembly code shown in \Cref{fig:mat-example-assembly-code} represents the code generated for the C code \texttt{sum = 0;}.
However, the compiler has not generated a store operation, instead the following code appears \texttt{movl \$0, -16(\%rbp)}.

This assembly instruction consists of three parts.
The first part shows the instruction that should be executed.
In the example above \texttt{movl} represents this instruction.
\texttt{movl} moves a \emph{long} value into a register.
A long value is on the x86\_64 architecture the size of 32 bit.
The second part represents the value that should be moved.
In the example from above a constant value is moved.
The fact that \texttt{0} is a constant value is indicated by the \texttt{\$} character.
The third part represents the address to which the value should be moved to.
In the example from above the target address is \texttt{-16(\%rbp)}.
In this case, \texttt{rbp} is a register indicated by the \texttt{\%} character.
Further, \texttt{\%rbp} is a so-called \emph{general-purpose} register of size 64 bit.
By now it is enough to know that this register holds a memory address which is used as base to compute the actual target address.
\texttt{-16} is the offset used to compute the actual target address.
\Cref{fig:mat-example-assembly-code} only consists of two different types of instructions.
The \texttt{movl} instruction is explained above.
The other operation is used as follows \texttt{addl \%eax -16(\%rbp)}.
The meaning of this instruction is to add the value stored at \texttt{-16(\%rbp)} to the value currently stored at the register \texttt{\%eax} and store the result at \texttt{\%eax}.
As indicated by the \texttt{l} character at the and of the instruction name, this instruction operates on values of the size of 32 bit.

\begin{figure}[!ht]
  \centering
  \begin{tabular}{c}
  \lstinputlisting[firstline=13, lastline=22]{figs/code/memory-access-trace/mat.s}
  \end{tabular}
  \caption{Assembly code snippet generated by compiling the code of \Cref{fig:mat-example-c-code} with GCC 4.8.5 on Ubuntu 16.04.5 for AMD Opteron\texttrademark~Processor 6376 with x86\_64 Architecture.}
  \label{fig:mat-example-assembly-code}
\end{figure}

\Cref{fig:mat-example-assembly-code} shows the assembly code of generated for the C code of \Cref{fig:mat-example-c-code}.
Obviously, the first three assembly instructions correlate to the three assignments of the C code.
It might be unexpected that the \texttt{movl} instruction is used instead of a store instruction.
However, moving a value to a certain location is semantically equivalent to a store.
Nevertheless, there are three addresses used, each with an offset.
These offsets increase exactly by the same size: four (-16, -12, and -8).
The operation \texttt{movl} operates on 32 bit which are 4 byte.
Hence, the variables \texttt{sum}, \texttt{x}, and \texttt{y} are stored contiguously in memory.

\begin{figure}[!ht]
  \centering
  \begin{tabular}{c}
  \begin{lstlisting}
sum = 0; | movl $0, -16(%rbp)
  x = 1; | movl $1, -12(%rbp)
  y = 2; | movl $2, -8(%rbp)
  \end{lstlisting}
  \end{tabular}
  \caption{Assignments: C code (left) and generated assembly code (right).}
  \label{fig:mat-example-comp-assignment}
\end{figure}

Different to assignments an addition leads to two lines of assembly code as illustrated by \Cref{fig:mat-example-comp-addition}.
The first instruction loads the value of \texttt{x} (stored at address \texttt{-12(\%rbp)}) into register \texttt{\%eax}.
Since the \ac{CPU} can apply arithmetic operations exclusively on registers, it is required to load the value of \texttt{x} into a register before computing the sum.
Again the \texttt{movl} instruction is used to load the data.
As before, the semantics of the \texttt{movl} is equivalent to a load instruction.
The second instruction generated is the actual computation.
The generated \texttt{addl} instruction operates on 32 bit, as well as the \texttt{movl} instruction.
Indicated by the last letter of the instruction name, \texttt{l}.
\texttt{addl} takes the value store in the register \texttt{\%eax} and adds the value store at the address \texttt{-16(\%rbp)}.
The result of this computation is stored in the \texttt{\%eax} register.

\begin{figure}[!ht]
  \centering
  \begin{tabular}{c}
  \begin{lstlisting}
sum = sum + x; | movl -12(%rbp), %eax
               | addl %eax, -16(%rbp)
  \end{lstlisting}
  \end{tabular}
  \caption{Addition: C code (left) and generated assembly code (right).}
  \label{fig:mat-example-comp-addition}
\end{figure}

After the addition, the assignment of variable \texttt{z} follows.
This assignment works exactly the same as discussed at the beginning of this section.
The same accounts for the remaining two additions (\texttt{sum = sum + y;} and \texttt{sum = sum + z;}).
The next step is to take look at the \ac{trace} used in this work.

\Cref{fig:mat-example-trace} presents the \ac{trace} observed by the example of \Cref{fig:mat-example-c-code}.
To illustrate that the \ac{trace} operates on the addresses the C like character \texttt{\&} is used as prefix of an address.
All information expects this kind of access and the accessed address is dropped.

\begin{figure}[!ht]
  \centering
  \begin{tabular}{c}
  \lstinputlisting{figs/code/memory-access-trace/mat.trace.manual}
  \end{tabular}
  \caption{Memory access trace of the assembly code shown in \Cref{fig:mat-example-assembly-code}.}
  \label{fig:mat-example-trace}
\end{figure}

\Cref{fig:mat-example-comp-assignment-all} compares the assignment of the C code with the generated assembly code and the resulting \ac{trace} of these instructions.
Assignments are translated into stores in the \ac{trace}.
In order to facilitate the procedure, the stored value is dropped, so that only the accessed address remains in the \ac{trace}.

\begin{figure}[!ht]
  \centering
  \begin{tabular}{c}
  \begin{lstlisting}
sum = 0; | movl $0, -16(%rbp) | store &sum
  x = 1; | movl $1, -12(%rbp) | store &x
  y = 2; | movl $2, -8(%rbp)  | store &y
  \end{lstlisting}
  \end{tabular}
  \caption{Assignments: C code (left), generated assembly code (middle), and \ac{trace} (right).}
  \label{fig:mat-example-comp-assignment-all}
\end{figure}

\Cref{fig:mat-example-comp-addition-all} presents the \ac{trace} for the addition \texttt{sum = sum + x;}.
The move instruction \texttt{movl -12(\%rbp), \%eax} loads the value of \texttt{x} into a register, which translates as expected into a load instruction in the \ac{trace}.
The addition itself translates into two instructions within the \ac{trace}.

\begin{figure}[!ht]
  \centering
  \begin{tabular}{c}
  \begin{lstlisting}
sum = sum + x; | movl -12(%rbp), %eax | load  &x
               | addl %eax, -16(%rbp) | load  &sum
                                      | store &sum
  \end{lstlisting}
  \end{tabular}
  \caption{Addition: C code (left), generated assembly code (middle), and \ac{trace} (right).}
  \label{fig:mat-example-comp-addition-all}
\end{figure}

An striking observation is that there are no arithmetic operations in the \ac{trace}, which makes sense, since this representation of a program shows all the interaction with the main memory.
However, this does not mean that while translating the assembly code to its \ac{trace}, arithmetic operations could be skipped.
Nevertheless, not only the move instructions could lead to a main memory access.
\Cref{fig:mat-example-addition-detail} shows a main memory access that is achieved via reading the value stored at address \texttt{-16(\%rbp)} for the computation.
Such an instruction leads to a \texttt{load} within the \ac{trace}.
Since the result of a computation is saved somewhere there is also a \texttt{store} observable in \ac{trace}.

\begin{figure}[!ht]
  \centering
  \begin{tabular}{c}
  \begin{lstlisting}
addl %eax, -16(%rbp)
  \end{lstlisting}
  \end{tabular}
  \caption{Addition assembly code.}
  \label{fig:mat-example-addition-detail}
\end{figure}

This section showed how the \ac{trace} of a program is observed.
Hence, the assembly code of a simple C program which sums three numbers is discussed and finally the correlating \ac{trace} is presented.
The \ac{trace} of a program is a sequence of tuples.
Each of them holds the type of access which is either load or store and the accessed address.
An address is prefixed with an \texttt{\&}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Liveness
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Liveness}\label{sec:liveness}

This section introduces the concept of \emph{liveness of an address}.
Roughly speaking, the liveness of an address describes the timespan in which the address is used by a program.
In general, the liveness of an address begins with its allocation and ends with its deallocation as illustrated by \Cref{fig:liveness-classical}.
Yet, \citeauthor{aigner2013acdc} showed in their work \cite{aigner2013acdc} that the general understanding of liveness offers potential for improvement.

In their work, they introduced the term of \emph{deallocation delay}, describing the timespan from the last access on an object until its deallocation.
This is based on the observation that objects often live longer than necessary, because of this deallocation delay, which is a waste of resources, especially memory.

\begin{remark}
Note that in the work \cite{aigner2013acdc} the liveness term is defined at object level. In this work we are exclusively focusing on address level.
\end{remark}

However, in this work liveness is defined differently.
First of all, liveness is defined on address level, since the whole work is operating on addresses rather than objects or data structures.
Further, an address does not consist of \emph{the one} liveness.
Rather, it consists of multiple timespans of liveness.
In the timespan beginning with allocating an address and deallocating it, there are periods in which the address is used heavily and there are periods where the address is not used at all.
These periods of usage are called \emph{liveness intervals}.
How these liveness intervals are defined is presented by \Cref{def:liveness-interval}.
The basic idea is that each store operation introduces a new liveness intervals.
This is similar to the initialization of an address.

\begin{definition}[Liveness interval]\label{def:liveness-interval}
Whenever there is a store on an address a new \emph{liveness interval} begins. A liveness interval ends with the last access at the address before the next store instruction occurs. A liveness interval also ends with the absolute last access on the address.
\end{definition}

\begin{remark}
As an implication of Definition~\ref{def:liveness-interval} a single address might have multiple liveness intervals.
\Cref{fig:liveness-intervals-example} presents the liveness interval for the address of variable \texttt{sum}.
\end{remark}

\begin{figure}[!ht]
  \centering
  \input{figs/tikz/liveness-classical}
  \caption{Classical liveness}
  \label{fig:liveness-classical}
\end{figure}

\begin{figure}[!ht]
  \centering
  \input{figs/tikz/liveness}
  \caption{Liveness intervals}
  \label{fig:liveness-intervals}
\end{figure}

\Cref{fig:liveness-intervals-example} illustrates the liveness intervals of the C code example presented in \Cref{fig:mat-example-c-code}, which shows the instruction number on the left side and the instruction itself on the right side.
On top of the figure the addresses used by the program are listed.
The core of the figure illustrates the liveness intervals of the addresses.
Each line represents a single liveness interval.
A liveness interval begins at the lower instruction number and ends at the higher one.
The liveness intervals are quite obvious for the addresses \texttt{\&x}, \texttt{\&y}, and \texttt{\&z}.
According to Definition \ref{def:liveness-interval}, a liveness interval always begins with a store instruction.
The instructions 2, 3, and 7 indicate the beginnings of the liveness intervals for the three addresses \texttt{\&x}, \texttt{\&y}, and \texttt{\&z}.
The endings are indicated by load instructions at the instruction numbers 5, 9, and 12.
None of the addresses are accessed after these lines again, e.g. \texttt{\&x} which is not used anymore after instruction number 5.
The address \texttt{\&sum} presents a much more interesting case as it consists of four liveness intervals.
The first liveness interval of \texttt{\&sum} begins at instruction number 1.
This is when \texttt{\&sum} is initialized.
Followed by the initializations of \texttt{\&x} and \texttt{\&y} before \texttt{\&sum} is accessed again.
The load at instruction number 4 indicates the end of \texttt{\&sum}s first liveness interval, because the next access is a store.
At instruction number 6 the second liveness interval begins.
The other liveness intervals follow the same pattern.
Note that also the store at instruction number 13 yields a liveness interval, even if it is the shortest possible.
At the instruction numbers 5, 9, and 12 the address \texttt{\&sum} is \emph{dead} according to the applied definition of liveness intervals, see \Cref{fig:liveness-intervals}.
It would allow to reuse the address of \texttt{\&sum} for other purposes.

\begin{figure}[!ht]
  \centering
  \input{figs/tikz/liveness-intervals-example}
  \caption{Liveness intervals of the C code example shown in \Cref{fig:mat-example-c-code}.}
  \label{fig:liveness-intervals-example}
\end{figure}

This section presented the definition of liveness and especially of liveness intervals.
The definition of liveness intervals is one of the most central components of this work.
It is the base for the trace transformations described in \Cref{sec:trace-transformation}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Performance
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Performance}\label{sec:performance}

This section presents how the performance of a trace is computed.
The performance of a trace is the most significant criteria of its quality.
In general \ac{trace}s with better performance use the memory is available in a more efficient way than others.
The performance is independent of the number of instructions that a \ac{trace} consists of.
The only important criteria is the number of memory accesses.
As \Cref{fig:hardware-model} shows there are different kinds of memory access.
Depending if the accessed address is in the cache or not, the time required to load the value of a address into a register varies.
Cached data can be accessed much faster than data which has to be loaded from the main memory.
The \emph{latency} to load data into a register is measured in \emph{\ac{CPU} cycles}.
\ac{CPU} cycles are a common metric to measure durations at hardware level.

In our system \emph{performance} is expressed in \ac{CPA}.
Indeed performance is not directly measured.
As \Cref{equ:performance-cpa} illustrates, it is computed according to the number of memory accesses that are raised during execution.
\ac{CPA} describes the average number of \ac{CPU} cycles per memory access.
A memory access is equivalent to an instruction of the trace.

We proceed as follows. After the \ac{trace} of a program has been generated its performance is analyzed by applying the trace on a simulated cache and counting the different kinds of memory accesses.
At the end of the execution the performance is computed as illustrated by \Cref{equ:performance-cpa}.

\begin{equation}\label{equ:performance-cpa}
\text{CPA}(T,C) = \frac{\text{Sum of cycles}(T, C)}{\text{Sum of accesses}(T)}
\end{equation}

$T$ represents the \ac{trace} of the analyzed program.
$C$ represents the applied cache; the different caches are explained in \Cref{sec:caches}.

\emph{Sum of cycles(T, C)} represents the sum of all memory accesses and each one weighted according to the costs of its type.
The costs for a memory access depends on the latency of the memory unit that is accessed.
This is why cache accesses are cheaper than accessing the main memory.
The \emph{sum of accesses(T)} represents the total number of load and store instructions executed by the \ac{trace} $T$.

\Cref{tab:memory-access-cost} shows the costs for the different types of memory accesses.
These numbers are taken from literature \cite{drepper2007every}, \cite{skylake}.
The actual values are not that important than the relation of the costs.

\begin{table}[!ht]
  \centering
  \begin{tabular}{lc}
  \hline
  Memory Access Type & Cost in Cycles \\
  \hline
  Cache  load  & 1 \\
  Cache  store & 1 \\
  Memory load  & 5 \\
  Memory store & 5 \\
  \hline
  \end{tabular}
  \caption{Cost for memory access types}
  \label{tab:memory-access-cost}
\end{table}

This section presents our definition of performance and how the performance of a trace is computed.
For this procedure the memory accesses of a \ac{trace} are recored and finally used for computation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Metrics
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Metrics}\label{sec:metrics}

This section presents the metrics chosen to characterize a \ac{trace}.
The characteristic of a \ac{trace} consists of the four metrics \emph{accesses}, \emph{access distance}, \emph{overlapping liveness}, and \emph{liveness interval length}.
These are used to reason about the resulting performance of a \ac{trace} and further to compare a \ac{trace} with other \ac{trace}s.
The metrics presented by this section are all applied after \ac{trace} transformation.
Hence, these operate on variables rather than directly on addresses.
Nevertheless, variables could easily be mapped to addresses.
Furthermore, for all four metrics the same statistical parameters are computed: minimum, maximum, average, 5\% percentile, 25\% percentile, 50\% percentile, 75\% percentile, and 95\% percentile.

\begin{remark} Statical metrics\
\begin{itemize}
\item The \emph{minimum} is the numerical smallest value of all samples.
\item The \emph{maximum} is the numerical largest value of all samples.
\item The \emph{average} is the arithmetic mean of all samples. It is computed by dividing the sum of all samples by the number of samples.
\item The \emph{percentile} is the value below which a given percentage of all samples fall, i.e., the 25\% percentile represents the value for which holds that 25\% of all samples are smaller than this value.
\end{itemize}
\end{remark}

\begin{figure}[!ht]
  \centering
  \begin{tabular}{c}
  \begin{lstlisting}
1: load  A
2: load  B
3: store A
  \end{lstlisting}
  \end{tabular}
  \caption{Tiny \ac{trace} example. Note: this trace has been transformed, i.e., there is no \texttt{\&} so \texttt{A} and \texttt{B} represent variables not addresses. On the left the instruction number is shown and on the right the correlating instruction is presented.}
  \label{fig:metrics-exmaple}
\end{figure}

\subsection{Accesses}\label{ssec:metric-accesses}
The metric called \emph{accesses} is defined as the number of accesses on a certain variable.
For this reason the number of accesses on each variable are counted, regardless of the access type.
Applying this metric on the tiny example presented in \Cref{fig:metrics-exmaple}, results in the following list of samples $(1, 2)$.
Variable $A$ is accessed twice and variable $B$ is accessed only once.

\subsection{Access Distance}\label{ssec:metric-access-distance}
The \emph{access distance} metric shows the distance between two sequential accesses on the same variable.
For example, if there is only one variable accessed, the access distance of such a \ac{trace} is 0.
Assume a \ac{trace} that accesses two variables alternating as shown by \Cref{fig:metrics-exmaple}, then the access distance of $A$ is 2.
It is computed by subtracting the instruction number of the current access and the instruction number of the previous access on a certain variable.
For example, the access distance of variable $A$ is computed by $3-1 = 2$, same of $B$ ($2 - 2 = 0$).
Applying this metric on the tiny example presented in \Cref{fig:metrics-exmaple}, results in the following list of samples $(0, 2)$.
Between the load of variable $A$ and the store on it there is only one other instruction, that the reason why there is $2$ in the list.

\subsection{Overlapping Liveness}\label{ssec:metric-concurrently-live}
The \emph{overlapping liveness} metric is defined as the number of overlapping liveness intervals at a certain point of the execution.
The set of live variables is called \emph{working set}.
The term working set has been introduced by the work \cite{denning1968working}.
This metric shows how the working set of a \ac{trace}s grows and shrinks.
For this reason at each instruction the currently live variables are recorded.
Applying this metric on the tiny example presented in \Cref{fig:metrics-exmaple}, results in the following list of samples $(1, 1, 2)$.
At instruction number 1 and 3 only one variable is live $A$.
At instruction number 2 there are two variables live respectively $A$, and $B$.

\subsection{Liveness Interval Length}\label{ssec:metric-liveness-interval-length}
The \emph{liveness interval length} metric is defined as the different lengths of liveness intervals.
In general, a variable is used for a specific purpose that yields the variable to be live a certain timespan.
There are variables that are used for a short period and others are live for longer.
If a variable is only accessed once, the liveness interval length is $0$.
The liveness interval length is computed by subtracting the instruction number of the first access of the liveness interval from the instruction number of its last access.
Applying this metric on the tiny example presented in \Cref{fig:metrics-exmaple}, results in the following list of samples $(0, 2)$.
As explained above, if a variable is only accessed once, the liveness interval length is $0$, this is the case with $B$.
The length of the liveness interval of variable $A$ is represented by the second value of the list, $2$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Trace Transformation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Trace Transformation}\label{sec:trace-transformation}

\Cref{sec:memory-access-trace} illustrated how to observe the \ac{trace} of a program.
\Cref{sec:liveness} describes our definition of liveness and introduces liveness intervals.
This section shows how to use this information to transform the \ac{trace} $T$ of a program.
The aim is to transform a \ac{trace} $T$ into a \ac{trace} $T'$ that is semantically equivalent to $T$, but offers better performance.
The performance of a \ac{trace} is determined as presented in \Cref{sec:performance}.
The transformation of a \ac{trace} does not effect the sequence of load and store instructions.
Nevertheless, the addresses used by the \ac{trace} are replaced according to a given algorithm.
The algorithms available are described below.
We distinguish between the originally used addresses and the addresses used after transformation; the latter ones are called \emph{variables}.
The different naming simplifies talking about \ac{trace}s.
Each time we talk about an address, it refers to the \ac{trace} observed from the binary.
Talking about variables indicates that the \ac{trace} has been transformed already.

\subsection{Identity Trace}

The identity trace reproduces the original \ac{trace}.
For transformation each unique address of a \ac{trace} is replaced by an unique variable.
Based on the \ac{trace} (see \Cref{fig:mat-example-trace}) observed from the C code, illustrated in \Cref{fig:mat-example-c-code}, the addresses are replaced by variables, as shown in \Cref{fig:trace-transformation-original}.
Basically, \texttt{\&sum} becomes $A$, \texttt{\&x} becomes $B$, \texttt{\&y} becomes $C$, and \texttt{\&z} becomes $D$.
The x-axis and y-axis are switched, because it requires less space vertically.
However, note that liveness intervals of the variables shown in \Cref{fig:trace-transformation-original} are exactly the same as those of the addresses illustrated in \Cref{fig:liveness-intervals-example}.

\begin{figure}[!ht]
  \centering
  \input{figs/tikz/trace-transformation-original}
  \caption{Liveness intervals of the C code example shown in \Cref{fig:mat-example-c-code}.}
  \label{fig:trace-transformation-original}
\end{figure}

In this work two different kinds of \ac{trace} transformations are applied, respectively expanding the \ac{trace} and collapsing the \ac{trace}.

\subsection{Single Assignment Trace}

The single assignment trace expands the original \ac{trace}, i.e., it uses more variables than the original \ac{trace} addresses.
For expanding a \ac{trace} we decided to use a \emph{single assignment} form.
More specific, each liveness interval of a \ac{trace} is assigned to a variable.
Further, a variable is never assigned to a liveness interval more often than exactly once.
\Cref{fig:trace-transformation-sa} presents the \emph{single assignment trace} of \Cref{fig:mat-example-trace}.
It is not surprising that the number of variables required increases when applying such an approach.
In detail, the four liveness intervals of address \texttt{\&sum} are now assigned to the variables $A$, $D$, $F$, and $G$.
In this example, those variables are assigned in alphabetical order according to the beginning of an liveness interval.
This is the reason why the variables used to map the liveness intervals of address \texttt{\&sum} are not contiguous.
For this approach iterate through the \ac{trace} and each time a store instruction occurs, assign the next free variable.
The implementation details are explained in \Cref{ssec:allocator-single-assignment}.

\begin{figure}[!ht]
  \centering
  \input{figs/tikz/trace-transformation-sa}
  \caption{Liveness intervals of the C code example shown in \Cref{fig:mat-example-c-code} in \emph{single assignment} form.}
  \label{fig:trace-transformation-sa}
\end{figure}

\subsection{Compact Trace}

For collapsing a \ac{trace} we use a \emph{compact} form.
Each liveness interval of a \ac{trace} is assigned to a variable, but now variables are \emph{reused}.
If the liveness interval ends, the variable is added to a \emph{free list}.
A free list is a list of variables that have been used before but their liveness intervals have ended.
Hence, these variables can be used again for another liveness interval.
The procedure is as follows: Before using a new variable, the free list is checked; if there are variables available at the free list, these are used.
Otherwise a new variable is assigned.
The implementation details are explained in \Cref{ssec:allocator-compact}.
There are various data structures that can be used to implement a free.
The selected data structure influences the semantic of the free list.
In this work three variants are investigated, (1) stack semantic, (2) queue semantic, and (3) set semantic.
The set semantic is thought to represent a random selection of a free variable.
\Cref{fig:trace-transformation-compact} illustrates the compaction of the \ac{trace} shown in \Cref{fig:mat-example-trace}.
For this example a free list with stack semantic is used.
As expected, this approach requires less variables.
The liveness intervals of \texttt{\&sum} remain the same with variable $A$.
Furthermore, the addresses \texttt{\&x} and \texttt{\&z} now share variable $B$.

\begin{remark}
Note that only by coexistent the liveness intervals of address \texttt{\&sum} are assigned to the same variable $A$.
In general, it depends on the semantics of the free list and the currently free variables which variable a liveness interval is assigned to.
\end{remark}

\begin{figure}[!ht]
  \centering
  \input{figs/tikz/trace-transformation-compact}
  \caption{Liveness intervals of the C code example shown in \Cref{fig:mat-example-c-code} in \emph{compacted} form.}
  \label{fig:trace-transformation-compact}
\end{figure}

This section presents two different kinds of \ac{trace} transformations, single assignment and compaction.
Single assignment potentially increases the number of used variables and compaction potentially decreases the number of used variables.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Summarizing Example
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Summarizing Example}

Lets take a look at the performance of the three \ac{trace}s from above.
Assume a \ac{LRU} cache with 2 cache lines, where each cache line fits exactly one variable.
For details on \ac{LRU} caches see \Cref{ssec:cache-lru}.
As a breath description of the applied cache, imagine as long as there is a free cache line within the cache, this one is used.
In case there is no more free cache line it is required to evict one of the used cache lines.
To make space, the \emph{least recently used} cache line is evicted.

If a variable is accessed that is already in the cache, a cheap cache access can be executed, either a load or store instruction.
Otherwise, the main memory has to be accessed that is much more expensive
The costs of the different kinds of accesses are presented in \Cref{tab:memory-access-cost}.
For the following performance computation we assume a simple optimization: The first store instruction on a variable can be executed directly into the cache.
Further, assume that in case of an eviction the data of a variable has to be stored in the main memory, regardless if the variable is live or dead.

\subsection{Identity Trace}
\Cref{fig:trace-transformation-original-marked} shows the annotated liveness intervals of the identity \ac{trace} illustrated in \Cref{fig:trace-transformation-original}.
According to the assumptions from above, every beginning of a liveness interval is annotated with a cache store.
Since the assumed cache fits exactly two variables, the first eviction occurs when writing variable $C$ at instruction number 3.
According to the eviction policy of the applied \ac{LRU} cache variable $A$ is evicted.
The memory store of variable $A$ is annotated with a filled red circle.
Unfortunately, variable $A$ is accessed at the next instruction.
For this reason it has to be loaded from the main memory again, but before $B$ has to be stored in in the main memory to make space in the cache for $A$.
The same procedure is repeated for the variables $B$ and $C$ at instruction number 5.
At instruction number 6 we are able to load a variable directly from cache for the first time.
Nevertheless, there are two more evictions required to finish the execution of this trace.
One at instruction number 7, variable $B$ is evicted to make space for variable $D$.
An interesting aspect at this instruction is that the liveness interval of variable $B$ ended two instructions before, hence $B$ is dead.
Furthermore, $B$ is not needed anymore for the following instructions.
In general, this information is not available, this is the reason why there is a memory store.
However, if a system knows about the liveness intervals of its variables it might decide to overwrite the value of $B$ with the value of $D$ and avoid the memory store.
The last eviction appears at instruction number 9, where variable $C$ is accessed for the last time. Variable $A$ is accessed several time without any interaction with the main memory, because each time it is accessed, it is in the cache.

\begin{figure}[!ht]
  \centering
  \input{figs/tikz/trace-transformation-original-marked}
  \caption{Liveness intervals of the C code example shown in \Cref{fig:mat-example-c-code}. Annotated by the different kinds of memory accesses. Assuming a LRU cache with 2 cache lines, each cache line fits exactly one variable.}
  \label{fig:trace-transformation-original-marked}
\end{figure}

The metrics of this \ac{trace} are illustrated in \Cref{fig:trace-transformation-original-marked}.
The \emph{accesses} metric is observed by recording the number of accesses for each variable.
In this example, the list of samples holds four values one for each variable.
The number of accesses on variable $A$ is $7$; all other variables are accessed twice.
The resulting sorted list of samples looks as follows $(2, 2, 2, 7)$.
The \emph{access distance} presents a kind of the access frequency on a variable.
Depending on the number of accesses, the list of access distances could be significantly larger than the number of used variables.
In this case the sorted list of samples looks as follows $(0, 1, 2, 2, 2, 2, 3, 3, 5, 6)$.
For variable $A$ there are multiple access distances (from left to write) $3$, $2$, $2$, $2$, $1$, and $0$.
The \emph{overlapping liveness} metrics shows how many variables are used at a certain instruction number.
The list of samples is computed by counting the overlapping liveness intervals for each instruction number.
As a result the size of the samples list depends on the number of instructions rather than on the number of used variables.
In the case of the current example the samples are $(1, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 1, 1)$.
For the purpose of better understanding the list is not sorted.
Instead it is ordered according to the correlating instruction number, i.e., the first entry represents the number of overlapping liveness intervals at instruction number 1.
The fourth metric is the \emph{liveness interval length}.
The number of used variables and the number of accesses define the number of samples for this metric.
The number of accesses on a variable are significant, because these define the liveness intervals.
For this example the list of samples looks as follows $(0, 1, 2, 3, 3, 5, 6)$.
For variable $A$ there are four entries in the list $0$, $1$, $2$, and $3$.
The resulting metrics are presented in \Cref{tab:summarizing-example-metrics-original}.

\begin{table}[!ht]
  \centering
  \begin{tabular}{lrrrrrrrr}
    \hline
    \multirow{2}{*}{Metric} & \multirow{2}{*}{Min.} & \multirow{2}{*}{Max.} & \multirow{2}{*}{Avg.} & \multicolumn{5}{c}{Percentile} \tabularnewline
    & & & & 5\% & 25\% & 50\% & 75\% & 95\% \tabularnewline
    \hline
    Accesses                 & 2.00 & 7.00 & 3.25 & 2.00 & 2.00 & 2.00 & 3.25 & 6.25 \\
    Access Distance          & 0.00 & 6.00 & 2.60 & 0.45 & 2.00 & 2.00 & 3.00 & 5.55 \\
    Overlapping Liveness     & 1.00 & 3.00 & 2.08 & 1.00 & 2.00 & 2.00 & 3.00 & 3.00 \\
    Liveness Interval Length & 0.00 & 6.00 & 2.86 & 0.30 & 1.50 & 3.00 & 4.00 & 5.69 \\
    \hline
  \end{tabular}
  \caption{Metrics of the identity \ac{trace} illustrated in \Cref{fig:trace-transformation-original-marked} (values are rounded).}
  \label{tab:summarizing-example-metrics-original}
\end{table}

To compute the performance of \Cref{equ:trace-transformation-original-marked} the different kinds of memory accesses have to be counted and weighted as illustrated by \Cref{equ:trace-transformation-original-marked}.
According to the annotations of \Cref{fig:trace-transformation-original-marked} there are 2 cache load, 7 cache stores, 4 memory loads, and 5 memory stores.
The result of this equation indicates that $4.15$ \ac{CPU} cycles are required in average to process one instruction of the \ac{trace}.

\begin{equation}\label{equ:trace-transformation-original-marked}
\text{\ac{CPA}}(T_{original}, C_{LRU}) = \frac{1 * (2 + 7) + 5 * (4 + 5)}{13} = 4.15
\end{equation}

\subsection{Single Assignment Trace}
\Cref{fig:trace-transformation-sa-marked} shows the annotated liveness intervals of the single assignment trace of \Cref{fig:trace-transformation-sa}.
Unexpectedly, the number of cache stores increases according to the number of variables used.
The combination of using more variables and the assumption that the first access on a variable yields a cache store, increases the number of cache stores executed.
Further, using more variables seems to reduce the number of cache loads at least for this example.
In total, the number of cache accesses is the same as for the identity \ac{trace}, but the distribution differs.
Taking a look at the main memory accesses, we observe an increase in the number of memory stores.
The single assignment \ac{trace} requires the same amount of memory loads as the identity \ac{trace}.
Compared to \Cref{fig:trace-transformation-original}, there are three times more memory stores on variables that are already dead.
The other four memory stores executed are identical to those of the identity \ac{trace}.

\begin{figure}
  \centering
  \input{figs/tikz/trace-transformation-sa-marked}
  \caption{Liveness intervals of the C code example shown in \Cref{fig:mat-example-c-code} in \emph{single assignment} form. Annotated by the different kinds of memory accesses. Assuming a \ac{LRU} cache with 2 cache lines, each cache line fits exactly one variable.}
  \label{fig:trace-transformation-sa-marked}
\end{figure}

The accesses for the \ac{trace} shown in \Cref{fig:trace-transformation-sa} is computed as introduced in \Cref{ssec:metric-accesses}.
The computation outputs this list of samples: $(1, 2, 2, 2, 2, 2, 2)$.
As expected for each variable there is only one entry in the list.
Different to the accesses of the identity \ac{trace}, there are more entries.
By coincidence each variable except $G$ is accessed twice.
Computing the \emph{access distance} results in the following list of samples $(0, 1, 2, 3, 3, 5, 6)$.
The \emph{overlapping liveness} variables are observed by counting the overlapping liveness intervals and result in the displayed list $(1, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 1, 1)$.
Since the liveness intervals are not changed during transforming a \ac{trace}, these values are the same as for the identity \ac{trace}.
The samples for the \emph{liveness interval length} look as follows $(0, 1, 2, 3, 3, 5, 6)$.
For this example the samples of the access distance and the liveness interval length are identical, but this is a coincidence.
\Cref{tab:summarizing-example-metrics-sa} presents the statistical metrics.

\begin{table}[!ht]
  \centering
  \begin{tabular}{lrrrrrrrr}
    \hline
    \multirow{2}{*}{Metric} & \multirow{2}{*}{Min.} & \multirow{2}{*}{Max.} & \multirow{2}{*}{Avg.} & \multicolumn{5}{c}{Percentile} \tabularnewline
    & & & & 5\% & 25\% & 50\% & 75\% & 95\% \tabularnewline
    \hline
    Accesses                 & 1.00 & 2.00 & 1.86 & 1.30 & 2.00 & 2.00 & 2.00 & 2.00 \\
    Access Distance          & 0.00 & 6.00 & 2.86 & 0.30 & 1.50 & 3.00 & 4.00 & 5.67 \\
    Overlapping Liveness.    & 1.00 & 3.00 & 2.08 & 1.00 & 2.00 & 2.00 & 3.00 & 3.00 \\
    Liveness Interval Length & 0.00 & 6.00 & 2.86 & 0.30 & 1.50 & 3.00 & 4.00 & 5.69 \\
    \hline
  \end{tabular}
  \caption{Metrics of the single assignment trace \ac{trace} are illustrated by \Cref{fig:trace-transformation-sa-marked} (values are rounded).}
  \label{tab:summarizing-example-metrics-sa}
\end{table}

For this single assignment trace we observe 2 cache loads, 7 cache stores, 4 memory loads, and 7 memory stores that result in a \ac{CPA} of $4.92$.
This result is slightly worse than the \ac{CPA} of the identity \ac{trace} presented by \Cref{equ:trace-transformation-original-marked}.

\begin{equation}\label{equ:trace-transformation-sa-marked}
\text{\ac{CPA}}(T_{single~assigment}, C_{LRU}) = \frac{1 * (2 + 7) + 5 * (4 + 7)}{13} = 4.92
\end{equation}

\subsection{Compact Trace}
\Cref{fig:trace-transformation-compact-marked} shows the annotated liveness intervals of the compact trace based on \Cref{fig:trace-transformation-compact}.
The most significant difference compared to the others two traces is the number of variables used.
Variable $B$ is reused instead of picking a new variable.
Obviously, there are less memory stores required than for the single assignment trace.
Furthermore, there are even less memory stores required than for the identity \ac{trace}.
The reason is that reusing variable $B$ turns the memory store into a cache store.
The other memory accesses are quite similar as explained above.
Each liveness interval begins with a cache store.
Variable $A$ allows two cache loads.
The memory stores on the instruction numbers 3, 4, 5, and 9 are identical for all three traces.
Same for the memory loads at the instruction numbers 4, 5, 9, and 12.

\begin{figure}
  \centering
  \input{figs/tikz/trace-transformation-compact-marked}
  \caption{Liveness intervals of the C code example shown in \Cref{fig:mat-example-c-code} in \emph{compacted} form. Annotated by the different kinds of memory accesses. Assuming a \ac{LRU} cache with 2 cache lines, each cache line fits exactly one variable.}
  \label{fig:trace-transformation-compact-marked}
\end{figure}

For the \ac{trace} presented in \Cref{fig:trace-transformation-compact} the list of samples for the \emph{accesses} metric is $(2, 4, 7)$.
According the number of used variables the list consists of three values.
The samples of the \emph{access distance} look as follows $(0, 1, 2, 2, 2, 2, 2, 3, 3, 5, 6)$.
The list of overlapping liveness intervals of the compact \ac{trace} is identical to the list of the identity \ac{trace}, $(1, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 1, 1)$.
The reason is that the liveness intervals are not changed, but are rearranged.
As already mentioned the samples for the \emph{liveness interval length} are the same as for the single assignment \ac{trace} and the identity \ac{trace}, $(0, 1, 2, 3, 3, 5, 6)$.
The reason for this is that the liveness intervals are never changed.
The resulting statistical metrics are presented by \Cref{tab:summarizing-example-metrics-compact}.

\begin{table}[!ht]
  \centering
  \begin{tabular}{lrrrrrrrr}
    \hline
    \multirow{2}{*}{Metric} & \multirow{2}{*}{Min.} & \multirow{2}{*}{Max.} & \multirow{2}{*}{Avg.} & \multicolumn{5}{c}{Percentile} \tabularnewline
    & & & & 5\% & 25\% & 50\% & 75\% & 95\% \tabularnewline
    \hline
    Accesses                 & 2.00 & 7.00 & 4.33 & 2.20 & 3.00 & 4.00 & 5.50 & 6.70 \\
    Access Distance          & 0.00 & 6.00 & 2.55 & 0.50 & 2.00 & 2.00 & 3.00 & 5.50 \\
    Overlapping Liveness     & 1.00 & 3.00 & 2.08 & 1.00 & 2.00 & 2.00 & 3.00 & 3.00 \\
    Liveness Interval Length & 0.00 & 6.00 & 2.86 & 0.30 & 1.50 & 3.00 & 4.00 & 5.69 \\
    \hline
  \end{tabular}
  \caption{Metrics of the compact trace \ac{trace} illustrated by \Cref{fig:trace-transformation-compact-marked} (values are rounded).}
  \label{tab:summarizing-example-metrics-compact}
\end{table}

There are 2 cache loads, 7 cache stores, 4 memory loads, and 4 memory stores required for the execution that result in a \ac{CPA} of $3.77$.
That shows that for this example the presented compact trace performs best.

\begin{equation}\label{equ:trace-transformation-compact-marked}
\text{\ac{CPA}}(T_{compact}, C_{LRU}) = \frac{1 * (2 + 7) + 5 * (4 + 4)}{13} = 3.77
\end{equation}

\subsection{Conclusion}
To conclude, for each metric and the performance the three \ac{trace}s are compared. This example illustrates the differences of the identity \ac{trace}, the single assignment \ac{trace}, and the compact \ac{trace}.

\subsubsection{Performance}
\Cref{tab:summarizing-example-performance} presents the computed performance for each \ac{trace} executed on a simulated cache based on \ac{LRU} eviction policy.
As the table shows, a transformation not always leads to an improvement.
For this example the compacted \ac{trace} yields the best performance.

\begin{table}[!ht]
  \centering
  \begin{tabular}{lrrrrrrrr}
    \hline
    Trace & \ac{CPA} \tabularnewline
    \hline
    Identity          & 4.15 \\
    Single Assignment & 4.92 \\
    Compact           & 3.77 \\
    \hline
  \end{tabular}
  \caption{Compare the \emph{performance} of all three different \ac{trace}s.}
  \label{tab:summarizing-example-performance}
\end{table}

\subsubsection{Accesses}

By transforming a \ac{trace} into another one, the total number of accesses never changes.
Nevertheless, which variable and how many times it is accessed could be different.
\Cref{tab:summarizing-example-metrics-overview-accesses} illustrates the comparison of the three \ac{trace}s discussed.
Obviously, the variables of the single assignment \ac{trace} are accessed significantly less often than it is the case for the others.
To pick one value, 95 percentage of all variables of the single assignment \ac{trace} are accessed only twice.
For the identity \ac{trace} and the compact \ac{trace} the 95\% percentile is larger than 6.
Hence, the variables of the compact \ac{trace} and the identity \ac{trace} are accessed $\sim3$ times more often that those of the single assignment \ac{trace}.

\begin{table}[!ht]
  \centering
  \begin{tabular}{lrrrrrrrr}
    \hline
    \multirow{2}{*}{Trace} & \multirow{2}{*}{Min.} & \multirow{2}{*}{Max.} & \multirow{2}{*}{Avg.} & \multicolumn{5}{c}{Percentile} \tabularnewline
    & & & & 5\% & 25\% & 50\% & 75\% & 95\% \tabularnewline
    \hline
    Identity          & 2.00 & 7.00 & 3.25 & 2.00 & 2.00 & 2.00 & 3.25 & 6.25 \\
    Single Assignment & 1.00 & 2.00 & 1.86 & 1.30 & 2.00 & 2.00 & 2.00 & 2.00 \\
    Compact           & 2.00 & 7.00 & 4.33 & 2.20 & 3.00 & 4.00 & 5.50 & 6.70 \\
    \hline
  \end{tabular}
  \caption{Compare the \emph{accesses} metric of all three different \ac{trace}s.}
  \label{tab:summarizing-example-metrics-overview-accesses}
\end{table}

\subsubsection{Access Distance}

\Cref{tab:summarizing-example-metrics-overview-access-distance} shows the comparison of the access distance metric.
The differences between identity \ac{trace} and compact \ac{trace} are minimal.
The reason is that there is only one difference, the liveness interval of variable $D$ of the identity \ac{trace} is merged into variable $B$ of the compact \ac{trace}.
For the calculation this means that there is one sample more in the list of the compact \ac{trace}.
All other samples are identical to those the identity \ac{trace}.
The differences between identity \ac{trace} and single assignment \ac{trace} are more significant.
The reason is that the access distance of the single assignment \ac{trace} is reduced to the liveness intervals.
In this case the list of samples for the single assignment \ac{trace} is shorter than the one of the identity \ac{trace}.

\begin{table}[!ht]
  \centering
  \begin{tabular}{lrrrrrrrr}
    \hline
    \multirow{2}{*}{Trace} & \multirow{2}{*}{Min.} & \multirow{2}{*}{Max.} & \multirow{2}{*}{Avg.} & \multicolumn{5}{c}{Percentile} \tabularnewline
    & & & & 5\% & 25\% & 50\% & 75\% & 95\% \tabularnewline
    \hline
    Identity          & 0.00 & 6.00 & 2.60 & 0.45 & 2.00 & 2.00 & 3.00 & 5.55 \\
    Single Assignment & 0.00 & 6.00 & 2.86 & 0.30 & 1.50 & 3.00 & 4.00 & 5.67 \\
    Compact           & 0.00 & 6.00 & 2.55 & 0.50 & 2.00 & 2.00 & 3.00 & 5.50 \\
    \hline
  \end{tabular}
  \caption{Compare the \emph{access distance} metric of all three different \ac{trace}s.}
  \label{tab:summarizing-example-metrics-overview-access-distance}
\end{table}

\subsubsection{Overlapping Liveness}
The results for the overlapping liveness metric are presented in \Cref{tab:summarizing-example-metrics-overview-concurrently-live}.
Unexpectedly, the results for all three \ac{trace}s are identical.
Hence, the liveness intervals are not modified by any transformation.

\begin{table}[!ht]
  \centering
  \begin{tabular}{lrrrrrrrr}
    \hline
    \multirow{2}{*}{Trace} & \multirow{2}{*}{Min.} & \multirow{2}{*}{Max.} & \multirow{2}{*}{Avg.} & \multicolumn{5}{c}{Percentile} \tabularnewline
    & & & & 5\% & 25\% & 50\% & 75\% & 95\% \tabularnewline
    \hline
    Identity          & 1.00 & 3.00 & 2.08 & 1.00 & 2.00 & 2.00 & 3.00 & 3.00 \\
    Single Assignment & 1.00 & 3.00 & 2.08 & 1.00 & 2.00 & 2.00 & 3.00 & 3.00 \\
    Compact           & 1.00 & 3.00 & 2.08 & 1.00 & 2.00 & 2.00 & 3.00 & 3.00 \\
    \hline
  \end{tabular}
  \caption{Compare the \emph{overlapping liveness} metric of all three different \ac{trace}s.}
  \label{tab:summarizing-example-metrics-overview-concurrently-live}
\end{table}

\subsubsection{Liveness Interval Length}
The results for the liveness interval length metric does not show any differences between the three \ac{trace}s as \Cref{tab:summarizing-example-metrics-overview-liveness-interval-length} illustrates.
It is as expected, because the liveness intervals are not modified by any transformation.

\begin{table}[!ht]
  \centering
  \begin{tabular}{lrrrrrrrr}
    \hline
    \multirow{2}{*}{Trace} & \multirow{2}{*}{Min.} & \multirow{2}{*}{Max.} & \multirow{2}{*}{Avg.} & \multicolumn{5}{c}{Percentile} \tabularnewline
    & & & & 5\% & 25\% & 50\% & 75\% & 95\% \tabularnewline
    \hline
    Identity          & 0.00 & 6.00 & 2.86 & 0.30 & 1.50 & 3.00 & 4.00 & 5.69 \\
    Single Assignment & 0.00 & 6.00 & 2.86 & 0.30 & 1.50 & 3.00 & 4.00 & 5.69 \\
    Compact           & 0.00 & 6.00 & 2.86 & 0.30 & 1.50 & 3.00 & 4.00 & 5.69 \\
    \hline
  \end{tabular}
  \caption{Compare the \emph{liveness interval length} metric of all three different \ac{trace}s.}
  \label{tab:summarizing-example-metrics-overview-liveness-interval-length}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem Statement
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Problem Statement}\label{cha:problems-tatement}
\begin{definition}[Problem statement]
Given a \ac{trace} $T$ of load and store instructions find metrics that characterize the trace performance for a given cache $C$ and implement an execution engine for computing their quantities.
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Implementation}\label{cha:implementation}

This chapter explains the implementation of our environment used for the experiments presented by \Cref{cha:experiment}.
\Cref{cha:theoretical-foundations} explains the theoretical background for the tool chain applied to observe the required information.
In general, there are two major aspects we are interested in, (1) the performance of a \ac{trace} and (2) the metrics of a \ac{trace} which characterize its performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Workflow
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Workflow}

We implemented a multistate offline approach to observe all the information as illustrated by \Cref{fig:workflow}.
In this context \emph{offline} indicates that there are analyses before executing the \ac{trace}. More specific, this is about \emph{Preparation}, and \emph{Transformation \& Analysis}.

\begin{description}
  \item[Compilation] It is required to generate a binary of a benchmark which should be analyzed.
  The benchmarks we used are presented by \Cref{sec:benchmarks}.
  For our experiments these benchmarks are compiled with \emph{GCC 4.8.5} on \emph{Ubuntu 16.04} for \emph{AMD Opteron\texttrademark Processor 6376} with \emph{x86\_64 Architecture}.

  \item[Preparation] Next is to obtain a \ac{trace} of load and store instruction on addresses.
  We use the Valgrind\cite{Valgrind} tool called \emph{Lackey} to obtain all the memory accesses of an x86\_64 executable.
  This represents the \ac{trace} of the benchmark which will be analyzed and transformed later on.

  \item[Analysis] After the \ac{trace} has been obtained and before it is executed, we analyze the \ac{trace} to observe the metrics explained in \Cref{sec:metrics}.
  The observed information is partly used by some of the caches presented by \Cref{sec:caches}.

  \item[Execution \& Transformation] Finally the \ac{trace} can be executed to figure out its performance.
  During execution the \ac{trace} is transformed.
  An allocator is used to proceed the transformation into a single assignment \ac{trace}, or a compact \ac{trace}, or an identity \ac{trace}.
  In this work there are several caches available which are presented by \Cref{sec:caches}.
  During execution the memory accesses are counted which are finally used to calculate the performance.
\end{description}

\begin{figure}[!ht]
  \centering
  \input{figs/tikz/workflow}
  \caption{Workflow}
  \label{fig:workflow}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Allocators
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Allocators}
\label{sec:allocators}

This section presents the implementation of the trace transformation.
In general, for every trace there is a transformation required to replace the addresses observed via Valgrind by variables.
Even for analyzing the identity \ac{trace} a transformation is required.
The actual transformation is done during the execution phase by an \emph{allocator}.
Each store instruction is interpreted like a \texttt{malloc} in C.
Hence, each store instruction forces the allocator to return a variable to operate on.
This is the point in time where an address is replaced by a variable.
It is up to the used allocator if a new variable is returned or a variable is reused.
This architecture is the reason why also for the execution the identity \ac{trace}, a transformation is applied.
In our system there are three types of allocators: the \emph{Identity Allocator}, the \emph{Single Assignment Allocator} and the \emph{Compact Allocator}.

\subsection{Identity Allocator}\label{ssec:allocator-original}

The identity allocator is used to measure the performance of a programs original \ac{trace}
This allocator produces the \emph{identity \ac{trace}} as explained by \Cref{sec:trace-transformation}.
The identity \ac{trace} is important to figure out which one of the other transformations shows an improvement.
The implementation of the identity allocator is as simple as it simply used the addresses of a \ac{trace} as variables.

\subsection{Single Assignment Allocator}\label{ssec:allocator-single-assignment}

The single assignment allocator is used to illustrate the performance of never reusing any address at all.
This can be interpreted as a compacting allocator with a compaction ration of 0 percentage.
That means there is no compaction.
It is implemented by a bump pointer allocator assuming endless memory.
Each time there is a store instruction, the bump pointer is increased and the new variable is used.
The bump pointer is never decreased.

\subsection{Compacting Allocator}\label{ssec:allocator-compact}

The compacting allocators are used to illustrate the advantages and disadvantages of compaction.
Compaction is achieved by using a free list.
If the free list is empty, the bump pointer is increased and the new variable is used.
Otherwise a variable of the free list is taken.
The variable is removed from the list and returned for usage.
This work presents three different implementations of the compact allocator.
These differ in the semantics of the free list, there is one implementation with \emph{stack} semantic, one with \emph{queue} semantic, and one with \emph{set} semantic.
The stack semantic is equivalent with picking the most recently freed variable.
In contrast to the stack semantic there is the queue semantics which represents picking the least recently freed variable.
And finally the set semantic represents picking a random variable of the freed ones.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Caches
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Caches}\label{sec:caches}

This section presents our implementations of caches.
As explained by \Cref{sec:hardware-model} within the environment of computer where \ac{CPU} communicates with the main memory a cache is a hardware component.
For the purpose of this work all caches simulated are implemented in software.
This allows us to analyze multiple different caches.
Obviously, we cannot measure real time for the execution of a \ac{trace} because every component of our environment is implemented in software.
This is the reason why in \Cref{sec:performance} the performance is defined by the number of different memory accesses instead of using the total execution time of a benchmark as usually.

Nevertheless, our cache implementations offer the most basic properties as configurable parameters.
For all implementations the \emph{cache size} and the \emph{cache line size} are configurable.
The \ac{LRU} caches offer several more configurable parameters.

\begin{remark}
Note that our cache implementations operate on variables rather than on addresses.
This is because the applied allocator transforms the \ac{trace} of addresses into a \ac{trace} of variables.
\end{remark}

In our environment four caches are available.
There two different eviction policies implemented.
One using the classical \ac{LRU} algorithm to decide which item of the cache will be evicted.
And the other one using an algorithm proposed by \Citeauthor{belady1966study} in his work \cite{belady1966study} from \citeyear{belady1966study} to decide on the item which will be evicted.

For both algorithms used as eviction policy, there is another implementation which has full information about the liveness intervals of all variables of a \ac{trace}.
This enables further optimizations as explained below.

\subsection{MIN Cache}\label{ssec:cache-MIN}

This implementation of a cache is based on the algorithm proposed in \cite{belady1966study}, called \emph{\ac{MIN}}.
The basic idea behind \citeauthor{belady1966study}s algorithm is to evict the item within the cache which is not need for the longest time.
More precise, if there is a dead item in the cache this is evicted, otherwise the item that is accessed again furthest in the future is evicted.
In praxis this algorithm is hard to apply, because it would require to know the future which is not the case in real systems.
Nevertheless, analyzing an implementation of this approach is quite interesting, because the algorithm has been proven to be the optimal eviction policy (\cite{mattson1970evaluation}, \cite{van2007short}, \cite{vogler2008another}, \cite{lee2016simple}).
Even if this algorithm is hardly applicable in reality, our environment offers all requirements.
This is one advantage of the offline analysis of a \ac{trace}. All required information for the \citeauthor{belady1966study} algorithm is already available before the execution begins.

\subsection{Least Recently Used Cache}\label{ssec:cache-lru}

This implementation of a cache is based on the \ac{LRU} algorithm, which is part of the literature since years (\cite{johnson1994x3}, \cite{drepper2007every}, \cite{patterson2011computer}, \cite{jacob2010memory}, and many more).
Different than \citeauthor{belady1966study}s algorithm the \ac{LRU} approach is widely used in praxis.
Instead of caring about future accesses which are unknown in realty, this approach takes a look at the past accesses.
For this reason it is required to keep track of the accesses on the currently cached items.
A \ac{LRU} cache decides based on its track information and always decided on the item in the cache which has not been accessed for the longest time.
This is also the source of its name, \emph{least recently used}.
The idea behind is that an item that has not been accessed for longer, therefore is a possibility that it is not needed anymore but is still in the cache.
For the purpose of this work the \ac{LRU} cache implementation which comes with Valgrind is used.

\subsection{Cache Proceeding}
\subsubsection{Without Liveness Information}\label{ssec:cache-behavior}

The proceeding of a cache without liveness information in general is identical for the \ac{LRU} implantation and the \Citeauthor{belady1966study} implementation.
The only difference is in the choice of the eviction candidate.

\begin{figure}[!ht]
  \centering
  \input{figs/tikz/cache-behavior}
  \caption{Cache behavior without liveness information}
  \label{fig:cache-behavior}
\end{figure}

\Cref{fig:cache-behavior} illustrates the proceeding of the caches.
Everything starts with the access on a variable.
Right now it does not matter if the access is a load or a store instruction.
At first, it is checked if the accessed variable is currently in the cache.
In the case that the variable is cached, this is a cache hit; we are done and the initial access can be executed without any further actions.
Otherwise, if the accessed variable is not cached, it is a cache miss.
Such a case requires further action; namely the next step is to check if there is space for at least one more variable in the cache.

If the cache is full, it is required to make some space.
This is achieved by applying the eviction policy to decide on the variable which will be kicked out of the cache.
The value of the eviction candidate has to be saved.
For this reason its value is written back to the main memory by a store instruction.
Then, the accessed variables value can be loaded from the main memory and finally the requested access can be executed.

In case that the cache is not full, we are able to skip the selection of an eviction candidate and of course it is not required to save its value in the main memory, therefore the store instruction is not needed.
Such a situation allows to immediately load the value of the requested variable.
In this case an expensive store instruction on the main memory is saved.

Summarizing, in the worst case it is required to execute two expensive main memory instructions before the actual variable can be accessed.
This is not optimal in comparison to the best case which does not require any main memory access.
However, as \Cref{fig:cache-behavior} illustrates, there is the possibility to get rid of at least one of the expensive instructions.

\subsubsection{With Liveness Information}\label{ssec:cache-behavior-liveness}

\Cref{ssec:cache-behavior} shows that there are certain cases in which the number of expensive main memory accesses can be reduced.
This sections tries to minimize the number of main memory accesses by using the information about the liveness intervals of a \ac{trace}.
Again the \ac{LRU} implementation and the \Citeauthor{belady1966study} implementation differ only by the selection of the eviction candidate.

\begin{figure}[!ht]
  \centering
  \scalebox{0.8}{\input{figs/tikz/cache-behavior-liveness}}
  \caption{Cache behavior with liveness information}
  \label{fig:cache-behavior-liveness}
\end{figure}

\Cref{fig:cache-behavior-liveness} illustrates the proceeding for caches with liveness information.
Everything starts with the access on a variable.
By now it does not matter which kind of access it is.
In case it becomes relevant, it will be discussed.
As without liveness information the first thing to do, is to check whether the accessed variable is cached or not.
If the accessed variable is in the cache, it is a cache hit and we are done.
The actual access can be executed directly on the cache.
Otherwise, if the accessed variable is not in the cache yet, its a cache miss.
A cache miss requires further steps to get the accessed variable into the cache.

Next is to check whether the cache is full or not.
In case there is no more space in the cache for another variable, it is necessary to evict one of the cached variables.
The eviction candidate is chosen by the eviction policy of the cache, either via \ac{LRU} algorithm or via \Citeauthor{belady1966study}s algorithm.
While proceeding a variable access, the selection of the eviction candidate is the only difference which might occur between the two available implementations which use liveness information.
Next, we make use of the liveness information by proving if the eviction candidate is live or dead.
If the eviction candidate is still live; this means its liveness interval did not end yet, and it is necessary to save the variables value.
For this reason a store instruction on the main memory is executed to save the eviction candidates value.
If the variable chosen for eviction is dead, it is not required to save its value, because it is not used anymore in future.
This is why the store instruction on the main memory can be skipped.
If the cache is not full yet, then the whole eviction procedure, selecting a variable to evict and saving its value if needed, can be skipped.

After all the possible steps and opportunities so far the cache has reached a state in which there is at least one free spot available for the accessed variable.
The only question remaining is: Do we have to load the accessed variable from main memory?
This is the only situation in the whole procedure where the type of access on the currently accessed variable matters.
In case that the access is of type load, it is required to load the previously written value from the main memory.
In case of a store on the currently accessed variable, its previously stored value does not matter any more because it is overwritten anyway.
For this reason, the expensive load instruction on the main memory can be skipped.

And finally, we are able to execute the actual access on the variable directly on the cache.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Benchmarks
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Benchmarks}\label{sec:benchmarks}

Our experiments are built on two benchmark suites, the \emph{SPEC 2006 Benchmarks}\cite{henning2006spec} and the \emph{V8 Benchmarks}\cite{v8benchmarks}. From both benchmark suites only a subset of the available benchmarks are used which are are explained below.

\subsection{SPEC 2006 Benchmarks}

This section describes the subset of benchmarks of the SPEC 2006 benchmark suite \cite{henning2006spec} which we used for our experiments. The benchmark descriptions below are taken from the SPEC 2006 paper.

\subsubsection{445.gobmk}

\begin{quote}
\begin{description}
\item[Authors:] (in chronological order of contribution) are Man Lung Li, Wayne Iba, Daniel Bump, David Denholm, Gunnar Farneb\"ack, Nils Lohner, Jerome Dumonteil, Tommy Thorn, Nicklas Ekstrand, Inge Wallin, Thomas Traber, Douglas Ridgway, Teun Burgers, Tanguy Urvoy, Thien-Thi Nguyen, Heikki Levanto, Mark Vytlacil, Adriaan van Kessel, Wolfgang Manner, Jens Yllman, Don Dailey, Mans Ullerstam, Arend Bayer, Trevor Morris, Evan Berggren Daniel, Fernando Portela, Paul Pogonyshev, S.P. Lee, Stephane Nicolet and Martin Holters. General

\item[General Category:] Artificial intelligence - game playing.

\item[Description:]\footnote{\url{www.gnu.org/software/gnugo/devel.html}} The program plays Go and executes a set of commands to analyze Go positions.
\end{description}
\end{quote}

\subsubsection{450.soplex}

\begin{quote}
\begin{description}
\item[Authors:] Roland Wunderling, Thorsten Koch, Tobias Achterberg

\item[General Category:] Simplex Linear Program (LP) Solver

\item[Description:] 450.soplex is based on SoPlex Version 1.2.1. SoPlex solves a linear program using the Simplex algorithm. The LP is given as a sparse m by n matrix A, together with a right hand side vector b of dimension m and an objective function coefficient vector c of dimension n. The matrix is sparse in practice. SoPlex employs algorithms for sparse linear algebra, in particular a sparse LU-Factorization and solving routines for the resulting triangular equation systems.
\end{description}
\end{quote}

\subsubsection{454.calculix}

\begin{quote}
\begin{description}
\item[Authors:] Guido D.C. Dhondt

\item[General Category:] Structural Mechanics

\item[Description:]\footnote{\url{www.calculix.de}} 454.calculix is based on CalculiX, a free software finite element code for linear and nonlinear three- dimensional structural applications. It uses classical theory of finite elements described in books such as \cite{zienkiewicz1977finite}. CalculiX can solve problems such as static problems (bridge and building design), buckling, dynamic applications (crash, earthquake resistance) and eigenmode analysis (resonance phenomena).
\end{description}
\end{quote}

\subsubsection{462 libquantum}

\begin{quote}
\begin{description}
\item[Authors:] Bj\"orn Butscher, Hendrik Weimer

\item[General Category:] Physics / Quantum Computing

\item[Description:]\footnote{\url{http://www.libquantum.de}} libquantum is a library for the simulation of a quantum computer. Quantum computers are based on the principles of quantum mechanics and can solve certain computationally hard tasks in polynomial time.
In 1994, Peter Shor discovered a polynomial-time algorithm for the factorization of numbers, a problem of particular interest for cryptanalysis, as the widely used RSA cryptosystem depends on prime factorization being a problem only to be solvable in exponential time. An implementation of Shor's factorization algorithm is included in libquantum.
Libquantum provides a structure for representing a quantum register and some elementary gates. Measurements can be used to extract information from the system. Additionally, libquantum offers the simulation of decoherence, the most important obstacle in building practical quantum computers. It is thus not only possible to simulate any quantum algorithm, but also to develop quantum error correction algorithms. As libquantum allows to add new gates, it can easily be extended to fit the ongoing research, e.g. it has been deployed to analyze quantum cryptography.
\end{description}
\end{quote}

\subsubsection{471 omnetpp}

\begin{quote}
\begin{description}
\item[Authors:] Andr\'as Varga, Omnest Global, Inc.

\item[General Category:] Discrete Event Simulation

\item[Description:] simulation of a large Ethernet network, based on the OMNeT++ discrete event simulation system\footnote{\url{www.omnetpp.org}}, using an ethernet model which is publicly available\footnote{\url{http://ctieware.eng.monash.edu.au/twiki/bin/view/Simulation/EtherNet}}.
For the reference workload, the simulated network models a large Ethernet campus backbone, with several smaller LANs of various sizes hanging off each backbone switch. It contains about 8000 computers and 900 switches and hubs, including Gigabit Ethernet, 100Mb full duplex, 100Mb half duplex, 10Mb UTP, and 10Mb bus. The training workload models a small LAN.
The model is accurate in that the CSMA/CD protocol of Ethernet and the Ethernet frame are faithfully modelled. The host model contains a traffic generator which implements a generic request-response based protocol. (Higher layer protocols are not modelled in detail.)
\end{description}
\end{quote}

\subsubsection{483 xalancbmk}

\begin{quote}
\begin{description}
\item[Authors:] IBM Corporation, Apache Inc, plus modifications for SPEC purposes by Christopher Cambly, Andrew Godbout, Neil Graham, Sasha Kasapinovic, Jim McInnes, June Ng, Michael Wong. Primary contact: Michael Wong

\item[General Category:] XSLT processor for transforming XML documents into HTML, text, or other XML document types

\item[Description:] a modified version of Xalan-C++\footnote{\url{http://xml.apache.org/xalan-c/}}, an XSLT processor written in a portable subset of C++ . Xalan-C++ version 1.8 is a robust implementation of the W3C Recommendations for XSL Transformations (XSLT)\footnote{\url{http://www.w3.org/TR/xslt}} and the XML Path Language (XPath)\footnote{\url{http://www.w3.org/TR/xpath}}. It works with a compatible release of the Xerces-C++\footnote{\url{http://xml.apache.org/xerces-c}} XML parser: Xerces-C++ version 2.5.0. The XSLT language is use to compose XSL stylesheets. An XSL stylesheet contains instructions for transforming XML documents from one document type to another document type (XML, HTML, or other). In structural terms, an XSL stylesheet specifies the transformation of one tree of nodes (the XML input) into another tree of nodes (the output or transformation result).
Modifications for SPEC benchmarking purposes include: combining code to make a standalone executable, removing compiler incompatibilities and improving standard conformance, changing output to display intermediate values, removing large parts of unexecuted code, and moving all the include locations to fit better into the SPEC harness.
\end{description}
\end{quote}

\subsection{V8 Benchmarks}

This section describes the subset of benchmarks of the V8 benchmark suite \cite{v8benchmarks} which we used for our experiments. The benchmark descriptions below are taken from the website.

\subsubsection{Richards}

\begin{quote}
\begin{description}
\item[Description:] OS kernel simulation benchmark, originally written in BCPL by Martin Richards\footnote{\url{http://www.cl.cam.ac.uk/~mr10/}} (539 lines).
\item[Main focus:] property load/store, function/method calls
\item[Secondary focus:] code optimization, elimination of redundant code
\end{description}
\end{quote}

\subsubsection{Raytrace}

\begin{quote}
\begin{description}
\item[Description:] Ray tracer benchmark based on code by Adam Burmister\footnote{\url{http://burmister.com}} (904 lines).
\item[Main focus:] argument object, apply
\item[Secondary focus:] prototype library object, creation pattern
\end{description}
\end{quote}

\subsubsection{Deltablue}

\begin{quote}
\begin{description}
\item[Description:] One-way constraint solver\footnote{\url{http://constraints.cs.washington.edu/deltablue/}}, originally written in Smalltalk by John Maloney and Mario Wolczko (880 lines).
\item[Main focus:] polymorphism
\item[Secondary focus:] OO-style programming
\end{description}
\end{quote}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Experiments}\label{cha:experiment}

This chapter presents experiments executed on our environment.
For this experiments the benchmarks presented by \Cref{sec:benchmarks} are analyzed.
The \ac{trace} of each benchmark is observed by using Valgrind Lackey.
During these experiments the \ac{trace} of each benchmark is applied to all five allocators available.
In this section the \ac{trace}s are named according to the applied allocator.
Hence, \emph{Identity} represents the \ac{trace} generated by the identity allocator, \emph{CompactStack} represents a compact \ac{trace} using a free list with stack semantic, \emph{CompactQueue} represents a compact \ac{trace} using a free list with queue semantic, \emph{CompactSet} represents a compact \ac{trace} using a free list with set semantic, and \emph{SingleAssignment} represents the \ac{trace} generated by the single assignment allocator.
Each allocator is executed on each cache introduced in \Cref{sec:caches}.
In the following section the caches are named \emph{\ac{MIN}}, \emph{\ac{MIN}+Liveness}, \emph{\ac{LRU}}, and \emph{\ac{LRU}+Liveness}.
The suffix \emph{+Liveness} indicates that the cache makes use of the liveness information of a \ac{trace}.
Further, all caches of this experiment are configured for cache size of 32KB and cache line size of 64 byte.
These values are chosen to represent a realistic cache.

\begin{remark}
When speaking about an allocator this is equivalent to speaking about the transformed \ac{trace} that results by applying this allocator.
\end{remark}

\section{Speedup \& Compaction}
\label{sec:exp-speedup-compaction}

This section presents the results about speedup and compaction of our allocators.
The speedup describes the relation of an allocators \ac{CPA} and the \ac{CPA} of the identity allocator.
The \ac{CPA} of all allocators is computed as the \Cref{equ:performance-cpa} illustrates.
The speedup is calculated as follows:
$$Speedup(Allocator, Cache) = \frac{CPA(Identity, Cache)}{CPA(Allocator, Cache)}$$

The compaction describes the memory used by a \ac{trace} in relation to the memory used by the identity allocator.
The number of variables used by an allocator on a given cache is indicated by $\#Addresses(Allocator, Cache)$.
The compaction is computed as follows:
$$%
Compaction(Allocator, Cache) = \frac{\#Addresses(Identity, Cache)}{\#Addressed(Allocator, Cache)}
$$

The figures below show the speedup on the y-axis and the compaction is presented on the x-axis.
Values greater than one represent an improvement and values less than one represent a worsening, in terms of performance or compaction.
For this reason we are aiming for values within the upper right corner because values in this area represent an improvement in performance and compaction.
Values at the lower left corner represent a worsening in performance and compaction; this area should be avoided.
The remaining two areas represent either an improvement in performance and a worsening in compaction or vice versa.

\begin{figure}[!ht]
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/correlation-MIN-CompactStack.eps}
    \caption{CompactStack}
    \label{fig:correlation-min-compactstack}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/correlation-MIN-CompactSet.eps}
    \caption{CompactSet}
    \label{fig:correlation-min-compactset}
  \end{subfigure}%
  \qquad
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/correlation-MIN-CompactQueue.eps}
    \caption{CompactQueue}
    \label{fig:correlation-min-compactqueue}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/correlation-MIN-SingleAssignment.eps}
    \caption{SingleAssignment}
    \label{fig:correlation-min-singleassignment}
  \end{subfigure}%
  \caption{Correlation of Speedup and Compaction: \ac{MIN}}
  \label{fig:correlation-min}
\end{figure}

\Cref{fig:correlation-min} illustrates the results of all allocators and benchmark combinations applied on the \ac{MIN} cache.

\Cref{fig:correlation-min-compactstack} shows the results of the CompactStack allocator in combination with the \ac{MIN} cache for all available benchmarks.
Obviously, applying the CompactStack allocator leads to an improvement in the number of used variables for all benchmarks.
Furthermore, most benchmarks present a better performance than the identity allocator.
Most interesting results are those of the 445.gobmk, 471.omnetpp, and 454.calculix.
The 445.gobmk benchmark presents the most significant speedup of all benchmarks.
Furthermore, it uses only the half of the variables used by the Identity allocator.
Nevertheless, there are benchmarks with a much better compaction.
The 471.omnetpp benchmark presents the best compaction result with nearly equivalent performance as the Identity allocator.
The 483.xalancbmk presents a speedup less than one with a compaction of approximately two.

\Cref{fig:correlation-min-compactset} presents the results of the CompactSet allocator in combination with the \ac{MIN} cache for all available benchmarks.
In comparison to the CompactStack allocators results, the results of the CompactSet allocator are slightly worse.
Especially, in terms of speedup there is a significant worsening observable.
The most outstanding benchmarks are again 445.gobmk, 471.omnetpp, and 454.calculix.
The results of 445.gobmk and 471.omnetpp are quite similar to those of the CompactStack.
The result of the 483.xalancbmk is clearly different to the result achieved by the CompactStack allocator.

\Cref{fig:correlation-min-compactqueue} presents the results of the CompactQueue allocator in combination with the \ac{MIN} cache for all available benchmarks.
Obviously, using the CompactQueue allocator is not beneficial for most benchmarks in terms of speedup.
Nevertheless, the 445.gobmk benchmark still achieved a performance improvement with a similar compaction to the CompactStack and CompactSet allocators.
Also the 471.omnetpp benchmark presents quite identical results for speedup and compaction.
All other benchmarks show a decrease in speedup.
Furthermore, for this allocator 454.calculix is not the allocator with the worst speedup, it is 483.xalancbmk.

\Cref{fig:correlation-min-singleassignment} presents the results of the SingleAssignment allocator in combination with the \ac{MIN} cache for all available benchmarks.
Without any detailed explanation it is clear that the SingleAssignment allocator presents the worst results of all four allocators.
Obviously, in terms of compaction this allocator is not able to achieve any improvement by definition.
Using a new variable for each store instruction inevitably yields in more or at least the same number of variable than used by the Identity allocator.
As expected, all data points are within the left half of the figure.
Unfortunately, none of the benchmarks are able to least achieve the same performance as the Identity allocator.
Only the 445.gobmk benchmark is close to one.
Note that the 471.omnetpp benchmark, which achieved the best compaction for the other three allocators, now presents the worst compaction.
And additionally, it also offers the worst speedup.

\begin{figure}[!ht]
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/correlation-MIN+Liveness-CompactStack.eps}
    \caption{CompactStack}
    \label{fig:correlation-min-liveness-compactstack}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/correlation-MIN+Liveness-CompactSet.eps}
    \caption{CompactSet}
    \label{fig:correlation-min-liveness-compactset}
  \end{subfigure}%
  \qquad
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/correlation-MIN+Liveness-CompactQueue.eps}
    \caption{CompactQueue}
    \label{fig:correlation-min-liveness-compactqueue}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/correlation-MIN+Liveness-SingleAssignment.eps}
    \caption{SingleAssignment}
    \label{fig:correlation-min-liveness-singleassignment}
  \end{subfigure}%
  \caption{Correlation of Speedup and Compaction: \ac{MIN}+Liveness}
  \label{fig:correlation-min-liveness}
\end{figure}

\Cref{fig:correlation-min-liveness} illustrates the results of all benchmarks and allocator combinations applied on the \ac{MIN}+Liveness cache.
According to the proceeding presented by \Cref{ssec:cache-behavior-liveness}, the compaction is not influenced by using the liveness information.
For this reason we can focus on performance while comparing the caches based on \citeauthor{belady1966study}s algorithm (\ac{MIN}).

\Cref{fig:correlation-min-liveness-compactstack} presents the results of the CompactStack allocator in combination with the \ac{MIN}+Liveness cache for all benchmarks.
The relations between the benchmarks are quite similar to those of the \ac{MIN} cache.
As mentioned above compaction is not influenced by using the liveness information.
This is why the 471.omnetpp benchmark still shows the most compaction and 445.gobmk is still one of the benchmarks with the least improvement in compaction.
Even with the least improvement the 445.gobmk benchmark uses only half of the addresses required by the Identity allocator.
In terms of performance the 445.gobmk benchmark remains as the benchmarks with the most speedup.
In general, the speedup for the \ac{MIN}+Liveness cache is slightly worse than for the \ac{MIN} cache.
The reason is that also the performance of the Identity allocator improves by using the \ac{MIN}+Liveness cache.
Hence, the gap shrinks and the speedup decreases.

\Cref{fig:correlation-min-liveness-compactset} presents the results of the CompactSet allocator in combination with the \ac{MIN}+Liveness cache for all benchmarks.
For the CompactSet allocator the differences to the \ac{MIN} cache without liveness information are hard to see on this figure.
The details for the benchmarks 445.gobmk, 471.omnetpp, and 483.xalancbmk are presented by \Cref{fig:speedup-compaction-445-gobmk,fig:speedup-compaction-471-omnetpp,fig:speedup-compaction-483-xalancbmk}.

\Cref{fig:correlation-min-liveness-compactqueue} presents the results of the CompactQueue allocator in combination with the \ac{MIN}+Liveness cache for all benchmarks.
Unfortunately, the \ac{MIN}+Liveness cache is not able to push the results up into the right upper corner.
Hence, the benchmarks performance is worse using the CompactionQueue allocator than using the Identity allocator.
Nevertheless, there is an improvement observable in comparison it the \ac{MIN} cache.

\Cref{fig:correlation-min-liveness-singleassignment} presents the results of the SingleAssignment allocator in combination with the \ac{MIN}+Liveness cache for all benchmarks.
The usage of liveness information has an positive influence on the speedup of the benchmarks.
Nevertheless, the influence is too small to push the results into an area which is more beneficial.

\begin{figure}[!ht]
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/correlation-LRU-CompactStack.eps}
    \caption{CompactStack}
    \label{fig:correlation-lru-compactstack}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/correlation-LRU-CompactSet.eps}
    \caption{CompactSet}
    \label{fig:correlation-lru-compactset}
  \end{subfigure}%
  \qquad
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/correlation-LRU-CompactQueue.eps}
    \caption{CompactQueue}
    \label{fig:correlation-lru-compactqueue}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/correlation-LRU-SingleAssignment.eps}
    \caption{SingleAssignment}
    \label{fig:correlation-lru-singleassignment}
  \end{subfigure}%
  \caption{Correlation of Speedup and Compaction: LRU}
  \label{fig:correlation-lru}
\end{figure}

\Cref{fig:correlation-lru} illustrates the results of all benchmarks and allocator combinations applied on the \ac{LRU} cache.
Obviously, the algorithm that decided which item is evicted has an enormous influence.

\Cref{fig:correlation-lru-compactstack} presents the results of the CompactStack allocator in combination with the \ac{LRU} cache for all benchmarks.
This figure presents a quite different result as shown by \Cref{fig:correlation-min-compactstack}.
Overall the speedup is much worse for most of the benchmark in comparison to the \ac{MIN} caches.
Nevertheless, there are still three outstanding benchmarks to discuss.
(1) The 471.omnetpp benchmark remains the benchmark with the most compaction.
(2) The 483.xalancbmk benchmark presents the highest decrease in performance.
For the \ac{MIN} caches this benchmark already shows a poor speedup, but with the \ac{LRU} cache it becomes the benchmark with the least speedup.
(3) The 445.gobmk benchmark and the 462.libquantum benchmark that present the best performance.
As for the \ac{MIN} caches the 445.gobmk benchmark remains the benchmark with the highest speedup.
Furthermore, the 462.libquantum benchmark shows a significant improvement in speedup to the \ac{LRU} cache.

\Cref{fig:correlation-lru-compactset} presents the results of the CompactSet allocator in combination with the \ac{LRU} cache for all benchmarks.
In an overall perspective also the CompactSet allocator is not able to achieve that much speedup using the \ac{LRU} cache compared to a \ac{MIN} cache.
Most of the benchmarks present a worsening in performance which results in less speedup.
Nevertheless, two benchmarks present a speedup larger than one 445.gobmk and 462.libquantum, namely.
The 485.xalancbmk presents again the least speedup of all benchmarks and the 471.omnetpp remains the benchmark with most compaction.

\Cref{fig:correlation-lru-compactqueue} presents the results of the CompactQueue allocator in combination with the \ac{LRU} cache for all benchmarks.
The CompactQueue allocator presents a similar behavior as the CompactStack and CompactSet allocators.
Most of the data points show a worsening in speedup.
In an overall perspective the CompactQueue allocator has the worst speedup results of all four allocators.
In detail there are two quite interesting things to observe.
First, the 445.gobmk benchmark shows a speedup less than one, while 462.libquantum remains the only benchmark with a speedup greater than one.
Secondly 483.xalancbmk is not the benchmark with the worst speedup for this allocator.
The benchmark with the worst speedup is the Richards benchmark.

\Cref{fig:correlation-lru-singleassignment} presents the results of the SingleAssignment allocator in combination with the \ac{LRU} cache for all benchmarks.
As expected, the results of the SingleAssignment allocator occur in the left lower corner.
In comparison to the \ac{MIN}+Liveness cache the speedups vary more.
Unexpectedly the 445.gobmk presents a speedup greater than one for this allocator for the first time.

\begin{figure}[!ht]
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/correlation-LRU+Liveness-CompactStack.eps}
    \caption{CompactStack}
    \label{fig:correlation-lru-liveness-compactstack}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/correlation-LRU+Liveness-CompactSet.eps}
    \caption{CompactSet}
    \label{fig:correlation-lru-liveness-compactset}
  \end{subfigure}%
  \qquad
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/correlation-LRU+Liveness-CompactQueue.eps}
    \caption{CompactQueue}
    \label{fig:correlation-lru-liveness-compactqueue}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/correlation-LRU+Liveness-SingleAssignment.eps}
    \caption{SingleAssignment}
    \label{fig:correlation-lru-liveness-singleassignment}
  \end{subfigure}%
  \caption{Correlation of Speedup and Compaction: LRU+Liveness}
  \label{fig:correlation-lru-liveness}
\end{figure}

\Cref{fig:correlation-lru-liveness} illustrates the results of all benchmarks and allocator combinations applied on the \ac{LRU}+Liveness cache.

\Cref{fig:correlation-lru-liveness-compactstack} presents the results of the CompactStack allocator in combination with the \ac{LRU}+Liveness cache for all benchmarks.
The presented speedup results are quite similar to those of the \ac{LRU} cache without liveness information.
Nevertheless, one thing is still worth mentioning, the 462.libquantum has significantly improved in terms of speedup.
It reaches a speedup close 445.gobmk benchmark.

\Cref{fig:correlation-lru-liveness-compactset} presents the results of the CompactSet allocator in combination with the \ac{LRU}+Liveness cache for all benchmarks.
The CompactSet allocator shows only minimal improvements as the CompactStack allocator.
Except the 462.libquantum benchmark which improves its speedup significantly.
All other benchmarks remain within the right left corner as before.
This indicates a good compaction and a poor speedup.

\Cref{fig:correlation-lru-liveness-compactqueue} presents the results of the CompactQueue allocator in combination with the \ac{LRU}+Liveness cache for all benchmarks.
For the CompactQueue allocator in combination with the \ac{LRU} algorithm as base of the eviction policy is seems that the liveness information has no significant influence.

\Cref{fig:correlation-lru-liveness-singleassignment} presents the results of the SingleAssignment allocator in combination with the \ac{LRU}+Liveness cache for all benchmarks.
Similar to the CompactQueue allocator and also for the SingleAssignment allocator is seems that the liveness information does not improve the speedup significantly.

\subsection{445.gobmk}

In this section we take a closer look at the speedup and compaction of the 445.gobmk benchmark.
\Cref{fig:speedup-compaction-445-gobmk-speedup} presents an overview of the speedup results of the 445.gobmk benchmark of all allocators and caches applied.

A speedup below indicates that the performance of this allocator is worse than the performance of the Identity allocator by using the same cache.
In case of the 445.gobmk there are 4 such scenarios in which an allocator performance is worse than the Identity allocator.

For the \ac{MIN} cache the SingleAssignment allocators presents a speedup of $0.71$ which means that its performance is 29\% slower in comparison to the Identity allocator.
For the \ac{MIN}+Liveness cache also the SingleAssignment allocator has the worst performance.
As expected the usage of the liveness information yields an improvement, the performance decreases by 14\%.
Because the SingleAssignment allocator uses many more variables than the other allocators its results are not surprising.

For the more realistic cache implementations based on the \ac{LRU} algorithm the SingleAssignment allocator performs better and presents a speedup above one.
Nevertheless, there is another allocator which does not perform well on the \ac{LRU} caches, the CompactQueue.
The CompactQueue allocator shows a speedup of $0.29$ for the \ac{LRU} cache and $0.34$ for the \ac{LRU} cache using liveness information.
Again the liveness information has a significant influence.
Nevertheless, the performance of the CompactQueue allocator is 64\%-71\% worse than the performance of the Identity allocator.

However, the CompactStack allocator is the allocator with the highest speedup for each cache.
The most impressive speedup is shown for the \ac{LRU} cache, in this scenario the CompactStack allocator reaches a speedup of $4.37$.

The compaction shown by \Cref{fig:speedup-compaction-445-gobmk-compaction} is identical for all compacting allocators, which is as expected according to the workflow presented by \Cref{fig:cache-behavior-liveness}.
It is worth mentioning that all three compacting allocators are able to use $2.78$ times less variables than the Identity allocator does.
Furthermore, it is not surprising that the SingleAssignment allocators requires that many more variables than the Identity allocator.

\begin{figure}[!ht]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/speedup-445-gobmk.eps}
    \subcaption{Speedup}
  \label{fig:speedup-compaction-445-gobmk-speedup}
  \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/compaction-445-gobmk.eps}
    \subcaption{Compaction}
  \label{fig:speedup-compaction-445-gobmk-compaction}
  \end{subfigure}%
  \caption{Speedup \& Compaction: 445.gobmk}
  \label{fig:speedup-compaction-445-gobmk}
\end{figure}

\subsection{471.omnetpp}

\Cref{fig:speedup-compaction-471-omnetpp} presents the results of the 471.omnetpp benchmark in detail.

\Cref{fig:speedup-compaction-471-omnetpp-speedup} shows the speedup of all allocators and caches of the 471.omnetpp benchmark.
Obviously, the speedup results of the 471.omnetpp benchmark are significant worse than those of the 445.gobmk benchmark.
Just for the two caches \ac{MIN} and \ac{MIN}+Liveness the 471.omnetpp benchmark is able to achieve a speedup greater than one at all.
And for these caches only the compacting allocators are slightly greater than one.
The results for the \ac{LRU} caches are dramatically worse.
For the \ac{LRU} caches, none of the allocators are able to achieve a speedup close to one.
However, the CompactionStack allocator presents the best results of all allocators and again the CompactQueue allocators shows the worst results.

\Cref{fig:speedup-compaction-471-omnetpp-compaction} presents the compaction of the 471.omnetpp benchmark.
As the previous figures suggested, the compaction of the 471.omnetpp benchmark is extremely high.
All compacting allocators require $39.76$ times less variables than the Identity allocator does.
Furthermore, the compaction of the SingleAssignment allocator is quite interesting, because it indicates that the \ac{trace} consists of many store instructions which lead to a compaction of $0.004$.

\begin{figure}[!ht]
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/speedup-471-omnetpp.eps}
    \subcaption{Speedup}
    \label{fig:speedup-compaction-471-omnetpp-speedup}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/compaction-471-omnetpp.eps}
    \subcaption{Compaction}
    \label{fig:speedup-compaction-471-omnetpp-compaction}
  \end{subfigure}%
  \caption{Speedup \& Compaction: 471.omnetpp}
  \label{fig:speedup-compaction-471-omnetpp}
\end{figure}

\subsection{483.xalancbmk}

\Cref{fig:speedup-compaction-483-xalancbmk} presents the results of the 483.xalancbmk benchmark in detail.

\Cref{fig:speedup-compaction-483-xalancbmk-speedup} shows the speedup results of the 483.xalancbmk.
Unfortunately, the presented results show only two scenarios for which a speedup greater than one can be achieved.
Namely, this scenarios are the CompactStack allocator in combination with the \ac{MIN} cache and the CompactStack allocator in combination with the \ac{MIN}+Liveness cache.
The CompactSet allocator is close to one but stays below even if a cache uses the liveness information.
On the \ac{LRU} caches, none of the presented allocators is able to achieve a speedup above $0.15$.
Which means that all allocators show a performance at least 85\% worse than the Identity allocators performance.

However, \Cref{fig:speedup-compaction-483-xalancbmk-compaction} presents quite good results for the compaction.
The compacting allocators are able to use $7.14$ times less variables than the Identity allocators does.

\begin{figure}[!ht]
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/speedup-483-xalancbmk.eps}
    \subcaption{Speedup}
    \label{fig:speedup-compaction-483-xalancbmk-speedup}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/compaction-483-xalancbmk.eps}
    \subcaption{Compaction}
    \label{fig:speedup-compaction-483-xalancbmk-compaction}
  \end{subfigure}%
  \caption{Speedup \& Compaction: 483.xalancbmk}
  \label{fig:speedup-compaction-483-xalancbmk}
\end{figure}

\section{Performance}\label{sec:experiment-performance}

This section presents the performance of the benchmarks.
The performance is computed as explained by \Cref{sec:performance}.
All figures of this section show the \ac{CPA} on the y-axis, computed as illustrated by \Cref{equ:performance-cpa}.
On the x-axis the allocators are shown in groups of the applied cache.
Additional to the performance we take a look at the distribution of cache misses and caches hits and influences of the different kinds of memory accesses.
By now we focus on the benchmarks namely \emph{445.gobmk}, \emph{471.omnetpp}, and \emph{483.xalancbmk}.
The remaining figures can be found at \Cref{app:experiment}.
For each benchmark two figures are presented.
Both figures show the performance in \ac{CPA}, but the figure on the left hand-side additionally presents the distribution of cache misses and cache hits and the figure on the right hand-side illustrates the different kinds of memory accesses.

\subsection{445.gobmk}
\Cref{fig:performance-445-gobmk} presents the results of the 445.gobmk benchmark.
For each group of allocators, e.g., the five allocators Identity, CompactStack, CompactSet, CompactQueue, and SingleAssignment of the \ac{LRU} group, the results of Identity represent the baseline.
Lets stick with the first group.
The baseline is a CPA of $15.22$.
The allocators CompactStack, CompactSet, and SingleAssignment perform better than the Identity.
This means the performance of the original \ac{trace} can offer potential improvement, as shown by these three allocators.
However, not all of them show an improvement.
The CompactQueue allocators presents a dramatically worse performance than Identity.
Nevertheless, we observe that the \ac{CPA} of the CompactQueue presented by the \ac{LRU} cache is the worst.
For the other cache implementations its performance is significant better.
To describe it in more detail the CompactQueue allocator performs much better on the MIN caches than on the \ac{LRU} caches.
For the MIN caches the performance of the SingleAssignment approach is the worst.
The CompactStack allocator is best on all cache implementations.
This nicely illustrates the influence of the underlining cache.
It seems that all allocators benefit from the usage of liveness information.
Not surprisingly the MIN cache is beneficial for all allocators.
Taking to account the shown cache misses and cache hits, the poor performance of the CompactQueue allocator is not that surprising anymore.
Obviously, the CompactQueue implementation yields the most cache misses, e.g., for the \ac{LRU} cache.
Which in this case leads to more main memory accesses as illustrated by \Cref{fig:performance-445-gobmk-memops}.
For all other allocators there much less main memory loads and main memory stores.
Summarizing the \Cref{fig:performance-445-gobmk} illustrates that there is potential for performance improvement of the 445.gobmk benchmark.

\begin{figure}[!ht]
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/perf-misses-445-gobmk.eps}
    \subcaption{Cache misses and cache hits}
    \label{fig:performance-445-gobmk-misses}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/perf-445-gobmk.eps}
    \subcaption{Types of memory operations}
    \label{fig:performance-445-gobmk-memops}
  \end{subfigure}%
  \caption{Performance: 445.gobmk}
  \label{fig:performance-445-gobmk}
\end{figure}

\subsection{471.omnetpp}
\Cref{fig:performance-471-omnetpp} presents the performance results of the 471.omnetpp benchmark.
As before both figures illustrate the performance in \ac{CPU}, but on the left hand-side the distribution of cache misses and cache hits is shown and on the right hand-side the relation of the different memory accesses is presented.
For this benchmark the Identity results are much better than the results of the 445.gobmk benchmark.
Since the Identity \ac{CPA}s are quite close to 1 ($1.85$, $1.82$, $1.04$, and $1.03$), there is not much space for improvement, especially for the \ac{MIN} implementations.
Nevertheless, only for the \ac{MIN} implementation we are able to achieve an improvement with the allocators CompactStack, CompactSet, and CompactQueue.
The approach without compaction, SingleAssignment, seems to be a bad choice for this benchmark, because for all four cache implementations it shows poor performance.
As before it is observable that those allocators with less cache misses end up with less main memory accesses.
What is plausible although a cache miss does not necessarily force a main memory access, as \Cref{fig:cache-behavior-liveness} illustrates.
According to the work flow shown by \Cref{fig:cache-behavior-liveness} the hypothesis raises that the 471.omnetpp benchmark consists of many variables with overlapping liveness intervals or this benchmark is very load intensive or both aspects are more pronounced compared to 445.gobmk.


\begin{figure}[!ht]
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/perf-misses-471-omnetpp.eps}
    \subcaption{Cache misses and cache hits}
    \label{fig:performance-471-omnetpp-misses}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/perf-471-omnetpp.eps}
    \subcaption{Types of memory operations}
    \label{fig:performance-471-omnetpp-memops}
  \end{subfigure}%
  \caption{Performance: 471.omnetpp}
  \label{fig:performance-471-omnetpp}
\end{figure}

\subsection{483.xalancbmk}
\Cref{fig:performance-483-xalancbmk} illustrates the performance of the 483.xalancbmk.
The most obvious observation is that the performance gap of the \ac{LRU} cache implementations and the \ac{MIN} implementations is dramatical.
Hence, the \ac{LRU} cache is not the optimal cache for our compacting allocators.
In a worst case our transformations yield approximately a 10 times worse \ac{CPA} than the Identity.
The source of this poor performance seems to be that the compaction increases the number of cache misses.
Unfortunately, many of the cache misses yield a main memory access, as illustrated by \Cref{fig:performance-483-xalancbmk-memops}.
Peeking CompactQueue, as an outstanding example the number of cache misses is nearly identical with the number of main memory access which leads to its poor performance.
As expected, the implementation of the \ac{LRU} cache which uses the liveness information is able archive a better \ac{CPA} for all allocators.
But even those results are far from good.
However, the results for the \ac{MIN} caches are much better than those of the \ac{LRU} caches.
Even though the performance of the \ac{MIN} caches is much better; there is only one allocator which is able to improve the performance in comparison to the Identity allocator namely CompactStack.
CompactSet is close to the original performance but the other two CompactQueue and SingleAssignment do not perform as good.

\begin{figure}[!ht]
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/perf-misses-483-xalancbmk.eps}
    \subcaption{Cache misses and cache hits}
    \label{fig:performance-483-xalancbmk-misses}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/plots/perf-483-xalancbmk.eps}
    \subcaption{Types of memory operations}
    \label{fig:performance-483-xalancbmk-memops}
  \end{subfigure}%
  \caption{Performance: 483.xalancbmk}
  \label{fig:performance-483-xalancbmk}
\end{figure}

\section{Statistical Analysis}
\label{sec:experiment-statistical-analysis}

This section presents the statistical analysis on the benchmarks according to the metrics presented in \Cref{sec:metrics}.
The metrics presented are observed from the Identity allocator.
More details and further tables can be found in \Cref{app:experiment}.

\begin{table}[!ht]
  \centering
  \begin{tabular}[c]{lrrrr}
    \hline
    Benchmark & Size [MB] & Number of Instructions & Loads [\%] & Stores [\%] \tabularnewline
    \hline
    445.gobmk      & 1781.76 &  \num{186740466} & 50.20 & 49.80 \tabularnewline
    450.soplex     &  196.29 &   \num{20582523} & 27.90 & 72.10 \tabularnewline
    454.calculix   &  505.71 &   \num{53027458} & 31.93 & 68.07 \tabularnewline
    462.libquantum &  945.71 &   \num{99165026} & 14.28 & 85.72 \tabularnewline
    741.omnetpp    & 8069.12 & \num{8459396786} & 39.40 & 60.60 \tabularnewline
    483.xalancbmk  & 1269.76 &  \num{133660557} & 25.47 & 74.53 \tabularnewline
    richards       &  790.51 &   \num{82890768} & 24.90 & 75.10 \tabularnewline
    raytrace       &  591.43 &   \num{62015988} & 35.33 & 64.67 \tabularnewline
    deltablue      & 2590.72 &  \num{272165756} & 25.62 & 74.38 \tabularnewline
    \hline
  \end{tabular}
  \caption{Basic benchmark data}
  \label{tab:basic-benchmark-data}
\end{table}

\Cref{tab:basic-benchmark-data} shows the most basic data for all analyzed benchmarks.
The number of instructions varies according to the size of a benchmark.
The column \emph{Number of Instructions} shows the total number of load and store instructions.
The columns \emph{Loads} and \emph{Store} present the distribution of loads and stores shown as percentage.
Most of the listed benchmarks are store-intensive and consist of more store instructions than load instructions.
Except the 445.gobmk benchmark that presents a distribution of $50.20$ percent loads and $49.8$ percent stores.
462.libquantum consists of $85.72$ percent stores which is the maximum for all listed benchmarks.

\begin{table}[!ht]
  \centering
  \begin{tabular}{lrrr}
    \hline
     & 445.gobmk & 471.omnetpp & 483.xalancbmk\\
    \hline
    \csvreader[late after line=\\]{figs/statistical-analysis-data/prepared-address-accesses-3.csv}%
    {1=\collabel, 2=\colgobmk, 3=\colomnetpp, 4=\colxalancbmk}%
    {\collabel & \num{\colgobmk} & \num{\colomnetpp} & \num{\colxalancbmk}}%
    \hline
  \end{tabular}
  \caption{Metric: Accesses}
  \label{tab:metric-accesses-3}
\end{table}

\Cref{tab:metric-accesses-3} presents the results of the access distance for the benchmarks 445.gobmk, 471.omnetpp, and 483.xalancbmk.
\Cref{tab:metric-accesses-all} presents the results for all benchmarks.
\emph{Count} represents the number of different used variables.
The arithmetic mean shows that the variables of 445.gobmk are accessed $111.56$ times in average, variables of 471.omnetpp are accessed $859.89$ times in average, and variables of 483.xalancbmk are accessed $155.57$ times in average.
Clearly the variables of the 471.omnetpp benchmarks are accessed most often.
\emph{Minimum} and \emph{maximum} show that at least one variable exists that is accessed only once and that there is at least on variable which is accessed $12057070$, $19444797$, and $2416458$ for the benchmarks 445.gobmk, 471.omnetpp, and 483.xalancbmk.
Investigating the results of the percentiles shows that the \emph{average} is misleading.
For the 445.gobmk benchmark, the 95\% percentile presents a value of $23$.
Hence, 95\% of all variables are accessed at most 23 times.
This indicates that the remaining 5\% of the variables are accessed significantly more often.
Otherwise the \emph{average} would not result in a value that high.
These values indicate that half of the variables are accessed with a high frequency.
The 25\% percentile shows that the variables which are accessed with an even higher frequency.
The 445.gobmk benchmark presents a 25\% percentile of $3$.
On the opposite, the results of the 445.gobmk benchmark for the 95\% percentile illustrate that 5\% of the variables used are accessed with large distances.
For the benchmarks 471.omnetpp and 483.xalancbmk the 95\% percentile is significantly larger then the 75\% percentile that shows the same behavior as for the 445.gobmk benchmark.
However, the increase is significantly less than for the 445.gobmk.
The 471.omnetpp benchmark presents a similar behavior as the 445.gobmk benchmark.
The most significant difference is that the variables are, in general, accessed more often.
This is for example observable from the 50\% percentile.
The 483.xalancbmk shows a significant gap between the 75\% percentile and the 95\% percentile.
This indicating that 25\% of the used variables are accessed significantly more often that the others.

\begin{table}[!ht]
  \centering
  \begin{tabular}{lrrr}
    \hline
     & 445.gobmk & 471.omnetpp & 483.xalancbmk\\
    \hline
    \csvreader[late after line=\\]{figs/statistical-analysis-data/prepared-access-distances-3.csv}%
    {1=\collabel, 2=\colgobmk, 3=\colomnetpp, 4=\colxalancbmk}%
    {\collabel & \num{\colgobmk} & \num{\colomnetpp} & \num{\colxalancbmk}}%
    \hline
  \end{tabular}
  \caption{Metric: Access Distance}
  \label{tab:metric-access-distance-3}
\end{table}

\Cref{tab:metric-access-distance-3} presents the results about the access distance for the benchmarks 445.gobmk, 471.omnetpp, and 483.xalancbmk.
\Cref{tab:metric-access-distance-all} presents the results for all benchmarks.
\emph{Count} represents the number of different access distances observed for a benchmark.
This table shows a large gap between the \emph{minimum} and the \emph{maximum} access distance.
The table show that the \emph{minimum} is $1$ for all benchmarks that indicates that at least one variable is accessed twice without accessing another variable in between.
The \emph{maximum} indicates that there is at least one variable for all three benchmarks which is accessed rarely.
The \emph{average} access distance is significantly smaller than the \emph{maximum}.
The percentiles offer a deeper knowledge about the actual access distances.
The $50$\% percentile is identical to the \emph{median}.
It shows for the 445.gobmk benchmark that $50$\% of the used variables have an access distance of less equal to $73$.
That is several magnitudes less than the average.
For the other two benchmarks the situation is similar, 471.omnetpp presents an 50\% percentile of $41$ and the 483.xalancbmk show a value of $394$.

\begin{table}[!ht]
  \centering
  \begin{tabular}{lrrr}
    \hline
     & 445.gobmk & 471.omnetpp & 483.xalancbmk\\
    \hline
    \csvreader[late after line=\\]{figs/statistical-analysis-data/prepared-live-addresses-3.csv}%
    {1=\collabel, 2=\colgobmk, 3=\colomnetpp, 4=\colxalancbmk}%
    {\collabel & \num{\colgobmk} & \num{\colomnetpp} & \num{\colxalancbmk}}%
    \hline
  \end{tabular}
  \caption{Metric: Overlapping Liveness}
  \label{tab:metric-overlapping-liveness-3}
\end{table}

\Cref{tab:metric-overlapping-liveness-3} presents the results about the access distance for the benchmarks 445.gobmk, 471.omnetpp, and 483.xalancbmk.
\Cref{tab:metric-overlapping-liveness-all} presents the results for all benchmarks.
\emph{Count} represents the number of overlapping liveness intervals.
The table shows that for the 445.gobmk benchmark at least $67$ variables are live at the same time, and respectively $83$ at the 471.omnetpp, and $75$ at the 283.xalancbmk.
The 5\% percentile for all three benchmarks shows that there are significantly more variables live at the same time during the execution.
This indicates that the working set of these three benchmarks definitely does not fit into the cache for this experiment.

\begin{table}[!ht]
  \centering
  \begin{tabular}{lrrr}
    \hline
     & 445.gobmk & 471.omnetpp & 483.xalancbmk\\
    \hline
    \csvreader[late after line=\\]{figs/statistical-analysis-data/prepared-liveness-length-3.csv}%
    {1=\collabel, 2=\colgobmk, 3=\colomnetpp, 4=\colxalancbmk}%
    {\collabel & \num{\colgobmk} & \num{\colomnetpp} & \num{\colxalancbmk}}%
    \hline
  \end{tabular}
  \caption{Metric: Liveness Interval Length}
  \label{tab:metric-liveness-length-3}
\end{table}

\Cref{tab:metric-liveness-length-3} presents the results about the access distance for the benchmarks 445.gobmk, 471.omnetpp, and 483.xalancbmk.
\Cref{tab:metric-liveness-interval-length-all} presents the results for all benchmarks.
\emph{Count} represents the number of different liveness interval lengths.
The results of the 445.gobmk benchmark indicate that most variables have short liveness intervals.
This is indicated by the percentiles 5\%, 25\%, 50\%, and 75\%, which all show value $0$.
In this context $0$ means that a variable is live for exactly one access.
Hence, 75\% of the variables of the 445.gobmk benchmark are accessed only once.
By the delta of the 75\% percentile and 95\% percentile we know that 20\% percent of the used variables are have a liveness interval of a length up to $156$.
In combination with the fact that all variables used accumulate to an average liveness interval length of $1402011.75$, this implies that 5\% of the variables used by 445.gobmk consists of a significantly longer liveness interval.
The results of the 471.omnetpp benchmark and the 483.xalancbmk benchmark indicate that there is a small amount of variables that are significantly longer than the majority of the used variables.

\Cref{fig:stats-overview} presents the results of all statistical metrics of all benchmarks.
Note that the y-axis is logarithmic and the x-axis is linear.

\Cref{fig:stats-liveness-interval-length} presents the liveness interval lengths on the y-axis.
On the x-axis the liveness interval lengths of all variables are shown in decreasing order.
% 445.gobmk
As illustrated by \Cref{fig:stats-liveness-interval-length}, over 90\% of the liveness intervals of the 445.gobmk are of length 1 and less than 5\% are of a length longer than 5.
% 462.libquantum
Approximately 20\% of the liveness intervals of the 462.libquantum benchmark are of a length greater equal 5, less than 10\% are of a length greater or equal 100, and over 65\% of the liveness intervals are of length 1.
% 471.omnetpp
The 471.omnetpp benchmarks consists of approximately 5\% of liveness intervals with a length greater 100, slightly more than 10\% are of a length great than 1, and slightly less than 90\% of the liveness intervals are of length 1.
% 483.xalancbmk
Approximately 20\% of the liveness intervals of the 483.xalancbmk benchmark are of a length greater than 1 and approximately 5\% of the liveness intervals are of a length greater than 2

\Cref{fig:stats-overlapping-liveness} shows the number of overlapping liveness intervals on the y-axis.
% 445.gobmk
The 445.gobmk benchmark presents the most overlapping liveness intervals.
Different to the other benchmarks the shape of the 445.gobmk benchmark decreases with several steps.
% 462.libquantum
The 462.libquantum benchmark is one of the benchmarks with the fewest overlapping liveness intervals.
The number of liveness intervals reduces linearly.
% 471.omnetpp
The 471.omnetpp benchmark consists of significantly more overlapping liveness intervals than all other benchmarks except the 445.gobmk benchmark.
In difference to the 445.gobmk benchmark the number of overlapping liveness intervals of the 471.omnetpp benchmark decreases linearly.
% 483.xalancbmk
The 483.xalancbmk benchmark is one of the benchmarks with the fewest overlapping liveness intervals. The number of overlapping liveness intervals decreases linear.

\Cref{fig:stats-accesses} shows the number of accesses at a variable on the y-axis.
On the x-axis the values of all variables are shown in a decreasing order.
% 445.gobmk
Approximately 5\% of the variables of the 445.gobmk benchmark are accessed more often than 20 times, almost 3\% are accessed less than 20 times, and the remaining $\sim$92\% of the variables are accessed exactly 20 times.
% 462.libquantum
For the 462.libquantum benchmark approximately 35\% of the variables are accessed once, $\sim$15\% are accessed 2 times, almost 40\% of the variables are accessed more often than 2 times and less often than 200 times, and less than 10\% are accessed over 200 times.
% 471.omnetpp
10\% of the 471.omnetpp benchmarks variables are accessed less than 20 times, approximately 85\% of the variables are accessed between 20 and 400 times, and the remaining around 5\% of the variables are accessed over 400 times.
% 483.xalancbmk
Almost 45\% of the variables of the 483.xalancbmk benchmark are accessed at most 10 times, approximately 30\% of the variables are accessed between 10 and 20 times, almost 15\% are accessed between 20 and 100 times, and the remaining $\sim$ 10\% are accessed more often than 100 times.

\Cref{fig:stats-access-distance} illustrates the access distances on the y-axis and on the x-axis all observed values are shown in decreasing order.
% 445.gobmk
Only approximately 35\% of the access distances of the 445.gobmk benchmark are greater than 1, less than 10\% are greater than 10, and just a few percentage of the access distances are greater than 100.
% 462.libquantum
Over 50\% of the access distances of the 462.libquantum benchmark are greater than 1, almost 32\% are greater than 5, less than 5\% of the access distances are greater than 100, and less than 3\% are even greater then 1000.
% 471.omnetpp
Slightly less than 40\% of the access distances are greater than 1, approximately 35\% are greater than 3, and only less than 20\% are greater than 3.
% 483.xalancbmk
Less than 30\% of the access distances are greater than 1, approximately 20\% are of access distance 2, and roughly 2\% of the access distances are greater than 10.

\begin{figure}[!ht]
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/statistical-analysis/plots/ll.eps}
    \subcaption{Liveness Interval Length}
    \label{fig:stats-liveness-interval-length}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/statistical-analysis/plots/ol.eps}
    \subcaption{Overlapping Liveness Intervals}
    \label{fig:stats-overlapping-liveness}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/statistical-analysis/plots/aa.eps}
    \subcaption{Accesses}
    \label{fig:stats-accesses}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\textwidth}%
    \includegraphics[width=\textwidth]{figs/statistical-analysis/plots/ad.eps}
    \subcaption{Access Distance}
    \label{fig:stats-access-distance}
  \end{subfigure}
  \caption{Statistical analysis: metrics overview}
  \label{fig:stats-overview}
\end{figure}

\section{Conclusion}\label{sec:exp-conclusion}

\Cref{cha:experiment} presents an experiment based on our environment to illustrate the difference of our cache implementations, of our allocator implementations, and the used benchmarks.
The correlation of speedup and compaction proves that there are several benchmarks that show an outstanding good results: 445.gobmk, 471.omnetpp, and 483.xalancbmk.

The 445.gobmk benchmark demonstrates that there is potential for performance improvement by picking the appropriate allocator.
In this case namely CompactStack and CompactSet.
Furthermore, it also illustrates that the performance can become significantly worse than the base line performance by deciding on one of the other allocators.
For this benchmark the CompactQueue allocators presents the worst performance results.
The 445.gobmk benchmark is characterized by few accesses and extremely short liveness intervals for most of the used variables that lead to a significant performance improvement indicated by the speedup.

The 471.omnetpp benchmark presents less potential for performance improvement.
Nevertheless, the \ac{MIN} caches end up with quite close to the optimal \ac{CPA}.
That is quite impressive facing that small margin of potential improvement.
Unfortunately, none of our allocators is able to result in a speedup greater than one.
However, the 471.omnetpp benchmark presents a remarkable compaction.
Using one of our compacting allocators reduced the necessary memory nearly by 40 times.
The 471.omnetpp benchmark is characterized by short access distances and short liveness intervals for most of the used variables, that yields in a dramatical reduction for the memory required to execute this benchmark.

The 483.xalancbmk presents a significant gap in terms of performance for the \ac{LRU} and \ac{MIN} cache implementations.
Further, the CompactStack allocator is the only implementation that is able to perform better than the original \ac{trace}.
As for the benchmarks 445.gobmk and 471.omnetpp, the CompactQueue allocator presents the worst performance.
The 483.xalancbmk benchmark is characterized by few accesses and long liveness intervals for most of the used variables, this indicates that long liveness intervals are not beneficial for the performance.

In most of our experiments, the results of the CompactQueue allocator have been outperformed by the CompactStack allocator, or the CompactSet allocator, or both.
Intuitively, the CompactQueue implementation picks always the variable which has been freed least recently (longest ago).
While the stack implementation of the free list enables the CompactStack to pick the most recently freed variable (shortest ago).
Obviously, the most recently freed variable has a higher probability to still be in the cache than the variable least recently freed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \chapter{Related Work}\label{cha:related-work}
% \torevise\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusion \& Future Work}\label{cha:conclusion}

% what we did
In this work, we have analyzed the memory access traces of nine benchmarks taken from two different benchmark suits, SPEC 2006 and V8.
We have transformed the observed traces of all benchmarks by using different allocators: Identity, CompactStack, CompactSet, CompactQueue, and SingleAssignment.
Each of the transformed traces has been executed on four different caches to observe the data about their performance and memory usage.
These four caches use two different algorithms to decide on an eviction candidate, namely \ac{LRU} and \citeauthor{belady1966study}.
For each eviction policy, there is one version that uses the liveness information of a trace and a second one that proceeds without using liveness information.
We analyzed the observed memory access trace according to the four metrics: accesses, access distance, liveness interval length, and overlapping liveness.
% results
The presented experiments illustrate that the analyzed memory access traces have potential for improvement.
Our extremely simple allocators used for transformation reveal that each \ac{trace} uses at least twice the memory than necessary.
However, the drawback of these extremely simple allocators is shown in the performance results, most benchmarks achieve a worse performance after transformation.
The defined metrics used to reason about performance and memory usage, illustrate tendencies for improvement rather than unique characteristics.
% conclustions
We were able to show that even modern computer programs have not reached there limits yet, their is still space for improvement.
Unfortunately, the chosen metrics are not as expressive than what we have assumed.

% future work
Implementation of more sophisticated allocators that improve the quality of the trace transformations remains as future work.
Further, the set of available caches could be extended by implementing additional eviction policies.
Moreover, other benchmark suites could be integrated and analyzed.
The additional data could lead to further metrics that may allow stronger statements or
 reveal new characteristics.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\backmatter%

\input{postface}%

\end{document}
