% Copyright (c) 2016, Mario Preishuber. All rights reserved.
%
% Notation: <option description> (<options>)
%
% Specify
%   - layout (onecolumn, twocolumn),
%   - thesis type (bachelor, master),
%   - language (english, naustrian)
%   - indicate that you want to use the university seal as background of the
%     titlepage (seal)
%   - indicate that you want signature fields (signatures)
%   - chapter headings to be on the next page or the next recto, verso page
%     (openany, openright, openleft) (standard memoir option, passed through)
\documentclass[onecolumn, openany, master, english, seal, signatures]{dbrgrptt}
\usepackage{listings}
\usepackage{tikz}
\usetikzlibrary{positioning}
\input{figs/tikz-settings}

\hypersetup{
    colorlinks=false,
    pdfborder={0 0 0},
}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Titling page / Initialization
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thesistitle{%
Towards cache-optimal address allocation:%
~How fast could your code have run if you had known where to allocate memory?%
}%
\thesisdate{\today}%
\thesisauthor{Mario Preishuber}{01120643}%
\setsupervisor{Univ.-Prof. Dr. Christoph Kirsch}%

% pages after this command (and before \mainmatter) are numbered roman
\frontmatter%

\input{preface}%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Main content
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mainmatter%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}\label{cha:introduction}
\torevise
It is well know that there is a performance gap between CPU speed and memory access time. Obviously, accessing data stored on memory is a bottleneck by executing programs. Form an abstract point of view a program consists of a sequence of load and store operation. In between those operations the CPU executes some computations. The sequence of load and store operations is called a \emph{trace}. To improve the access time on memory \emph{caches} has been introduced.

A \emph{cache} is a small and super fast memory buffer. A cache holds temporarily blocks of data of the programs content of memory. All data required for the current computation of the CPU is called \emph{working set}~\cite{denning1968working}.

In general caches are much small than memory, the reason is money. Any storage system, e.g., a cache, or memory, or a disk, \dots, can be characterized by its access time and cost per bit. Usually storage systems with lower access time tend to higher costs per bit, and storage systems with lower costs per bit tend to higher access time.

This is where another issue raises. Typically the working set does not completely fit into the cache. This leads to the situation that the cache is full but it is required to load data into it from memory. If data is accessed which is currently not in the cache, this is called a \emph{cache miss}. It is up to the cache to decide which data is kept and which one is \emph{evicted}. The decision is made by the so-called \emph{eviction policy}. Note, it is common to name a cache after its eviction policy, e.g., LRUCache for a cache which applies a least recently used strategy.

Although, caches are a good approach to address the performance gap, in case of a cache miss it yields a memory access which is expensive. A very common method to determine the performance of a cache is the \emph{miss rate}~\cite{patterson2011computer}. The miss rate is defined as \emph{cache misses per instruction}. Alternatively, the \emph{miss ratio} could be used, which is defined by \emph{cache misses per total number of memory accesses}. Because of the limited size and the prices for larger caches, cache misses have a significant influence on the execute time of a program. Reduce the number of caches misses became an interesting research topic.

For reducing the number of cache misses so-called \emph{cache lines} where introduced. The concept behind is based on observations made on programs. If a program accesses data once, it is likely to access data nearby in near future as well, \emph{spatial locality}~\cite{jacob2010memory}. Instead of load only the required data, also the data nearby is load into cache. How much data is load is defined by the cache line size. Each time a cache miss occurs a whole cache line is loaded. There was another observation which significantly influenced the design of caches. If a program accesses data once, it is likely to be accessed again in near future, \emph{temporal locality}~\cite{jacob2010memory}.

Nevertheless, which data is grouped together on a cache line influences the cache performance significantly. Assume iterating over an array. In such a case performance will be excellent, because this are perfect conditions for spatial locality and temporal locality. However, assume iterating over a dynamically allocated linked list. In difference to an array it is not ensured that the list elements are stored contiguous in memory. It's more likely that elements are distributed over the whole memory space. In a worst case scenario each iteration forces a cache miss. This issue is a consequence of \emph{memory layout} generated by the allocator through dynamic allocations. For improving the memory layout further information about the data is required, the \emph{liveness}.

Data is called \emph{live} by the first time it is written, and \emph{dead} after the last time it is accessed. The period from being live until data is dead is named \emph{liveness interval}. With the information about the liveness of data another opportunity of improvement appears, \emph{memory reuse}. The idea is to reuse dead data in memory as quickly as possible to store new live data. An obvious advantage of memory reuse is that less memory is required. Further, it improve the probability that reused memory is already cached when the new data goes live. Especially, latter one could reduce the number of cache misses.

Our conjecture is that quicker memory reuse dominates location of data in overall performance impact. We are convinced that caches maybe more effective if memory is reused quickly, independently of where data is located. We are  working on a number of micro benchmarks that allow us to reuse memory close to optimality.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Theoretical Foundations}\label{cha:theoretical-foundations}

\section{Hardware Model}\label{sec:hardware-model}

This section deals with the hardware model applied. The used model is reduced to the minimal required core components of a modern computer system. It consists of three components as illustrated by \autoref{fig:hardware-model}. The three components are the central processing unit, a cache, and the main memory, namely.

\begin{figure}[!ht]
  \centering
  \input{figs/tikz/hardware-model}
  \caption{Hardware Model}
  \label{fig:hardware-model}
\end{figure}

\subsection{Central Processing Unit}
The \emph{\ac{CPU}} is the core computational unit of computer systems. The purpose of a \ac{CPU} is to process and execute a given program. A \emph{program} consists of a sequence of instructions which operate on data. A program is processed sequentially instruction by instruction. Further, an \emph{instruction} is a command with the purpose to perform some specific action, e.g, to add two numbers or to modify data. \emph{Data} is the information required by a program for its execution, e.g., values for computations. The \ac{CPU} consists of a limited number of so-called \emph{registers}. A \emph{register} is a extremely small and extremely fast memory unit which allows the \ac{CPU} to execute computations. Registers are the only memory unit where the \ac{CPU} is able to apply arithmetic operations. The actual size of a single register and the number of registers available for computations depends on the architecture of the hardware.

For the purpose of this work only instructions reading or writing data are taken into account. To read data a so-called \emph{load} instruction is required. And to write data a so-call \emph{store} instructions is required. These two instructions access the data of a program stored at the main memory. E.g., for the purpose of computations which are done by the \ac{CPU} it might be necessary to save a result for later computations. Such data can be write to main memory by executing a store instruction. However, there are many more instructions available on modern computer systems, e.g., arithmetical operations.

\subsection{Main Memory}
The \emph{main memory} is a storage containing all data required to execute a program. Sometime the main memory is also called \ac{RAM}. In general the CPU has to load and store data from the main memory to process a program. Furthermore, even the program itself is stored at the main memory while its execution. Each instruction of a program has to be loaded before the \ac{CPU} is able to execute it. In case of an load instruction the \ac{CPU} first loads the instruction itself. Second the CPU interprets the instructions and finds out that is has to execute a load of data. And third the \ac{CPU} loads the actually required data into some of the available registers.
Further, the main memory is structured in equally sized chunks of memory, e.g., 8 byte which is called represents the size of something called a \emph{word}. Each such memory chunk can be accesses by the \ac{CPU} via an unique identifier, its \emph{physical address} or in short just \emph{address}. If the CPU needs data for, e.g., a computation data has to be loaded via the load instruction \texttt{load \&address}.

\subsection{Cache}
A \emph{cache} is a small, high-speed memory which temporarily holds data of the addresses used by the currently processed program. The concept of caches has been introduced by Smith in his work \cite{smith1982cache}.
Caches are based on two major observation. \emph{Temporal locality}, if data is accessed it is likely that the same data is accessed again in the near future. \emph{Spatial locality}, if data is accessed it is likely that other data near by is also accessed in the near future. Speaking about \emph{accessing an address} is equivalent with \emph{accessing data stored at an address in the main memory}. Same for cached addresses.
For the \ac{CPU} a cache is invisible. Independent if there is a cache or not the \ac{CPU} always just wants to access a certain address. If there is cache present it simply takes less time to load the value of an address into the \ac{CPU}s register. This is because caches are high-speed memory units.
A \emph{cache miss} occurs whenever the CPU wants to access an address which is not currently in the cache. Such a situation requires to load the data of the requested address from main memory into the cache. Before the value stored at this address is loaded into one of the registers. Such an operation is expensive as explained in \autoref{sec:performance}. If the requested address is already in the cache it is not required to access the main memory, this case is called a \emph{cache hit}. Since caches are small memory they are limited in the number of addresses which could be temporarily stored. In case the cache is full and a cache miss occurs it is required to make some space to load the requested address. The so-called \emph{cache policy} decides which address has to be \emph{evicted}, i.e., which address has to be written back into the main memory to get space. There are many different algorithms trying to make a good chose on the address to evict, e.g., \ac{LRU}. For more details see \autoref{sec:caches}

\section{Memory Access Trace}\label{sec:memory-access-trace}

The \emph{memory access trace} represents all memory accesses for a given program. More precise the memory access trace is a sequence load and store instructions observed by analyzing a given program. Furthermore, for the purpose of this work is does not matter which value is store at a certain address. For this reason the stored values are all dropped. This results in a sequence of tuples consisting of the instruction type, which is either \emph{load} or \emph{store}, and an address. \autoref{fig:mat-example-trace} shows an example of a memory access trace, addresses are annotated with a \texttt{\&} known form languages like C.

\autoref{fig:mat-example-c-code} shows a simple C program which is summing three numbers. At first all used variables are declared. Afterwards the variables \texttt{sum}, \texttt{x}, and \texttt{y} are initialized with the values \texttt{0}, \texttt{1}, and \texttt{2}. Followed by the first computation, the value of \texttt{x} is added to \texttt{sum}s value and stored in \texttt{sum}. Next the variable \texttt{z} is initialized with value \texttt{3}. Then \texttt{sum} is increased by the value of \texttt{y}. Finally the computation is completed by adding the value of \texttt{z} to \texttt{sum}.

\begin{figure}[!ht]
  \centering
  \begin{tabular}{c}
  \lstinputlisting[language=C]{figs/code/memory-access-trace/mat.c}
  \end{tabular}
  \caption{C code example of summing three numbers.}
  \label{fig:mat-example-c-code}
\end{figure}

\autoref{fig:mat-example-assembly-code} shows a snipped of assembly code generated by compiling the code of \autoref{fig:mat-example-c-code}. For compilation GCC 4.8.5 on Ubuntu 16.04 for AMD Opteron\texttrademark Processor 6376 with x86\_64 Architecture has been used. Since the declaration of the variables is only important for the compiler there has been no assembly code generated for these. For this reason the first line of assembly code shown in \autoref{fig:mat-example-assembly-code} represents the code generated for the C code \texttt{sum = 0;}. Other than expected the compiler has not generated a store operation instead the following code appears \texttt{movl \$0, -16(\%rbp)}.

For explanation this assembly instruction consists of three parts. The first part shows the instruction which should be executed. In the example above \texttt{movl} represents this instruction. \texttt{movl} moves a \emph{long} value into a register. A long value is on the x86\_64 architecture of size 32 bit. The second part represents the value which should be moved. In the example from above a constant value is moved. That \texttt{0} is a constant value is indicated by the \texttt{\$} character. The third part represents the address where the value should be moved to. In the example from above the target address is \texttt{-16(\%rbp)}. In this case \texttt{rbp} is an register as indicated by the \texttt{\%} character. Further, \texttt{\%rbp} is a so-called \emph{general-purpose} register of size 64 bit. By now it is enough to know that this register hold a memory address which is used as base to compute the actual target address. \texttt{-16} is the offset used to compute the actual target address.
\autoref{fig:mat-example-assembly-code} only consists of two different types of instructions. The \texttt{movl} instruction is explained above. The other operation is used as follows \texttt{addl \%eax -16(\%rbp)}. The meaning of this instruction is to add the value stored at \texttt{-16(\%rbp)} to the value currently stored at the register \texttt{\%eax} and store the result at \texttt{\%eax}. As indicated by the \texttt{l} character at the and of the instruction name, this instruction operates on value of size 32 bit.

\begin{figure}[!ht]
  \centering
  \begin{tabular}{c}
  \lstinputlisting[firstline=13, lastline=22]{figs/code/memory-access-trace/mat.s}
  \end{tabular}
  \caption{Assembly code snippet generated by compiling the code of \autoref{fig:mat-example-c-code} with GCC 4.8.5 on Ubuntu 16.04.5 for AMD Opteron\texttrademark~Processor 6376 with x86\_64 Architecture.}
  \label{fig:mat-example-assembly-code}
\end{figure}

Comparing the assembly code of \autoref{fig:mat-example-assembly-code} with the C code of \autoref{fig:mat-example-c-code} it is quite strait to identify correlations of single instructions. Obviously, the first three assembly instructions correlate to the three assignments of the C code. It might be unexpected that the \texttt{movl} instruction is used instead of a store instruction. However, moving a value to a certain location is semantically equivalent with a store. Nevertheless, the there are three addresses used each with an offset. These offsets increase exactly by same size: four, namely. The reason is that \texttt{movl} operates on 32 bit which are 4 byte. Hence, the variables \texttt{sum}, \texttt{x}, and \texttt{y} are stored contiguously in memory.

\begin{figure}[!ht]
  \centering
  \begin{tabular}{c}
  \begin{lstlisting}
sum = 0; | movl $0, -16(%rbp)
  x = 1; | movl $1, -12(%rbp)
  y = 2; | movl $2, -8(%rbp)
  \end{lstlisting}
  \end{tabular}
  \caption{Assignments: C code (left) and generated assembly code (right).}
  \label{fig:mat-example-comp-assignment}
\end{figure}

Other than for assignments an addition leads to two lines of assembly code as illustrated by \autoref{fig:mat-example-comp-addition}. The first instruction loads the value of \texttt{x} (stored at address \texttt{-12(\%rbp)}) into register \texttt{\%eax}. Since the \ac{CPU} can apply arithmetic operations only on registers it is required to load the value of \texttt{x} into a register before computing the sum. Again the \texttt{movl} instruction is used instead of a load. As before the semantics of the \texttt{movl} is equivalent with a load instruction. The second instruction generated is the actual computation. As the \texttt{movl} instruction also the generate \texttt{addl} instruction operates on 32 bit. Indicated by the last letter of the instruction name, \texttt{l}. \texttt{addl} takes the value store in the register \texttt{\%eax} and adds the value store at the address \texttt{-16(\%rbp)}. The result of this computation is stored in the \texttt{\%eax} register.

\begin{figure}[!ht]
  \centering
  \begin{tabular}{c}
  \begin{lstlisting}
sum = sum + x; | movl -12(%rbp), %eax
               | addl %eax, -16(%rbp)
  \end{lstlisting}
  \end{tabular}
  \caption{Addition: C code (left) and generated assembly code (right).}
  \label{fig:mat-example-comp-addition}
\end{figure}

After the addition the assignment of variable \texttt{z} follows. This assignment works exactly the same as discussed at the beginning of this section. Same for the remaining two additions (\texttt{sum = sum + y;} and \texttt{sum = sum + z;}). The explanations above illustrate the generate code for the simple example of \autoref{fig:mat-example-c-code}. Further, the behavior of the code is discussed. Furthermore, there already tiny hints about the memory accesses. The next step is to take look at the memory access trace used in this work.

\autoref{fig:mat-example-trace} presents the memory access trace observed by the example of \autoref{fig:mat-example-c-code}. For reasons of better human readability the variable names of the C code are kept. To illustrate that the memory access trace operates on the addresses the C like character \texttt{\&} is used as prefix of an address. All information expect the kind of access and the accessed address is dropped.

\begin{figure}[!ht]
  \centering
  \begin{tabular}{c}
  \lstinputlisting{figs/code/memory-access-trace/mat.trace.manual}
  \end{tabular}
  \caption{Memory access trace of the assembly code shown in \autoref{fig:mat-example-assembly-code}.}
  \label{fig:mat-example-trace}
\end{figure}

\autoref{fig:mat-example-comp-assignment-all} compares the assignment of the C code with the generate assembly code and the resulting memory access trace of these instructions. Assignments are strait translated into stores within in the memory access trace. For simplicity reason the stored value is dropped. Only the accessed address remains in the memory access trace.

\begin{figure}[!ht]
  \centering
  \begin{tabular}{c}
  \begin{lstlisting}
sum = 0; | movl $0, -16(%rbp) | store &sum
  x = 1; | movl $1, -12(%rbp) | store &x
  y = 2; | movl $2, -8(%rbp)  | store &y
  \end{lstlisting}
  \end{tabular}
  \caption{Assignments: C code (left), generated assembly code (middle), and memory access trace (right).}
  \label{fig:mat-example-comp-assignment-all}
\end{figure}

\autoref{fig:mat-example-comp-addition-all} presents the memory access trace for the addition \texttt{sum = sum + x;}. The move instruction \texttt{movl -12(\%rbp), \%eax} loads the value of \texttt{x} into a register. Which translates as expected into a load instruction in the memory access trace. The addition itself translates into two instructions within the memory access trace. First the value of \texttt{sum} has to be loaded and finally the result has to be stored again. Still this is quite strait.

\begin{figure}[!ht]
  \centering
  \begin{tabular}{c}
  \begin{lstlisting}
sum = sum + x; | movl -12(%rbp), %eax | load  &x
               | addl %eax, -16(%rbp) | load  &sum
                                      | store &sum
  \end{lstlisting}
  \end{tabular}
  \caption{Addition: C code (left), generated assembly code (middle), and memory access trace (right).}
  \label{fig:mat-example-comp-addition-all}
\end{figure}

An obvious observation is that there are no arithmetic operations in the memory access trace. Which is reasonable, because this representation of a program shows all the interaction with the main memory. But this does not mean that while translating the assembly code to its memory access trace arithmetic operations could be skipped. Not only the move instructions could lead to a main memory access. Take a look at \autoref{fig:mat-example-addition-detail}. There is a main memory access by reading the value stored at address \texttt{-16(\%rbp)} for the computation. Such an instruction leads to a \texttt{load} within the memory access trace. Since the result of a computation is saved somewhere there is also a \texttt{store} observable in memory access trace.

\begin{figure}[!ht]
  \centering
  \begin{tabular}{c}
  \begin{lstlisting}
addl %eax, -16(%rbp)
  \end{lstlisting}
  \end{tabular}
  \caption{Addition assembly code.}
  \label{fig:mat-example-addition-detail}
\end{figure}

This section showed how the memory access trace of a program is observed. For this reason the assembly code of a simple C program which sums three numbers is discussed and finally the correlating memory access trace is presented. The memory access trace of a program is a sequence of tuples. Each hold the type of access which is either load or store and the accessed address. An address is prefixed with a \texttt{\&}.

\section{Liveness}\label{sec:liveness}

This section introduces the concept of \emph{liveness of an address}. Informal the liveness of an address describes the timespan in which the address is used by a program. In general the liveness of an address begins with its allocation and ends with its deallocation as illustrated by \autoref{fig:liveness-classical}. Already Aigner and Kirsch showed in their work \cite{aigner2013acdc} that the general understanding of liveness offers potential for improvement.

In there work they introduce the term of \emph{deallocation delay}. Which describes the timespan beginning with the last access on an object until its deallocation. This is based on the observation that objects often live longer than necessary, because of this deallocation delay. Which is a waste of resources, especially memory.

\begin{remark}
Note that in the work \cite{aigner2013acdc} the liveness term is defined at object level. In this work we are exclusively focusing on address level.
\end{remark}

However, in this work liveness is defined differently. First of all liveness is defined on address level. As the whole work is operating on addresses rather than objects, or data structures, or what ever. Further, an address does not consists of \emph{the one} liveness. Rather than an address consists of multiple timespans of liveness. In the timespan beginning with allocating an address and deallocating it there are periods in which the address is used heavily and there are periods where the address is not used at all. This periods of usage are called \emph{liveness intervals}. How these liveness intervals are exactly defined is presented by \autoref{def:liveness-interval}. The idea behind is that each store operation on an address overwrites the currently stored value at an address. This is similar to the initialization of an address.

\begin{definition}[Liveness interval]\label{def:liveness-interval}
Each time there is a store on an address a new \emph{liveness interval} begins. A liveness interval ends at the last access at the address before the next store instruction occurs. A liveness intervals ends as well with the absolute last access on the address.
\end{definition}

\begin{remark}
As an implication of Definition~\ref{def:liveness-interval} a single address might have multiple liveness intervals. \autoref{fig:liveness-intervals-example} presents this case for the address of variable \texttt{sum}.
\end{remark}

\begin{figure}[!ht]
  \centering
  \input{figs/tikz/liveness-classical}
  \caption{Classical liveness}
  \label{fig:liveness-classical}
\end{figure}

\begin{figure}[!ht]
  \centering
  \input{figs/tikz/liveness}
  \caption{Liveness intervals}
  \label{fig:liveness-intervals}
\end{figure}

\autoref{fig:liveness-intervals-example} illustrates the liveness intervals of the C code example presented by \autoref{fig:mat-example-c-code}. The figure shows on the left hand-side the instruction number. On the right hand-side the instruction itself is shown. On top of the figure the addresses used by the program are listed. The core of the figure are the liveness intervals of the addresses. Each line represents a single liveness interval. A liveness interval begins at the lower instruction number and ends at the higher one. The liveness intervals are quite obvious for the addresses \texttt{\&x}, \texttt{\&y}, and \texttt{\&z}, respectively. According to Definition \ref{def:liveness-interval} a liveness interval always begins with a store instruction. Taking a look the instructions 2, 3, and 7 these indicate the beginnings of the liveness intervals for the three addresses \texttt{\&x}, \texttt{\&y}, and \texttt{\&z}. The ends of all three are indicated by load instructions at the instruction number 5, 9, and 12. Non of the addresses is access after these lines again, e.g., \texttt{\&x} not used anymore after instruction number 5. However, these are quite strait cases. The address \texttt{\&sum} presents a much more interesting case. \texttt{\&sum} consists of four liveness intervals. The first one begins at instruction number 1. This is when \texttt{\&sum} is initialized. Afterwards follow the initializations of \texttt{\&x} and \texttt{\&y} before \texttt{\&sum} is accessed again. The load at instruction number 4 indicates the end of \texttt{\&sum}s first liveness interval, because the next access at instruction number 6 is a store. This is when the second liveness interval begins. The other liveness intervals follow the same pattern. Note that even the store at instruction number 13 yields a liveness interval, even if it is the shortest possible. At the instruction number 5, 9, and 12 the address \texttt{\&sum} is \emph{dead} according to the applied definition of liveness intervals, see \autoref{fig:liveness-intervals}. It would allow to reuse the address of \texttt{\&sum} for other purposes.

\begin{figure}[!ht]
  \centering
  \input{figs/tikz/liveness-intervals-example}
  \caption{Liveness intervals of the C code example shown in \autoref{fig:mat-example-c-code}.}
  \label{fig:liveness-intervals-example}
\end{figure}

This section presented the definition of liveness and especially of liveness intervals. The definition of liveness intervals is one of the most central components of this work. It is the base for the trace transformations described in \autoref{sec:trace-transformation}.


\section{Trace Transformation}\label{sec:trace-transformation}

\begin{figure}[!ht]
  \centering
  \input{figs/tikz/trace-transformation-original}
  \caption{Liveness intervals of the C code example shown in \autoref{fig:mat-example-c-code}.}
  \label{fig:liveness-intervals}
\end{figure}

\begin{figure}[!ht]
  \centering
  \input{figs/tikz/trace-transformation-sa}
  \caption{Liveness intervals of the C code example shown in \autoref{fig:mat-example-c-code} in \emph{single assignment} form.}
  \label{fig:trace-tranformation-sa}
\end{figure}

\begin{figure}[!ht]
  \centering
  \input{figs/tikz/trace-transformation-compact}
  \caption{Liveness intervals of the C code example shown in \autoref{fig:mat-example-c-code} in \emph{compacted} form.}
  \label{fig:trace-tranformation-compact}
\end{figure}

\section{Performance}\label{sec:performance}

\autoref{tab:memory-access-cost} shows the costs for the different types of memory accesses. These numbers are taken form literature \cite{drepper2007every}, \footnote{\url{http://www.7-cpu.com/cpu/Skylake.html}}. The actual values are not that important than the relation of cache instruction costs to the memory instruction costs.
\begin{table}
  \centering
  \begin{tabular}{lc}
  \hline
  Memory Access Type & Cost in Cycles \\
  \hline
  Cache  load  & 1 \\
  Cache  store & 1 \\
  Memory load  & 5 \\
  Memory store & 5 \\
  \hline
  \end{tabular}
  \caption{Cost for memory access types}
  \label{tab:memory-access-cost}
\end{table}

\section{Problem Statement}
Given a trace T of load and store instructions find metrics that characterize the trace performance for a given cache model C and implement an execution engine for computing their quantities.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experimental Setup}

\section{Caches}\label{sec:caches}
\subsection{Belady Cache}
\subsection{Belady Cache with Liveness Information}
\subsection{Least Recently Used Cache}
\subsection{Least Recently Used Cache with Liveness Information}

\section{Allocators}
\subsection{Original Allocator}
\subsection{Single Assignment Allocator}
\subsection{Compacting Allocator}
\subsubsection{Compacting Allocator with Stack Semantic Free List}
\subsubsection{Compacting Allocator with Queue Semantic Free List}
\subsubsection{Compacting Allocator with Set Semantic Free List}

\section{Benchmarks}
% \todo{For each benchmark: Description}

\subsection{SPEC 2006 Benchmarks}

This section describes the subset of benchmarks of the SPEC 2006 benchmark suite \cite{henning2006spec} which we used for our experiments. The benchmark descriptions below are taken from the SPEC 2006 paper.

\subsubsection{445.gobmk}

\begin{quote}
\begin{description}
\item[Authors:] (in chronological order of contribution) are Man Lung Li, Wayne Iba, Daniel Bump, David Denholm, Gunnar Farneb\"ack, Nils Lohner, Jerome Dumonteil, Tommy Thorn, Nicklas Ekstrand, Inge Wallin, Thomas Traber, Douglas Ridgway, Teun Burgers, Tanguy Urvoy, Thien-Thi Nguyen, Heikki Levanto, Mark Vytlacil, Adriaan van Kessel, Wolfgang Manner, Jens Yllman, Don Dailey, Mans Ullerstam, Arend Bayer, Trevor Morris, Evan Berggren Daniel, Fernando Portela, Paul Pogonyshev, S.P. Lee, Stephane Nicolet and Martin Holters. General

\item[General Category:] Artificial intelligence - game playing.

\item[Description\footnote{\url{www.gnu.org/software/gnugo/devel.html}}:] The program plays Go and executes a set of commands to analyze Go positions.
\end{description}
\end{quote}

\subsubsection{450.soplex}

\begin{quote}
\begin{description}
\item[Authors:] Roland Wunderling, Thorsten Koch, Tobias Achterberg

\item[General Category:] Simplex Linear Program (LP) Solver

\item[Description:] 450.soplex is based on SoPlex Version 1.2.1. SoPlex solves a linear program using the Simplex algorithm. The LP is given as a sparse m by n matrix A, together with a right hand side vector b of dimension m and an objective function coefficient vector c of dimension n. The matrix is sparse in practice. SoPlex employs algorithms for sparse linear algebra, in particular a sparse LU-Factorization and solving routines for the resulting triangular equation systems.
\end{description}
\end{quote}

\subsubsection{454.calculix}

\begin{quote}
\begin{description}
\item[Authors:] Guido D.C. Dhondt

\item[General Category:] Structural Mechanics

\item[Description\footnote{\url{www.calculix.de}}:] 454.calculix is based on CalculiX, a free software finite element code for linear and nonlinear three- dimensional structural applications. It uses classical theory of finite elements described in books such as \cite{zienkiewicz1977finite}. CalculiX can solve problems such as static problems (bridge and building design), buckling, dynamic applications (crash, earthquake resistance) and eigenmode analysis (resonance phenomena).
\end{description}
\end{quote}

\subsubsection{462 libquantum}

\begin{quote}
\begin{description}
\item[Authors:] Bj\"orn Butscher, Hendrik Weimer

\item[General Category:] Physics / Quantum Computing

\item[Description\footnote{\url{http://www.libquantum.de}}:] libquantum is a library for the simulation of a quantum computer. Quantum computers are based on the principles of quantum mechanics and can solve certain computationally hard tasks in polynomial time.
In 1994, Peter Shor discovered a polynomial-time algorithm for the factorization of numbers, a problem of particular interest for cryptanalysis, as the widely used RSA cryptosystem depends on prime factorization being a problem only to be solvable in exponential time. An implementation of Shor's factorization algorithm is included in libquantum.
Libquantum provides a structure for representing a quantum register and some elementary gates. Measurements can be used to extract information from the system. Additionally, libquantum offers the simulation of decoherence, the most important obstacle in building practical quantum computers. It is thus not only possible to simulate any quantum algorithm, but also to develop quantum error correction algorithms. As libquantum allows to add new gates, it can easily be extended to fit the ongoing research, e.g. it has been deployed to analyze quantum cryptography.
\end{description}
\end{quote}

\subsubsection{471 omnetpp}

\begin{quote}
\begin{description}
\item[Authors:] Andr\'as Varga, Omnest Global, Inc.

\item[General Category:] Discrete Event Simulation

\item[Description:] simulation of a large Ethernet network, based on the OMNeT++ discrete event simulation system\footnote{\url{www.omnetpp.org}}, using an ethernet model which is publicly available\footnote{\url{http://ctieware.eng.monash.edu.au/twiki/bin/view/Simulation/EtherNet}}.
For the reference workload, the simulated network models a large Ethernet campus backbone, with several smaller LANs of various sizes hanging off each backbone switch. It contains about 8000 computers and 900 switches and hubs, including Gigabit Ethernet, 100Mb full duplex, 100Mb half duplex, 10Mb UTP, and 10Mb bus. The training workload models a small LAN.
The model is accurate in that the CSMA/CD protocol of Ethernet and the Ethernet frame are faithfully modelled. The host model contains a traffic generator which implements a generic request-response based protocol. (Higher layer protocols are not modelled in detail.)
\end{description}
\end{quote}

\subsubsection{483 xalancbmk}

\begin{quote}
\begin{description}
\item[Authors:] IBM Corporation, Apache Inc, plus modifications for SPEC purposes by Christopher Cambly, Andrew Godbout, Neil Graham, Sasha Kasapinovic, Jim McInnes, June Ng, Michael Wong. Primary contact: Michael Wong

\item[General Category:] XSLT processor for transforming XML documents into HTML, text, or other XML document types

\item[Description:] a modified version of Xalan-C++\footnote{\url{http://xml.apache.org/xalan-c/}}, an XSLT processor written in a portable subset of C++ . Xalan-C++ version 1.8 is a robust implementation of the W3C Recommendations for XSL Transformations (XSLT)\footnote{\url{http://www.w3.org/TR/xslt}} and the XML Path Language (XPath)\footnote{\url{http://www.w3.org/TR/xpath}}. It works with a compatible release of the Xerces-C++\footnote{\url{http://xml.apache.org/xerces-c}} XML parser: Xerces-C++ version 2.5.0. The XSLT language is use to compose XSL stylesheets. An XSL stylesheet contains instructions for transforming XML documents from one document type to another document type (XML, HTML, or other). In structural terms, an XSL stylesheet specifies the transformation of one tree of nodes (the XML input) into another tree of nodes (the output or transformation result).
Modifications for SPEC benchmarking purposes include: combining code to make a standalone executable, removing compiler incompatibilities and improving standard conformance, changing output to display intermediate values, removing large parts of unexecuted code, and moving all the include locations to fit better into the SPEC harness.
\end{description}
\end{quote}

\subsection{Google JavaScript Engine (V8) Benchmarks}

This section describes the subset of benchmarks of the Octane benchmark suite \cite{v8benchmarks} which we used for our experiments. The benchmark descriptions below are taken from the Octane website.

\subsubsection{Richards}

\begin{quote}
\begin{description}
\item[Description:] OS kernel simulation benchmark, originally written in BCPL by Martin Richards\footnote{\url{http://www.cl.cam.ac.uk/~mr10/}} (539 lines).
\item[Main focus:] property load/store, function/method calls
\item[Secondary focus:] code optimization, elimination of redundant code
\end{description}
\end{quote}

\subsubsection{Raytrace}

\begin{quote}
\begin{description}
\item[Description:] Ray tracer benchmark based on code by Adam Burmister\footnote{\url{http://burmister.com}} (904 lines).
\item[Main focus:] argument object, apply
\item[Secondary focus:] prototype library object, creation pattern
\end{description}
\end{quote}

\subsubsection{Deltablue}

\begin{quote}
\begin{description}
\item[Description:] One-way constraint solver\footnote{\url{http://constraints.cs.washington.edu/deltablue/}}, originally written in Smalltalk by John Maloney and Mario Wolczko (880 lines).
\item[Main focus:] polymorphism
\item[Secondary focus:] OO-style programming
\end{description}
\end{quote}

\section{Metrics}
\subsection{Address Access}
\subsection{Access Distance}
\subsection{Live Addresses}
\subsection{Liveness Length}

\section{Trace generation (Valgrind / Cachegrind)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experiments}\label{cha:experimetns}
\section{8 Byte Cache Line Size}
\section{64 Byte Cache Line Size}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work}\label{cha:related-work}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}\label{cha:conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Future Work}\label{cha:future-work}

% \todo{Figure: Overview - CUP <-> Cache <-> Memory}

% \section{Allocators}
% \todo{Define: Allocator}
% \todo{Define: Free list}

% \subsection{Original Allocator}
% \subsection{Single Assignment Allocator}
% \todo{Define: Single Assignment}

% \subsection{Compacting Allocator}
% \todo{Define: Compaction}
% \subsubsection{Compacting Allocator with Stack Semantic Free List}
% \subsubsection{Compacting Allocator with Queue Semantic Free List}
% \subsubsection{Compacting Allocator with Set Semantic Free List}

% \subsection{Own Implementation}
% \todo{Pick one or two of our own improvement ideas.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Benchmarks}

% \section{\todo{HEADLINE}}
% \todo{For each benchmark: Description and Statistical Characteristic}

% \subsection{SPEC 2006 Benchmarks \cite{henning2006spec}}
% \todo{Benchmark description see paper above.}
% \subsubsection{445 gobmk}
% \subsubsection{445 gobmk 3}
% \subsubsection{445 gobmk 5}
% \subsubsection{445 gobmk 6}
% \subsubsection{450 soplex}
% \subsubsection{454 calculix}
% \subsubsection{462 libquantum}
% \subsubsection{471 omnetpp}
% \subsubsection{483 xalancbmk}

% \subsection{Google JavaScript Engine (V8) Benchmarks}
% \subsubsection{Richards}
% \subsubsection{Raytrace}
% \subsubsection{Deltablue}



% \section{Benchmark analysis}

% \section{Trace generation (Valgrind / Cachegrind)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Experiments}
% \section{8 Byte Cache Line Size}
% \section{64 Byte Cache Line Size}

\backmatter%

\input{postface}%

\end{document}



















\chapter{Introduction}
\section{Definitions - V1}
\subsection{General definitions}
\begin{definition}[Command]
A command is an instruction with the purpose to perform some specific action, e.g, to add two numbers.
\end{definition}

\begin{definition}[Data]
Data is the information required by a program for its execution, e.g., values for computations.
\end{definition}

\begin{definition}[Program]
A program consists of a sequence of commands which operate on the programs data.
\end{definition}

\begin{definition}[CPU]
The CPU (central processing unit) executes a program, i.e., command by command is processed.
\end{definition}

\begin{definition}[Main Memory (in short: memory)]
Main Memory is a physical storage. It is structured in chunks of same size. During execution of a program the main memory stores the programs data and the program itself.
\end{definition}

\begin{definition}[Address]
A physical address is an unique identifier of a single memory chunk.
\end{definition}

%  chunk             address
%  M +-------------+ m
%    &     ...     |
%    &     ...     |
%    &     ...     |
%  J +-------------+ j
%    &    Data     |
%    + - - - - - - +
%    &   Program   |
%  I +-------------+ i
%    &     ...     |
%    &     ...     |
%    &     ...     |
%  0 +-------------+ 0

\begin{definition}[Allocator]
An allocator maintains a specific part of a programs data, the so-called \textit{heap}. The purpose of an allocator is to keep track about the addresses which are currently in use by the program (\textit{allocated}) and which are not (\textit{free}).
\end{definition}

%  M +-------------+     /  J +-------------+ address j
%    &     ...     &    /     &    Stack    |
%    &     ...     &   /      + ~ ~ ~ ~ ~ ~ + (*)
%    &     ...     &  /       & Free memory |
%  J +-------------+          + ~ ~ ~ ~ ~ ~ + (*)
%    &    Data     &          &             |
%  K + - - - - - - +          &    Heap     |
%    &   Program   &          &             |
%  I +-------------+          +-------------+
%    &     ...     &  \       &  Constants  |
%    &     ...     &   \    K + - - - - - - +
%    &     ...     &    \     &   Program   |
%  0 +-------------+     \  I +-------------+ address i
%  (*) Dynamic bound: Grows and shrinks during program execution.

\begin{definition}[Memory layout]
A memory layout describes which data is stored at which address. The memory layout is determined by the allocator.
\end{definition}

\begin{example}
Following example shows two possible memory layouts for the \texttt{data 0-10}. 
%  J +-------------+ j   /  + ~ ~ ~ ~ ~ ~ +  + ~ ~ ~ ~ ~ ~ +
%    &    Stack    &    /   &   data 00   &  &   data 10   &  address (k + 10)
%    + ~ ~ ~ ~ ~ ~ +   /    &   data 01   &  &   data 09   &  address (k + 9)
%    & Free memory &  /     &   data 02   &  &   data 08   &  address (k + 8)
%    + ~ ~ ~ ~ ~ ~ +        &   data 03   &  &   data 07   &  address (k + 7)
%    &             &        &   data 04   &  &   data 06   &  address (k + 6)
%    &    Heap     &        &   data 05   &  &   data 05   &  address (k + 5)
%    &             &        &   data 06   &  &   data 04   &  address (k + 4)
%  K +-------------+ k      &   data 07   &  &   data 03   &  address (k + 3)
%    &  Constants  &  \     &   data 08   &  &   data 02   &  address (k + 2)
%    + - - - - - - +   \    &   data 09   &  &   data 01   &  address (k + 1)
%    &   Program   &    \   &   data 10   &  &   data 00   &  address (k + 0)
%  I +-------------+ i   \  +-------------+  +-------------+
\end{example}

\begin{definition}[Register]
A register is a unit of memory directly accessed by the CPU. The CPU disposes over a limited number of register to process the commands of a program. Registers store commands, addresses of data or data.
\end{definition}

%  +---------------------+   +----------------------+ 
%  & Processor           &   & Memory               |
%  & +-----+-----------+ &   & +-----------+------+ |
%  & | CPU & Registers & <---> & Addresses & Data & |
%  & +-----+-----------+ &   & +-----------+------+ |
%  +---------------------+   +----------------------+

\begin{definition}[Data access]
A data access is a command aiming on processing data of a program. 
There are two \textit{access types}:
1. \textit{read} data: load the value stored at an address into a register
2. \textit{write} data: store the value of a register at an address

In general a data access consists of 3 components: (1) an access type, (2) an address, and (3) a register. For our purpose we don't care about the register. Hence, a data access is a tuple $(t, a)$ where $t$ is an access type and $a$ is an address (within the bounds of the program).
\end{definition}

\begin{definition}[Data access time]
The data access time is the amount of time it takes to data between a register and memory or vice versa.
\end{definition}

\begin{definition}[Trace]
The trace $T$ of a program $P$ is the sequence of data accesses $((t_0, a_0), (t_1, a_1), .. (t_{N-1}, a_{N-1}))$ of $P$, with $N > 0$ and $N =|T| \leq |P|$.
\end{definition}

\begin{definition}[Liveness interval of an address]
Assume a trace $T$ which contains an address $a$. The liveness interval of $a$ begins with the first data access on $a$ and ends with the last data access on $a$.

Assume tuple $(i,j)_a$ represents the liveness interval of an address $a$ of a trace $T$, $a \in T$ with $|T| = N$ and $0 \leq i,k \leq j,l < N$, then 
 - $(t_i, a_i) \in T$ s.t. $\not\exists k < i: (t_k, a) \wedge a_i = a$, and
 - $(t_j, a_j) \in T$ s.t. $\not\exists l > j: (t_l, a) \wedge a_j = a$.
\end{definition}

\begin{definition}[Cache]
A cache is a buffer which stores data currently used by the CPU. To be more precise if there is a data access on an address, this address and its data is temporally store within the cache.
\end{definition}

\begin{quote}
\textit{Note:} Caches where introduced, because the offer a smaller data access time than main memory does. This is beneficial for the total execution time of a program.

%  +---------------------+ +---------------------------+ +----------------------+
%  & Processor           & & Cache                     & & Memory               |
%  & +-----+-----------+ & & +----------------+------+ & & +-----------+------+ |
%  & | CPU & Registers & <-> & Some Addresses & Data & <-> & Addresses & Data & |
%  & +-----+-----------+ & & +----------------+------+ & & +-----------+------+ |
%  +---------------------+ +---------------------------+ +----------------------+
\end{quote}

\begin{definition}[Cache hit]
If the CPU processes a data access on an address which is already stored in the cache, this is called a \textit{cache hit}.
\end{definition}

\begin{definition}[Cache miss]
If the CPU processes a data access on an address which is not yet stored in the cache, this is called a \textit{cache miss}.
\end{definition}

\begin{definition}[Eviction strategy]
In case of a cache miss it is required make space for the requested address and its data. The \textit{eviction strategy} decides which address is evicted from the cache, i.e., the data of this address is written back to main memory.
\end{definition}

\subsection{Performance}
\begin{definition}[Program performance]
The performance of a program is the time (milliseconds) it take the CPU to process all commands, this is called \textit{total execution time}.
\end{definition}

\begin{definition}[Allocator performance]
The performance of an allocator is the number of cache misses yield by a programs trace.
\end{definition}

\subsection{Definitions required to transform a trace $T$ into single-assignment form}
\begin{definition}[Variable]
A variable $v$ is a unique identifier of the single-assignment form. 
\end{definition}

\begin{definition}[Assignment of a variable]
Each time an data access $(t, a)$ occurs in $T$, where $t$ is a \textit{write} access a new variable $v$ is chosen to replace $a$ for this data access and all following data accesses which are of type \textit{read}. (If address $a$ of trace $T$ is written multiple times each time an new variable is used.)
\end{definition}

\begin{definition}[Static-single-assignment form (SSA)]
A program is in single-assignment form if every variable has exactly one assignment.
\end{definition}


\section{Definitions - V2}

\subsection{The Memory Hierarchy}

% +-----+ cache store  +-------+ memory store  +-------------+
% |     |------------->|       |-------------->|             |
% | CPU |              |  SPM  |               | Main Memory |
% |     |<-------------|       |<--------------|             |
% +-----+  cache load  +-------+  memory load  +-------------+

\subsubsection{About Access Times}

Wikipedia 'Cache hierarchy'\footnote{\url{https://en.wikipedia.org/wiki/Cache\_hierarchy\#Properties}} describes several properties of caches that influence cache behaviour and access times:
\begin{itemize}
  \item Banked (separation of data and instruction caches) vs unified
  \item Inclusion vs exclusion vs non-inclusive non-exclusive: Defines whether data of lower caches is duplicated in higher caches.
  \item Write through vs write back: Defines whether data is written immediately or on eviction
  \item Write allocate vs write no-allocate: Defines whether data is fetched from memory on a write miss or not.
\end{itemize}

What every programmer should know about memory\cite{drepper2007every} has these numbers for access times:

\begin{table}
  \centering
  \begin{tabular}[c]{|l|r|r|}
    \hline
    To Where & Cycles & Factor to lower memory \tabularnewline
    \hline \hline
    Register & $\leq$  1 &     - \tabularnewline
    L1d      &       \~3 &     3 \tabularnewline
    L2       &      \~14 &  4.67 \tabularnewline
    RAM      &     \~240 & 17.14 \tabularnewline
    \hline
  \end{tabular}
  \caption{\todo{add caption}}
  \label{tab:def-access-times-1}
\end{table}

The 7zip benchmark pages\footnote{\url{http://www.7-cpu.com/cpu/Skylake.html}} has cycle time numbers for modern processors. 
For example for a Intel i7-6700 (Skylake):

\begin{table}
  \centering
  \begin{tabular}[c]{|l|r|r|}
    \hline
    To Where & Latency & Factor to lower memory \tabularnewline
    \hline \hline
    L1d &                          4 or 5 cycles &  5.0 \tabularnewline
    L2  &                              12 cycles &  2.4 \tabularnewline
    L3  &                        38 or 42 cycles &  3.5 \tabularnewline
    RAM & 42 cycles + 51 ns (= 246 cycles @4GHz) & 5.86 \tabularnewline
  \end{tabular}
  \caption{\todo{add caption}}
  \label{tab:def-access-times-2}
\end{table}

\subsection{The Model}

\begin{definition}[trace]
A trace is a sequence of instructions.
\end{definition}

\begin{definition}[instruction]
An instruction is an access to an address given by the tupel $(t, a)$ with $t$ in ${Read, Write}$ and $a$ an address in main memory.
\end{definition}

\begin{definition}[Address]
A location in the memory that one variable can occupy for a certain period of time.
\end{definition}

\begin{definition}[Variable]
A value that can change overtime. In this context, a variable is a name that represents a data used by the trace. it can change according to the trace transformations.
\end{definition}

\begin{definition}[Single assignment form (SA)]
A trace is in single assignment form if every variable has exactly one assignment. \cite{rosen1988global} \cite{alpern1988detecting}
\end{definition}


\begin{definition}[Trace transformation]
Trace transformation
\end{definition}

\begin{definition}[Initial trace]
The initial trace $T$ of a program $P$ is the sequence of 'data' accesses $((t_0, a_0), (t_1, a_1), \dots (t_{N-1}, a_{N-1}))$ of $P$, with $N > 0$ and $N = |T| \leq |P|$.
\end{definition}

\begin{definition}[Data]
A data is an information processed or stored by a computer.
\end{definition}

\begin{definition}[Variable]
A variable $v$ is a unique identifier of the single-assignment form.
\end{definition}

\begin{definition}[Assignment of a variable]
Each time an data access $(t, a)$ occurs in $T$, where $t$ is a \textit{write} access a new variable $v$ is chosen to replace $a$ for this data access and all following data accesses which are of type \textit{read}. (If address $a$ of trace $T$ is written multiple times each time an new variable is used.)
\end{definition}

\begin{definition}[Canonical Form]
A canonical form trace is the trace obtained after the single Assignment transformation.
\end{definition}

\begin{definition}[Liveness interval of a variable]
The liveness interval of a variable is the continuous period of time where the variable is occupying an addresse. (the change of the addresse occupied implies a new liveness interval / "continous" means no other variable was placed in this address during this time period).
\end{definition}

% +------------+                                     +------------+
% | Trace file |                                     | Single     |
% |            | ----------------------------------> | Assignment |
% | (.trace)   |  Single Assignment Transformation   | Trace      |
% +------------+                                     +------------+

\begin{definition}[Allocator]
An allocator maintains a specific part of a programs data, the so-called \textit{heap}. The purpose of an allocator is to keep track about the addresses which are currently in use by the program (\textit{allocated}) and which are not (\textit{free}).
\end{definition}

\begin{definition}[Allocator Trace]
Allocator Trace
\end{definition}

\begin{definition}[Memory layout]
A memory layout describes which data is stored at which address. The memory layout is determined by the allocator.
\end{definition}

\begin{definition}[Address]
A location in the memory that one variable can occupy for a certain period of time.
\end{definition}

 %  chunk             address
 % M +-------------+ m
 %   |     ...     |
 %   |     ...     |
 %   |     ...     |
 % J +-------------+ j
 %   |    Data     |
 %   + - - - - - - +
 %   |   Program   |
 % I +-------------+ i
 %   |     ...     |
 %   |     ...     |
 %   |     ...     |
 % 0 +-------------+ 0

\begin{definition}[Physical Address]
A physical address is an unique identifier of a single memory chunk.
\end{definition}

\begin{definition}[Virtual Address]
A virtual address is an unique identifier of a physical address in the context of a program, i.e., there is only one virtual address 0 within a program but multiple programs might make use of an virtual address 0.
\end{definition}

\begin{definition}[Trace allocator]
The initial trace $T$ of a program $P$ is the sequence of 'addresses' accesses $((t_0, a_0), (t_1, a_1), \dots (t_{N-1}, a_{N-1}))$ of $P$, with $N > 0$ and $N = |T| \leq |P|$.
\end{definition}

\begin{definition}[Memory layout]
A memory layout describes which data is stored at which address. The memory layout is determined by the allocator.
\end{definition}

% +------------+   +-----------+   +-----------+
% | Single     |   |           |   | Allocator |
% | Assignment | + | Allocator | = | Trace     |
% | Trace      |   |           |   |           |
% +------------+   +-----------+   +-----------+

\begin{definition}[Memory Model]
Memory Model
\end{definition}

\begin{definition}[Performance]
Performance
\end{definition}

\begin{definition}[Cache]
A cache is a buffer which stores data currently used by the CPU. To be more precise if there is a data access on an address, this address and its data is temporally store within the cache.
\end{definition}

% +---------------------+ +---------------------------+ +----------------------+
% | Processor           | | Cache                     | | Memory               |
% | +-----+-----------+ | | +----------------+------+ | | +-----------+------+ |
% | | CPU | Registers | <-> | Some Addresses | Data | <-> | Addresses | Data | |
% | +-----+-----------+ | | +----------------+------+ | | +-----------+------+ |
% +---------------------+ +---------------------------+ +----------------------+

\begin{definition}[Address Spaces]
The address space is the set of addresses in the memory that will be used by the trace.
\end{definition}

\begin{definition}[Register]
A register is a unit of memory directly accessed by the CPU. The CPU disposes over a limited number of register to process the commands of a program. Registers store commands, addresses of data or data.
\end{definition}

% +---------------------+   +----------------------+
% | Processor           |   | Memory               |
% | +-----+-----------+ |   | +-----------+------+ |
% | | CPU | Registers | <---> | Addresses | Data | |
% | +-----+-----------+ |   | +-----------+------+ |
% +---------------------+   +----------------------+

\begin{definition}[Main Memory]
Main Memory is a physical storage. It is structured in chunks of same size. During execution of a program the main memory stores the programs data and the program itself.
\end{definition}

\begin{example}
Following example shows two possible memory layouts for the `data 0-10`.
% J +-------------+ j   /  + ~ ~ ~ ~ ~ ~ +  + ~ ~ ~ ~ ~ ~ +
%   |    Stack    |    /   |   data 00   |  |   data 10   |  address (k + 10)
%   + ~ ~ ~ ~ ~ ~ +   /    |   data 01   |  |   data 09   |  address (k + 9)
%   | Free memory |  /     |   data 02   |  |   data 08   |  address (k + 8)
%   + ~ ~ ~ ~ ~ ~ +        |   data 03   |  |   data 07   |  address (k + 7)
%   |             |        |   data 04   |  |   data 06   |  address (k + 6)
%   |    Heap     |        |   data 05   |  |   data 05   |  address (k + 5)
%   |             |        |   data 06   |  |   data 04   |  address (k + 4)
% K +-------------+ k      |   data 07   |  |   data 03   |  address (k + 3)
%   |  Constants  |  \     |   data 08   |  |   data 02   |  address (k + 2)
%   + - - - - - - +   \    |   data 09   |  |   data 01   |  address (k + 1)
%   |   Program   |    \   |   data 10   |  |   data 00   |  address (k + 0)
% I +-------------+ i   \  +-------------+  +-------------+
\end{example}

\begin{definition}[CPU]
The CPU (central processing unit) executes a program, i.e., command by command is processed.
\end{definition}

\begin{definition}[Data access]
A data access is a command aiming on processing data of a program. There are two \textit{access types}:
\begin{enumerate}
  \item \textit{read} data: load the value stored at an address into a register
  \item \textit{write} data: store the value of a register at an address
\end{enumerate}
In general a data access consists of 3 components: (1) an access type, (2) an address, and (3) a register. For our purpose we don't care about the register. Hence, a data access is a tuple $(t, a)$ where $t$ is an access type and $a$ is an address (within the bounds of the program).
\end{definition}

\begin{definition}[Data access time]
The data access time is the amount of time it takes to data between a register and memory or vice versa.
\end{definition}

\begin{definition}[Cache Hit]
If the CPU processes a data access on an address which is already stored in the cache, this is called a \textit{cache hit}.
\end{definition}

\begin{definition}[Cache Miss]
If the CPU processes a data access on an address which is not yet stored in the cache, this is called a \textit{cache miss}.
\end{definition}

\begin{definition}[Eviction strategy]
In case of a cache miss it is required make space for the requested address and its data. The \textit{eviction strategy} decides which address is evicted from the cache, i.e., the data of this address is written back to main memory.
\end{definition}

\subsection{Performance}
\begin{definition}[Program performance]
The performance of a program is the time it take the CPU to process all commands, this is called \textit{total execution time}
\end{definition}

\begin{definition}[Allocator performance]
The performance of an allocator is the number of cache misses yield by a programs trace.
\end{definition}

% +-----------+   +--------+
% | Allocator |   | Memory |
% | Trace     | + | Model  | = Performance metric
% |           |   |        |
% +-----------+   +--------+

\subsection{The Allocators}

\subsubsection{SingleAssignmentAllocator}
\subsubsection{IdentityAllocator}
\subsubsection{CompactStackAllocator}
\subsubsection{CompactQueueAllocator}
\subsubsection{CompactRandomAllocator}

\subsection{The Caches}
\subsubsection{BeladyCache}
\subsubsection{OPTCache}
\subsubsection{ValgrindCache}
\subsubsection{ValgrindLivenessCache}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Experiments}

\section{Compact Allocators}

\begin{quote}
For a given trace, can I get a better (the optimal) performance simply by changing addresses?
\end{quote}

\subsection{Previous Work}

The last reports have shown that in the context of scratchpad memory and with full knowledge about the liveness of addresses the number of memory accesses is independent of the memory layout. The number of misses, however, is influenced by it.
The plots suggest that a compacting allocator which reuses cached dead items gives a transformation that is minimal in the number of misses when using a scratchpad memory model.

\begin{quote}
Given a trace $T$, liveness information of addresses of $T$, a scratchpad memory and a scratchpad strategy based on $T$. Then, a compacting allocator that reuses cached dead items when possible gives the transformed trace $T'$ that is optimal in the number of misses.
\end{quote}

This report compares several implementations of compacting allocators.

\subsection{Methodology}

We compare the following allocators.
\begin{enumerate}
  \item \textbf{Identity} Addresses are unchanged from the original trace.
  \item \textbf{SSA} Addresses are not reused.
  \item \textbf{CompactStack} Bump pointer allocator with a free list with stack semantics. Freed addresses are appended at the end and popped from the end on allocation.
  \item \textbf{CompactQueue} Bump pointer allocator with a free list with queue semantics. Freed addresses are appended at the end and taken from the beginning on allocation.
  \item \textbf{CompactRandom} Bump pointer allocator with a free list where free addresses are chosen at random on allocation.
  \item \textbf{CompactCached} Bump pointer allocator that reuses a freed, cached address if there is one or a stack-like free list otherwise.
\end{enumerate}

The memory model is again a scratchpad memory with 8-byte lines and an increasing size.

Again, the results for the V8 benchmarks Richards, Deltablue, and Raytrace are plotted.

\subsection{Results}

\subsubsection{Richards - misses}
\begin{table}
  \centering
  \begin{tabular}[c]{|l|r|r|r|r|r|r|}
    \hline
    \textbf{SPM size in bytes} & \textbf{Identity} & \textbf{SSA} & \textbf{CompactStack} & \textbf{CompactQueue} & \textbf{CompactRandom} & \textbf{CompactCached} \tabularnewline
    \hline \hline
    1024  & 19906406 & 27186015 & 14288773 & 27185933 & 14659129 & 13501287 \tabularnewline   
    2048  & 13348365 & 21797929 &  7037009 & 21797723 &  7436173 &  6334160 \tabularnewline  
    4096  &  8502161 & 19039748 &  2529214 & 19038965 &  3015836 &  2094777 \tabularnewline  
    8192  &  5997722 & 18550483 &  1587408 & 18548641 &  2135818 &  1263075 \tabularnewline  
    16384 &  5278238 & 18315329 &  1128682 & 18305995 &  1707261 &   859009 \tabularnewline 
    32768 &  5077416 & 18178226 &   857012 & 18080705 &  1449062 &   618508 \tabularnewline 
    65536 &  4495702 & 18094113 &   677410 & 17684018 &  1262799 &   466934 \tabularnewline
    \hline
  \end{tabular}
  \caption{\todo{add caption}}
  \label{tab:experiment-compact-richards-misses}
\end{table}

\begin{figure}[ht]
  \centering
  \includegraphics[width = .75\textwidth]{plots/compact-richards.png}
  \caption{Misses of Richards benchmark}
  \label{fig:experiment-compact-richards-misses}
\end{figure}

\subsubsection{DeltaBlue - misses}

\begin{table}
  \centering
  \begin{tabular}[c]{|l|r|r|r|r|r|r|}
    \hline
    \textbf{SPM size in bytes} & \textbf{Identity} & \textbf{SSA} & \textbf{CompactStack} & \textbf{CompactQueue} & \textbf{CompactRandom} & \textbf{CompactCached} \tabularnewline
    \hline \hline
    1024  & 77146231 & 98768459 & 52700195 & 98768383 & 53306777 & 50758432 \tabularnewline
    2048  & 53120872 & 79005685 & 28072125 & 79005479 & 28715840 & 25892998 \tabularnewline
    4096  & 41637998 & 70953742 & 16673778 & 70952971 & 17370985 & 14835627 \tabularnewline
    8192  & 28707195 & 65036775 &  7392572 & 65035091 &  8198850 &  6270975 \tabularnewline
    16384 & 23906668 & 62731219 &  3346997 & 62714931 &  4278582 &  2612975 \tabularnewline
    32768 & 20440744 & 62151273 &  2151620 & 62022996 &  3109816 &  1576686 \tabularnewline
    65536 & 20079036 & 61858627 &  1516174 & 61517639 &  2497068 &  1043268 \tabularnewline
    \hline
  \end{tabular}
  \caption{\todo{add caption}}
  \label{tab:experiment-compact-deltablue-misses}
\end{table}

\begin{figure}[ht]
  \centering
  \includegraphics[width = .75\textwidth]{plots/compact-deltablue.png}
  \caption{Misses of DeltaBlue benchmark}
  \label{fig:experiment-compact-deltablue-misses}
\end{figure}

\subsubsection{Raytrace - misses}

\begin{table}
  \centering
  \begin{tabular}[c]{|l|r|r|r|r|r|r|}
    \hline
    \textbf{SPM size in bytes} & \textbf{Identity} & \textbf{SSA} & \textbf{CompactStack} & \textbf{CompactQueue} & \textbf{CompactRandom} & \textbf{CompactCached} \tabularnewline
    \hline \hline
    1024  & 17020972 & 24944756 & 11379998 & 24944674 & 11800338 & 10296539 \tabularnewline
    2048  & 14440844 & 23046938 &  8474762 & 23046732 &  8923334 &  7499105 \tabularnewline
    4096  & 12541349 & 21674551 &  6328499 & 21673768 &  6803800 &  5444919 \tabularnewline
    8192  & 10856047 & 20607354 &  4517425 & 20605669 &  5020860 &  3757431 \tabularnewline
    16384 &  9491867 & 19882091 &  3159250 & 19876330 &  3705843 &  2531928 \tabularnewline
    32768 &  8906151 & 19497833 &  2429807 & 19478760 &  2996836 &  1877661 \tabularnewline
    65536 &  8218054 & 19236973 &  1910619 & 19173814 &  2483494 &  1421185 \tabularnewline
    \hline
  \end{tabular}
  \caption{\todo{add caption}}
  \label{tab:experiment-compact-raytrace-misses}
\end{table}

\begin{figure}[ht]
  \centering
  \includegraphics[width = .75\textwidth]{plots/compact-raytrace.png}
  \caption{Misses of Raytrace benchmark}
  \label{fig:experiment-compact-raytrace-misses}
\end{figure}

\subsubsection{Navier-Stokes - misses}

From Chromium Blog\footnote{\url{https://blog.chromium.org/2012/03/v8-benchmark-suite-extended-with.html}}
\begin{quote}
  This new version adds Oliver Hunt's 2D Navier-Stokes fluid dynamic simulation, which stresses intense double array computations. These complex double array computations are today common in games, graphic and scientific applications.
\end{quote}

\begin{table}
  \centering
  \begin{tabular}[c]{|l|r|r|r|r|r|r|}
    \hline
    \textbf{SPM size in bytes} & \textbf{Identity} & \textbf{SSA} & \textbf{CompactStack} & \textbf{CompactQueue} & \textbf{CompactRandom} & \textbf{CompactCached} \tabularnewline
    \hline \hline
    1024  & 151591543 & 197638846 & 122279178 & 197638770 & 124040970 & 120281818 \tabularnewline
    2048  & 126350972 & 173010778 &  96389043 & 173010573 &  98196415 &  94309498 \tabularnewline
    4096  & 123796035 & 170945407 &  93487986 & 170944635 &  95316020 &  91414571 \tabularnewline
    8192  & 121982315 & 169615053 &  91176334 & 169613376 &  93022611 &  89148010 \tabularnewline
    16384 & 118622912 & 167610304 &  87596431 & 167604544 &  89467888 &  85647336 \tabularnewline
    32768 & 114717104 & 163980384 &  81473478 & 163960984 &  83360157 &  79583462 \tabularnewline
    65536 & 106277710 & 156998071 &  70413666 & 156881332 &  72288424 &  68566643 \tabularnewline
    \hline
  \end{tabular}
  \caption{\todo{add caption}}
  \label{tab:experiment-compact-navier-stokes-misses}
\end{table}

\begin{figure}[ht]
  \centering
  \includegraphics[width = .75\textwidth]{plots/compact-navier-stokes.png}
  \caption{Misses of Navier-Stokes benchmark}
  \label{fig:experiment-compact-navier-stokes-misses}
\end{figure}

\subsection{Conclusion}

The plots show that CompactCached has the fewest misses of all implemented compacting allocators.
Also, CompactStack has a comparable number of misses.
Interestingly also CompactRandom comes close.
We will examine if this is because of a flawed implementation or actual legit behavior.

While the plots suggest that CompactCached is better than any other examined allocator we still need solid evidence for this property.


\section{8th June, 2017 and 13th June, 2017}
\begin{quote}[Research question]
Can I get the optimal performance by changing the addresses used by a given trace $T$?
\end{quote}

\subsection{What is this report about?}
The purpose of this experiment is to show that under certain assumptions the memory layout has no impact on the number of memory accesses.

\subsection{Experimental setup}
\subsubsection{Definitions}

\begin{definition}[Instruction] 
An instruction is the pair $(t, a)$ where $t \in {R, W}$ denotes read and write access and $a$ the address being accessed.
\end{definition}

\begin{definition}[Trace] 
A trace is a sequence $((t_0, a_0), (t_1, a_1), ..., (t_{n-1}, a_{n-1}))$ of pairs with $0 \leq i < n$ where $t_i \in {R, W}$ denotes read or write access and $a_i$ the address being accessed.
\end{definition}

\begin{definition}[Liveness interval] 
A liveness interval is defined as the interval between a write to an address and the final access before the next write to the same address or the end of the trace.
\end{definition}

\begin{definition}[Trace transformation] 
We define a transformation between traces $T$ and $T'$ as a renaming of addresses such that liveness intervals are unchanged.
\end{definition}

\begin{definition}[Scratchpad memory (SPM)] 
A scratchpad memory is an application controlled, fast memory. A subset of addresses in main memory.
\end{definition}

% +-----+ store  +-------+ store  +-----+
% &     |------->|       |------->|     |
% & CPU &        &  SPM  &        & RAM |
% &     |<-------|       |<-------|     |
% +-----+  load  +-------+  load  +-----+

\begin{definition}[Memory accesses] 
A store or a load between scratchpad memory and main memory.
\end{definition}

\begin{definition}[Miss] 
Whenever the accesses item is not in the SPM it is a \textit{miss}, other wise it is a \textit{hit}.
\end{definition}

\begin{definition}[Performance] 
Performance is defined as the number of memory accesses during the the execution of a trace $T$.
\end{definition}

\begin{definition}[Optimal Performance] 
The optimal performance is the minimal number of memory accesses of all possible transformations of a trace $T$.
\end{definition}

\begin{definition}[The minimum heap size] 
Let $n_i$ $0 \leq i \leq N$ be the numbers of overlapping intervals at instruction $i$ of the trace. The minimum heap size is: $n_max * word_size$ where $n_max = \max{n_i, 0 \leq i \leq N}$.
\end{definition}

\subsection{Hypothesis}

Given a trace $T$, a scratchpad memory, liveness intervals of $T$, then the number of memory accesses is constant for any memory layout.

\subsection{Experiments}

The tiny experiment is repeated on traces of V8 benchmarks for an SPM size of $2^10$ bytes to $2^16$ bytes. Shown in this table is the number of memory accesses for each allocator as well as the average and standard deviation across all allocators.

Benchmark description taken from Chromium Blog (\url{https://blog.chromium.org/2010/10/v8-benchmark-suite-updated.html}).

\subsubsection{Tiny Example}

Take an example trace (all accesses are 8 bytes):

\begin{lstlisting}
0: W 8
1: W 16
2: W 24
3: R 16
4: R 8
5: W 32
6: R 24
7: R 32
8: R 24
9: R 16
\end{lstlisting}

We compute the misses and memory accesses of four allocators (= trace transformations):
\begin{enumerate}
\item \textbf{Identity:} Addresses are unchanged
\item \textbf{Compact:} Bump pointer allocator with a free stack
\item \textbf{Random:} Random addresses
\item \textbf{SSA:} Bump pointer without reusing of addresses
\end{enumerate}

The SPM has a size of 16 bytes and lines of 8 bytes.

Let us look at a few situations:
\begin{enumerate}
\item At instruction 2 there is a write access to address 24, the SPM contains 8 and 16. Both items in SPM are live. The item which access is the furthest in the future is chosen to be stored to main memory and replaced by the new item (see Belady's algorithm \cite{belady1966study}). This is 8 in this case. Since the new item is written to there is no need to load its content from main memory. This instruction therefore results in 1 memory access.
\item At instruction 4 there is a read request to address 8, the SPM contains 16 and 24. According to Belady 16 is selected for eviction and stored to main memory. The new item is read, therefore its content has to be loaded from main memory. This instruction results in 2 memory access.
\item At instruction 9 the address 16 is read. Both items in the SPM are dead, therefore they do not have to stored back to main memory for eviction. Regardless of the eviction choice, only the read item has to be loaded from main memory, resulting in 1 memory access for this instruction.
\end{enumerate}

The experiment is run with \texttt{semloc} and these parameters:

\begin{lstlisting}
bin/semloc -c 4 -C 4 -l 8 -L 8 -a 3 -A 3 traces/spm-example.trace 
\end{lstlisting}

The results are

\begin{table}
  \centering
  \begin{tabular}[c]{|l|c|c|}
    \hline
    \textbf{Allocator} & \textbf{Misses} & \textbf{Memory Accesses} \tabularnewline
    \hline\hline
    Identity & 6 & 4 \tabularnewline
    Compact  & 5 & 4 \tabularnewline
    Random   & 6 & 4 \tabularnewline
    SSA      & 6 & 4 \tabularnewline
    \hline
  \end{tabular}
  \caption{\todo{add caption}}
  \label{tab:experiment-2017-june-8}
\end{table}

The trace results in 4 memory access for all allocators. This suggests that memory accesses in the context of SPM are indeed independent of memory layout.

The Compact allocator has 5 misses, as opposed to all other which have 6 misses: At instruction 5 the new address 32 is written to. At this point, address 8 is dead and in the SPM. The Compact allocator reuses address 8, thus resulting in a hit. Other allocators do not exploit this possibility and have a miss at this instruction.

% \subsection{Conclusion}
% We presented the hypothesis that the memory layout does not matter for scratchpad memories, which know the trace T and liveness intervals of T. For illustrations we discussed a small example which confirms our hypothesis.

% In our example the Belady eviction policy in combination with liveness intervals make sure that memory accesses and misses remain minimal. The compact allocator only has a chance to get below that number of misses, because it turns a miss into a hit. At allocation time he can immediately achieve this by reusing a dead address in SPM.

% To conclude, this small example nicely illustrations that for systems which has certain information about there applications the memory layout has no influence.


\subsubsection{V8 benchmark: Richards}

Operating system kernel simulation benchmark originally written in BCPL by Martin Richards. The Richards benchmark effectively measures how fast the JavaScript engine is at accessing object properties, calling functions, and dealing with polymorphism. It is a standard benchmark that has been successfully used to measure the performance of many modern programming language implementations.

Trace size: 83M instructions

Memory accesses:
\begin{table}
  \centering
  \begin{tabular}[c]{|l|r|r|r|r|r|r|}
    \hline
    \textbf{SPM size in bytes} & \textbf{Identity} & \textbf{Compact} & \textbf{Random} & \textbf{SSA} & \textbf{Average} \tabularnewline
    \hline \hline
    1024  & 18654262 & 18654262 & 18654262 & 18654262 & 18654262 \tabularnewline
    2048  &  7878090 &  7878090 &  7878090 &  7878090 &  7878090 \tabularnewline
    4096  &  2361728 &  2361728 &  2361728 &  2361728 &  2361728 \tabularnewline
    8192  &  1383198 &  1383198 &  1383198 &  1383198 &  1383198 \tabularnewline
    16384 &   912890 &   912890 &   912890 &   912890 &   912890 \tabularnewline
    32768 &   638684 &   638684 &   638684 &   638684 &   638684 \tabularnewline
    65536 &   470458 &   470458 &   470458 &   470458 &   470458 \tabularnewline
    \hline
  \end{tabular}
  \caption{\todo{add caption}}
  \label{tab:experiment-2017-june-8-richards-mem}
\end{table}


Misses:
\begin{table}
  \centering
  \begin{tabular}[c]{|l|r|r|r|r|}
    \hline
    \textbf{SPM size in bytes} & \textbf{Identity} & \textbf{Compact} & \textbf{Random} & \textbf{SSA}  \tabularnewline
    \hline \hline
    1024  & 19906406 & 14288773 & 27186015 & 27186015 \tabularnewline
    2048  & 13348365 &  7037009 & 21797929 & 21797929 \tabularnewline
    4096  &  8502161 &  2529214 & 19039748 & 19039748 \tabularnewline
    8192  &  5997722 &  1587408 & 18550483 & 18550483 \tabularnewline
    16384 &  5278242 &  1128682 & 18315329 & 18315329 \tabularnewline
    32768 &  5077416 &   857012 & 18178226 & 18178226 \tabularnewline
    65536 &  4495702 &   677410 & 18094113 & 18094113 \tabularnewline
    \hline
  \end{tabular}
  \caption{\todo{add caption}}
  \label{tab:experiment-2017-june-8-richards-misses}
\end{table}

\begin{figure}[ht]
  \centering
  \includegraphics[width = .75\textwidth]{plots/2017-06-13-richards-misses.png}
  \caption{Misses of Richards benchmark}
  \label{fig:experiment-2017-june-8-richards-misses}
\end{figure}

\subsubsection{V8 benchmark: Deltablue}

>One-way constraint solver, originally written in Smalltalk by John Maloney and Mario Wolczko. The DeltaBlue benchmark is written in an object-oriented style with a multi-level class hierarchy. As such it measures how fast the JavaScript engine is at running well-structured applications with many objects and small functions.

Trace size: 272M instructions

Memory accesses:
\begin{table}
  \centering
  \begin{tabular}[c]{|l|r|r|r|r|r|r|}
    \hline
    \textbf{SPM size in bytes} & \textbf{Identity} & \textbf{Compact} & \textbf{Random} & \textbf{SSA} & \textbf{Average} \tabularnewline
    \hline \hline
    1024  & 74897566 & 74897566 & 74897566 & 74897566 & 74897566 \tabularnewline
    2048  & 35372018 & 35372018 & 35372018 & 35372018 & 35372018 \tabularnewline
    4096  & 19268132 & 19268132 & 19268132 & 19268132 & 19268132 \tabularnewline
    8192  &  7434198 &  7434198 &  7434198 &  7434198 &  7434198 \tabularnewline
    16384 &  2823086 &  2823086 &  2823086 &  2823086 &  2823086 \tabularnewline
    32768 &  1663194 &  1663194 &  1663194 &  1663194 &  1663194 \tabularnewline
    65536 &  1077902 &  1077902 &  1077902 &  1077902 &  1077902 \tabularnewline
    \hline
  \end{tabular}
  \caption{\todo{add caption}}
  \label{tab:experiment-2017-june-8-deltablue-mem}
\end{table}

Misses:
\begin{table}
  \centering
  \begin{tabular}[c]{|l|r|r|r|r|}
    \hline
    \textbf{SPM size in bytes} & \textbf{Identity} & \textbf{Compact} & \textbf{Random} & \textbf{SSA}  \tabularnewline
    \hline \hline
    1024  & 77146231 & 52700195 & 98768459 & 98768459 \tabularnewline
    2048  & 53120872 & 28072125 & 79005685 & 79005685 \tabularnewline
    4096  & 41637998 & 16673778 & 70953742 & 70953742 \tabularnewline
    8192  & 28707195 &  7392572 & 65036775 & 65036775 \tabularnewline
    16384 & 23906672 &  3346997 & 62731219 & 62731219 \tabularnewline
    32768 & 20440744 &  2151620 & 62151273 & 62151273 \tabularnewline
    65536 & 20079036 &  1516174 & 61858627 & 61858627 \tabularnewline
    \hline
  \end{tabular}
  \caption{\todo{add caption}}
  \label{tab:experiment-2017-june-8-deltablue-misses}
\end{table}

\begin{figure}[ht]
  \centering
  \includegraphics[width = .75\textwidth]{plots/2017-06-13-deltablue-misses.png}
  \caption{Misses of Deltablue benchmark}
  \label{fig:experiment-2017-june-8-deltablue-misses}
\end{figure}

\subsubsection{V8 benchmark: Raytrace}

Ray tracer benchmark based on code by Adam Burmister. The benchmark measures floating-point computations where the object structure is constructed using the Prototype JavaScript library.

Trace size: 62M instructions

Memory accesses:
\begin{table}
  \centering
  \begin{tabular}[c]{|l|r|r|r|r|r|r|}
    \hline
    \textbf{SPM size in bytes} & \textbf{Identity} & \textbf{Compact} & \textbf{Random} & \textbf{SSA} & \textbf{Average} \tabularnewline
    \hline \hline
    1024  & 12897373 & 12897373 & 12897373 & 12897373 & 12897373 \tabularnewline
    2048  &  9101737 &  9101737 &  9101737 &  9101737 &  9101737 \tabularnewline
    4096  &  6356963 &  6356963 &  6356963 &  6356963 &  6356963 \tabularnewline
    8192  &  4222569 &  4222569 &  4222569 &  4222569 &  4222569 \tabularnewline
    16384 &  2772043 &  2772043 &  2772043 &  2772043 &  2772043 \tabularnewline
    32768 &  2003527 &  2003527 &  2003527 &  2003527 &  2003527 \tabularnewline
    65536 &  1481807 &  1481807 &  1481807 &  1481807 &  1481807 \tabularnewline
    \hline
  \end{tabular}
  \caption{\todo{add caption}}
  \label{tab:experiment-2017-june-8-raytrace-mem}
\end{table}

Misses:
\begin{table}
  \centering
  \begin{tabular}[c]{|l|r|r|r|r|}
    \hline
    \textbf{SPM size in bytes} & \textbf{Identity} & \textbf{Compact} & \textbf{Random} & \textbf{SSA}  \tabularnewline
    \hline \hline
    1024  & 17020972 & 11379998 & 24944756 & 24944756 \tabularnewline
    2048  & 14440844 &  8474762 & 23046938 & 23046938 \tabularnewline
    4096  & 12541349 &  6328499 & 21674551 & 21674551 \tabularnewline
    8192  & 10856047 &  4517425 & 20607354 & 20607354 \tabularnewline
    16384 &  9491871 &  3159250 & 19882091 & 19882091 \tabularnewline
    32768 &  8906151 &  2429807 & 19497833 & 19497833 \tabularnewline
    65536 &  8218054 &  1910619 & 19236973 & 19236973 \tabularnewline
    \hline
  \end{tabular}
  \caption{\todo{add caption}}
  \label{tab:experiment-2017-june-8-raytrace-misses}
\end{table}

\begin{figure}[ht]
  \centering
  \includegraphics[width = .75\textwidth]{plots/2017-06-13-raytrace-misses.png}
  \caption{Misses of Raytrace benchmark}
  \label{fig:experiment-2017-june-8-raytrace-misses}
\end{figure}

\subsection{Conclusion}
We presented the hypothesis that the memory layout does not matter for scratchpad memories, which know the trace T and liveness intervals of T. For illustrations we discussed a small example which confirms our hypothesis.

In our example, the Belady eviction policy in combination with liveness intervals makes sure that memory accesses and misses remain minimal. The compact allocator only has a chance to get below that number of misses, because it turns a miss into a hit. At allocation time he can immediately achieve this by reusing a dead address in SPM.

To conclude, this small example nicely illustrations that for systems which have certain information about their applications the memory layout has no influence.

\subsection{Further steps}
In this (mini) report we have shown empirically that memory access is constant for all memory layouts. next, we have to prove that a compact allocator as we described in the conclusion is optimal with regards to cache misses in this SPM environment.

\section{27th July, 2017}

\subsection{Experiment: \textit{Bug fix}}
\begin{itemize}
  \item Rerun the experiments of this report, because of the counting bug fix of the LRU caches.
  \item \textit{Expectations:} The number of cycles per instruction for both LRU caches will shrink. Still we expect at both LRU implementations perform worse than OPTCache and BeladyCache.
\end{itemize}

\subsection{Experiment: \textit{Compact-Random-Allocator performance robustness}}
\begin{itemize}
  \item Find a trace optimized for the Compact-Queue-Allocator (Compact-Queue-Allocator performs better than the Compact-Stack-Allocator)
  \item Find a trace optimized for the Compact-Stack-Allocator (Compact-Stack-Allocator performs better than the Compact-Queue-Allocator)
  \item \textit{Expectations:} A good/an interesting result would be if the Compact-Random-Allocator performs (nearly) as good as the Compact-Stack-Allocator, and Compact-Queue-Allocator, respectively.
  \item \textit{Notes}
  \begin{itemize}
    \item 2017-07-31: Right now we think that it is not possible with a free list implementation based on a queue to outperform an stack implementation.
    \begin{itemize}
      \item Either the \textit{most-recently-freed} address is not in the cache anymore, then both implementations force a \textit{cache miss}.
      \item Or the \textit{most-recently-freed} address is still in the cache, then the stack implementation results in a \textit{cache hit} and the queue implementation results in a \textit{cache miss}.
    \end{itemize}
    \item Intuitively by counter-example. Assume high enough associativity. Assume CompactQueue is better than CompactStack in the same situation (same access, same state of free list), i.e. allocation  to address $a$ using CompatQueue results in a cache hit and to address $b$ with CompactStack in a cache miss. Order in free list $a < \dots < b$ and $a \not= b$. Since a free is an access this means that the last access of $a$ was before $b$. Since we have an LRU cache this would imply that $b$ was evicted before $a$, contradicting the access order.
  \end{itemize}
\end{itemize}

\subsection{Experiment: \textit{Associativity influence on LRU caches}}
\begin{itemize}
  \item Pick one of the experiments above, namely Richards.
  \item Pick one fixed cache size 
  \begin{itemize}
    \item either 1024 bytes (very small -> a lot of loads/stores)
    \item or 32KB (more realistic: l1d size of big-iron8 and MacBook Pro)
  \end{itemize}
  \item Run this experiment for different associativities of the LRU caches, e.g, 4, 8, 16, 32, fully
  \item \textit{Expectations:} The number of cycles per instruction should be identical for all associativities of the LRU caches to show that these are independent of the associativity.
\end{itemize}

\subsection{Experiment: \textit{Equally weight stores and loads (cache)}}
\begin{itemize}
  \item Generate some of the performance-indexed-figures with different weight for load and store operations.
  \item Pick two fixed cache sizes, namely 1KB and 64KB.
  \item Pick all 4 experiments from above.
  \item Pick the store-weights as multiples of the load-weights (based on the cache-loads, and cache-stores figures), e.g, 2, 3, 5, 10, ...
  \item \textit{Expectations:} The influence of the store-weight should become recognizable for unrealistic are values. To conclude it is reasonable to weight loads and stores the same.
\end{itemize}

\subsection{Experiment: \textit{Equally weight stores and loads (memory)}}
\begin{itemize}
  \item Same as for the cache loads and stores.
\end{itemize}

\subsection{Experiment: \textit{New benchmarks}}
\begin{itemize}
  \item Find new benchmarks apart of the V8 benchmarks (Richards, Raytrace, Deltablue, and Navier-stroke).
  \item Based on the Scalloc paper try to get access to the SPEC CPU2006 benchmarks.
  \begin{itemize}
    \item Generate traces.
    \item Rerun SemanticLocality with the new traces.
    \item Generate performance-indexed-figures.
  \end{itemize}
  \item \textit{Expectations:} Either confirmation of your previous results or new deep knowledge of the behavior of the caches and allocators, respectively.
\end{itemize}

\section{4th August, 2017}
\subsection{Bubble Pie Chart}

Colors
\begin{itemize}
  \item Hits in green, misses in red
  \item Cache loads, cache store, memory store, memory loads in green, yellow, orange, and red
  \item "Less is better" instead of "Lower is better"
  \item Relative and absolute number of trace reads and writes (= cache stores and loads) in legend of the plot
\end{itemize}

\subsection{Associativity Experiment}
Absolute numbers are enough

\subsection{Line Size}
\begin{itemize}
  \item Try 2 words
  \item A compacting allocator with a \textit{spatial locality threshold} $c$: The allocator remembers the last original address $a$. If a new address for $a+n$ is requested with $n \leq c$, then the allocator looks at the top 2 elements in the free stack and returns the one that is on the same cache line as the address for $a$, or any of the two if there is none.
  \item Change Belady for multi-word lines (next access data structure)
  \item Line size as CLI argument (no range, just a single value)
\end{itemize}

\subsection{Bar chart for number of addresses for all allocators}
Number of different addresses is print as a new column in the experiment results

\subsection{Workload for poor performance of CompactRandom}
\todo{no signs of interests in this direction by Christoph}

\subsection{Description why CompactQueue can never be better than CompactStack}
Intuitively by counter-example. Assume high enough associativity. Assume CompactQueue is better than CompactStack in the same situation (same access, same state of free list), i.e. allocation  to address $a$ using CompatQueue results in a cache hit and to address b with CompactStack in a cache miss. Order in free list $a < \dots < b$ and $a \not= b$. Since $a$ free is an access this means that the last access of $a$ was before $b$. Since we have an LRU cache this would imply that $b$ was evicted before $a$, contradicting the access order.

\todo{copy paste old content}